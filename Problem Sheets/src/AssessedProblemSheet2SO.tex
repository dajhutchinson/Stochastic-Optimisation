\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm}
\usepackage[section,nohyphen]{DomH}
\headertitle{Stochastic Optimisation - Assessed Problem Sheet 2}

\begin{document}

\questionsfalse
% \answersfalse

\title{Stochastic Optimisation - Assessed Problem Sheet 2}
\author{Dom Hutchinson (170 1111)}
\date{\today}
\maketitle


\begin{question}{1)}
  A company makes a single product. At the beginning of each month, the company decides how many products to make in the month. The company is able to make at most $m$ products in a single month. All products made in a month are sold at the end of the month, for the same unit price.
  \par The unit price products are sold at, randomly fluctuates in the set $\{b(1),\dots,b(n)\}$ where
  \[ 0<b(1)<\dots<b(n)\text{ and }b(i)\in\reals^{>0}\ \forall\ i\in[1,n] \]
  The unit price products are sold at depends on the number of products made in the month. If the company makes $a$ products in month $t$, then $b(s')$ is the unit price of the product at the end of month $t$ with probability $\pi(s'|s,a)$. Here $a,s,s'\in\nats$ with $a\in[1,m]$ and $s,s'\in[1,n]$. Note that $\pi(\cdot|\cdot,\cdot)$ are probability mass functions.
  \par The cost of making a single product is $\gamma\in(0,\infty)$, which is constant. If $b(s')$ is the unit price at the end of the month $t$ and $b(s')<\gamma$, then the company incurs a loss of $a\cdot(\gamma-b(s'))$ at the end of month $t$. The penatly for loss $z$ incurred at the end of a month is $\alpha z$ where $\alpha\in(0,\infty)$ is a constant.
  \par Assume the following
  \begin{itemize}
    \item $b(s)=\beta s$ where $\beta\in(0\infty)$ is a constant.
    \item $\gamma=l\beta$ where $l\in\nats$ and $l\in[1,n)$.
  \end{itemize}
  The company wants to select a production policy such that the corresponding expected total profit over $N$ months is maximal.
  \[ \expect^\pi\left[\sum_{t=0}^{N-1}R_t\right] \]
  where $R_t$ is the company's profit in month $t$.
\end{question}

\begin{question}{1) (a)}
  Formulate the described production management problem as a \textit{Finite-Horizon Markov Decision Problem}. Specifically, state the state-space $S$, action-space $A$, time-horizon $T$, transition probabilities $p_t(s'|s,a)$ and equivalent immediate rewards $r_t(s,a)$.\footnote{This does not require formulating a state-equation.}
\end{question}

\begin{answer}{1) (a)}
  Here I formulate the problem described in 1) as a \textit{Finite-Horizon Markov Decision Problem}.
  \begin{itemize}
    \item \textit{Decision Epoch} - Start of each month.
    \item \textit{Time-Horizon} - $T=\{0,1,\dots,N-1\}$.
    \item \textit{Agent Actions}.
    \par Let $Y_t$ denote the action taken by the agent in epoch $t$ and be defined as the amount of product the company produces in epoch $t$.
    \item \textit{Action-Space}.
    \par By the defintiion of $Y_t$ and the fact that the company can produce at most $m$ products in a month
    \[ A=\{0,\dots,m\} \]
    \item \textit{System States}.
    \par Let $X_t$ denote the system state in epoch $t$ and be defined as the index of the unit price which each product was sold at in epoch $t-1$ (ie at the end of the previous month).
    \item \textit{State -Space}.
    \par By the definition of $X_t$ and since unit prices are indexed from $1$ to $n$
    \[ S=\{1,\dots,n\} \]
    \item \textit{Transition Probabilities}.
    \par Let $s,s'\in S,a\in A(s)$ and define $p_t(s'|s,a)=\prob(X_{t+1}=s'|X_t=s,Y_t=a)$. Then
    \[\begin{array}{rcl}
      p_t(s'|s,a)&=&p(s'|s,a)\\
      &=&\pi(s'|s,a)
    \end{array}\]
    where $\pi(\cdot|\cdot,\cdot)$ is as defined in the question.
    \item \textit{Immediate Rewards}
    The immedaite reward $r_t$ recieved in each epoch $t$ depends on the unit price at the end of that epoch, but, due to my definition of system states, this value is not encoded into the system state until epoch $t+1$. This does not fit into the framework of Markovian decision problems so we take the expected reward in each epoch.
    \[\begin{array}{rcl}
      \expect^\pi[R_t]&=&\expect^\pi[r(X_t,Y_t)]\\
      &=&\begin{cases}
        \expect^\pi\left[Y_t\big(\beta(X_{t+1})-\gamma\big)\right]&\text{if }\beta(X_{t+1})\geq\gamma\\
        \expect^\pi\left[Y_t\big(\beta(X_{t+1})-\gamma\big)+\alpha Y_t\big(\beta(X_{t+1})-\gamma\big)\right]&\text{if }\beta(X_{t+1})<\gamma
      \end{cases}\\
      &=&\begin{cases}
        \expect^\pi\left[Y_t\beta\big(X_{t+1}-l\big)\right]&\text{if }X_{t+1}\geq l\\
        \expect^\pi\left[Y_t\beta\big(X_{t+1}-l\big)(1+\alpha)\right]&\text{if }X_{t+1}<l
      \end{cases}\text{ by def. }\beta(\cdot),\gamma\\
      &=&\expect^\pi\left[Y_t\beta\big(X_{t+1}-l\big)\big(1+\alpha\indexed\{X_{t+1}<l\}\big)\right]\\
      &=&\expect^\pi\bigg[\expect^\pi\left[Y_t\beta\big(X_{t+1}-l\big)\big(1+\alpha\indexed\{X_{t+1}<l\}\big)\right]\bigg|X_t,Y_t\bigg]\text{ by Tower property}\\
      &=&Y_t\beta\cdot\big(\expect^\pi[X_{t+1}|X_t,Y_t]-l\big)\cdot\big(1+\alpha\indexed\{\expect^\pi[X_{t+1}|X_t,Y_t]<l\}\big)
    \end{array}\]
  \end{itemize}
\end{answer}

\begin{question}{1) (b)}
  Assume $N=2,n=2,m=2,l=1,\alpha=0.3,\beta=1$ and
  \[\begin{array}{rclrcl}
    \pi(2|1,0)&=&0.6,&\pi(2|2,0)&=&0.8\\
    \pi(2|1,1)&=&0.3,&\pi(2|2,1)&=&0.4\\
    \pi(2|1,2)&=&0.1,&\pi(2|2,2)&=&0.3
  \end{array}\]
  Using the \textit{Dynamic Programming Algorithm}, find an optimal policy for the \textit{Markov Decision Problem} formulated in (a).
\end{question}

\begin{answer}{1) (b)}
  From the specification of the question, we know the following
  \[\begin{array}{rcl}
    T&=&\{0,1\}\\
    A&=&\{0,1,2\}\\
    S&=&\{1,2\}\\
    A(s)&=&\{0,1,2\}\ \forall\ s\in S\\
    r_t(s,a)&=&a\cdot\big(\expect^\pi[X_{t+1}|s,a]-1\big)\cdot\big(1+0.3\cdot\indexed\{\expect^\pi[X_{t+1}|s,a]<1\}\big)\text{ for }t\in T\\
    r_2(s)&=&0\ \forall\ s\in S\\\\
    \pi(1|s,a)&=&\begin{tabular}{c|ccc}
      s$\setminus$ a&0&1&2\\\hline
      1&0.6&0.3&0.1\\
      2&0.2&0.6&0.7
    \end{tabular}\\\\
    \pi(2|s,a)&=&\begin{tabular}{c|ccc}
      s$\setminus$ a&0&1&2\\\hline
      1&0.4&0.7&0.9\\
      2&0.8&0.4&0.3
    \end{tabular}
  \end{array}\]
  From this specification of the transition probabilities $p(s'|s,a)$ we can deduce the following values for the next expected system state
  \[\begin{array}{rcl}
    \expect^\pi(X_{t+1}|s,a)&=&\begin{tabular}{c|ccc}
      s$\setminus$a&0&1&2\\\hline
      1&1.6&1.3&1.1\\
      2&1.7&1.4&1.3
    \end{tabular}
  \end{array}\]
  Since $\expect^\pi(X_{t+1}|s,a)>1=l$ for all $s\in S,a\in A(s)$ we can simplify the reward function to the following
  \[\begin{array}{rcl}
    r_t(s,a)&=&a(\expect[X_{t+1}|s,a]-1)\text{ for }t\in T\\
    \implies r_t(s,a)&=&\begin{tabular}{c|ccc}
      s$\setminus$a&0&1&2\\\hline
      1&0&0.3&0.2\\
      2&0&0.4&0.6
    \end{tabular}
  \end{array}\]
  To find the optimal Markovian decision policy $\pi^*$ for this problem, using the dynamic programming algorithm we compute the following terms in a backwards recursion through $T$
  \[\begin{array}{rrl}
    w_t^*(s,a)&:=&r_t(s,a)+\sum_{s'\in S}u^*_{t+1}(s')p(s'|s,a)\\
    u_t^*(s)&=&\max_{a\in A(s)}(w_t^*)\\
    d_t^*(s)&\in&\argmax_{a\in A(s)}(w_t^*)
  \end{array}\]
  where $u_2^*(s):=r_2(s)=0\ \forall\ s\in S$.
  \begin{itemize}
    \item \textit{Time-Period} $t=1$.
    \par In this period
    \[\begin{array}{rcl}
      w_1^*(s,a)&=&r_1(s,a)+\sum_{s\in\{1,2\}}u_2^*(s')p(s'|s,a)\\
      &=&r_1(s,a)
    \end{array}\]
    Thus we can compute the following table of values for $w_1^*(s,a)$
    \[\begin{array}{rcl}
      w_1^*(s,a)&=&\begin{tabular}{c|ccc}
        s$\setminus$a&0&1&2\\\hline
        1&0&0.3&0.2\\
        2&0&0.4&0.6
      \end{tabular}
    \end{array}\]
    The table below summarises the values of $u_1^*(s),d_1^*(s)$ given this information
    \begin{center}
      \begin{tabular}{c|c|c}
        s&$u_1^*(s)$&$d_1^*(s)$\\\hline
        1&0.3&1\\
        2&0.6&2
      \end{tabular}
    \end{center}
    \item \textit{Time-Period} $t=0$.
    \par In this period
    \[\begin{array}{rcl}
      w_0^*(s,a)&=&r_0(s,a)+\sum_{s'\in\{1,2\}}u_1^*(s')p(s'|s,a)\\
      &=&r_0(s,a)+0.3\cdot p(1|s,a)+0.6\cdot p(2|s,a)
    \end{array}\]
    Thus we can compute the following table of values for $w_0^*(s,a)$
    \[\begin{array}{rcl}
      w_0^*(s,a)&=&\begin{tabular}{c|ccc}
      s$\setminus$ a&0&1&2\\\hline
      1&0.42&0.57&0.5\\
      2&0.54&0.7&0.9
      \end{tabular}
    \end{array}\]
    The table below summarises the values of $u_0^*(s),d_0^*(s)$ given this information
    \begin{center}
      \begin{tabular}{c|c|c}
        s&$u_0^*(s)$&$d_0^*(s)$\\\hline
        1&0.57&1\\
        2&0.9&2
      \end{tabular}
    \end{center}
  \end{itemize}
  The optimal value function is $u_0^*(s)$ and the optimal Markovian policy
  \[ \pi^*:=(d_1^*(s),d_2^*(s)) \]
\end{answer}

\begin{question}{2)}
  Consider the following \textit{Discounted Reward Markov Decision Problem}
  \begin{itemize}
    \item \textit{Time-Horizon} - $T=\{0,1,\dots\}$.
    \item \textit{State-Space} - $S$ is finite.
    \item \textit{Action-Space} - $A$ is finite.
    \item \textit{Admissible Action-Space} - $A(s)$.
    \item \textit{Transition Probabilities} - $p_t(s'|s,a)=p(s'|s,a)\ \forall\ t\in T$.
    \item \textit{Immediate Rewards} - $r_t(s,a)=r(s,a)\ \forall\ t\in T$.
    \item \textit{Discount Factor} - $\alpha\in(0,1)$.
  \end{itemize}
  Let $v^*(s)$ be the \textit{Optimal Value Function}\footnote{i.e. $v^*(s)$ is the unique solution to the \textit{Bellman Equation}.}. Let $u^*(s,a)$ be defined as
  \[ u^*(s,a)=v^*(s)-r(s,a)-\alpha\sum_{s'\in S}v^*(s')p(s'|s,a) \]
  Let $D^*(s)$ be the set of optimal actions given the system is in state $s$.
  \[ D^*(s)=\argmax_{a\in A(s)}\left\{r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right\} \]
  Let $q(a|s)$ be a decision probabilitiy and $\pi$ be the \textit{Stationary Markovian Policy} based $q(a|s)$.
\end{question}

\begin{question}{2) (a)}
  Show
  \[ u^*(s,a)\geq0\ \forall\ s\in S,a\in A(s) \]
\end{question}

\begin{answer}{2) (a)}
  \[\begin{array}{rrcll}
    &v^*(s)&=&\max_{a\in A(s)}\left\{r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right\}&\text{ by def}\\
    \implies&v^*(s)&\geq&r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)&\forall\ s\in S,a\in A(s)\\
    \implies&0&\leq&v^*(s)-r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)&\forall\ s\in S,a\in A(s)\\
    \implies&0&\leq&u^*(s,a)&\forall\ s\in S,a\in A(s)
  \end{array}\]
\end{answer}

\begin{question}{2) (b)}
  Show that if $q(\tilde{a}|\tilde{s})>0$ for some $\tilde{s}\in S,\tilde{a}\in(A(\tilde{s})\setminus D^*(\tilde{s}))$, then
  \[ \sum_{a\in A(\tilde{s})}u^*(\tilde{s},a)q(a|\tilde{s})>0 \]
\end{question}

\begin{answer}{2) (b)}
  Let $\tilde{s}\in S$ and $\tilde{a}\in (A(\tilde{s})\setminus D^*(\tilde{s}))$ (ie $a$ is a sub-optimal action).
  \par Suppose there is a non-negative probability that $\tilde{a}$ chosen to be played
  \[ q(\tilde{a}|\tilde{s})>0 \]
  Since $\tilde{a}\not\in D^*(\tilde{s})$ then
  \[\begin{array}{rrcl}
    &v^*(\tilde{s})&>&r(\tilde{s},\tilde{a})+\alpha\sum_{s'\in S}v^*(s')p(s'|\tilde{s},\tilde{a})\\
    \implies&0&<&v^*(\tilde{s})-r(\tilde{s},\tilde{a})-\alpha\sum_{s'\in S}v^*(s')p(s'|\tilde{s},\tilde{a})\\
    \implies&0&<&u^*(\tilde{s},\tilde{a})
  \end{array}\]
  Due to our assumption about $q(\tilde{a}|\tilde{s})$, we have that
  \[ u^*(\tilde{s},\tilde{a})q(\tilde{a},\tilde{s})>0\ \forall\ \tilde{a}\in A(\tilde{s})\setminus D^*(\tilde{s}) \]
  As this holds for all such $\tilde{a}$, their sum is strictly positive
  \[ \sum_{a\in A(\tilde{s})}u^*(\tilde{s},s)q(a|\tilde{s}) \]
\end{answer}

\begin{question}{2) (c)}
  Show
  \[ \expect^\pi\left[v^*(X_t)-\alpha v^*(X_{t+1})-r(X_t,Y_t)\right]=\expect^\pi\left[\sum_{a\in A(X_t)}u^*(X_t,a)q(a|X_t)\right]\geq0\ \forall\ t\in T \]
\end{question}

\begin{answer}{2) (c)}
  \[\begin{array}{rl}
    &\expect^\pi\left[v^*(X_t)-\alpha v^*(X_{t+1})-r(X_t,Y_t)\right]\\
    =&\expect^\pi\left[v^*(X_t)\right]-\alpha\expect^\pi\left[v^*(X_{t+1})\right]-\expect^\pi\left[r(X_t,Y_t)\right]\\
    =&\expect^\pi\left[v^*(X_t)\right]-\alpha\expect^\pi\big[\expect^\pi\left[v^*(X_{t+1})|X_t,Y_t\right]\big]-\expect^\pi\left[r(X_t,Y_t)\right]\\
    =&\expect^\pi\left[v^*(X_t)\right]-\alpha\expect^\pi\left[\sum_{s'\in S}v^*(s')p(s'|X_t,Y_t)\right]-\expect^\pi\left[r(X_t,Y_t)\right]\\
    =&\expect^\pi\left[v^*(X_t)-\alpha\sum_{s'\in S}v^*(s')p(s'|X_t,Y_t)-r(X_t,Y_t)\right]\\
    =&\expect^\pi\left[u^*(X_t,Y_t)\right]\text{ by definition}\\
    =&\expect^\pi\left[\expect^\pi\left[u^*(X_t,Y_t)|X_t\right]\right]\text{ by tower property}\\
    =&\expect^\pi\left[\sum_{a\in A(X_t)}u^*(X_t,a)q(a|X_t)\right]\text{ by def. conditional expectation}\\
    \geq&0\text{ due to 2) (b)}
  \end{array}\]
\end{answer}

\begin{question}{2) (d)}
  Show
  \[ \sum_{t=0}^\infty\alpha^t\expect\left[v^*(X_t)-\alpha v^*(X_{t+1})-r(X_t,Y_t)\right]=\expect^\pi\left[v^*(X_0)\right]-\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right] \]
\end{question}

\begin{answer}{2) (d)}
  \[\begin{array}{rl}
    &\sum_{t=0}^\infty\alpha^t\expect\left[v^*(X_t)-\alpha v^*(X_{t+1})-r(X_t,Y_t)\right]\\
    =&\sum_{t=0}^\infty\alpha^t\expect\left[v^*(X_t)-\alpha v^*(X_{t+1})\right]-\sum_{t=0}^\infty\alpha^t\expect\left[r(X_t,Y_t)\right]\\
    =&\expect\left[\sum_{t=0}^\infty\alpha^t\big(v^*(X_t)-\alpha v^*(X_{t+1})\big)\right]-\expect\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right]\\
    =&\expect\left[v^*(X_0)\right]-\expect\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right]
  \end{array}\]
  The last step is due to all terms, except $v^*(X_0)$, in the summation cancelling.
\end{answer}

\begin{question}{2) (e)}
  Assume that the stationary \textit{Markovian Policy} $\pi$ is optimal and that $P^\pi(X_0=s)>0\ \forall\ s\in S$.
  \par Using the results from (a)-(d) show that
  \[ q(a|s)=0\ \forall \ s\in S,a\in A(s)\setminus D^*(s)\]
\end{question}

\begin{answer}{2) (e)}
  Consider this result
  \[\begin{array}{rl}
    =&\expect^\pi\left[v^*(X_0)-\alpha v^*(X_1)-r(X_0,Y_0)\right]\\
    =&\expect^\pi\left[\max_{Y_0\in A(X_0)}\left(r(X_0,Y_0)+\alpha\sum_{s'\in S}v^*(s')p(s'|X_0,Y_0)\right)-\alpha v^*(X_1)-r(X_0,Y_0)\right]\\
    =&\max_{Y_0\in A(X_0)}\left(r(X_0,Y_0)+\alpha\sum_{s'\in S}v^*(s')p(s'|X_0,Y_0)\right)-\expect^\pi\left[r(X_0,Y_0)+\alpha v^*(X_1)\right]\\
    =&\max_{Y_0\in A(X_0)}\left(r(X_0,Y_0)+\alpha\sum_{s'\in S}v^*(s')p(s'|X_0,Y_0)\right)-\max_{Y_0\in A(X_0)}\big(r(X_0,Y_0)+\alpha\expect^\pi\left[ v^*(X_1)\right]\big)\text{ since }\pi\text{ is optimal}\\
    =&\max_{Y_0\in A(X_0)}\left(r(X_0,Y_0)+\alpha\sum_{s'\in S}v^*(s')p(s'|X_0,Y_0)\right)-\max_{Y_0\in A(X_0)}\left(r(X_0,Y_0)+\alpha\sum_{s'\in S}v^*(s')p(s'|X_0,Y_0)\right)\\
    =&0
  \end{array}\]
  Using this result and 2) (c)
  \[\begin{array}{rcl}
    0&=&\expect^\pi\left[\sum_{a\in A(X_0)}u^*(X_0,a)q(a|X_0)\right]\\
    &=&\sum_{s\in S}\left(\sum_{a\in A(s)}u^*(s,a)q(a|s)\right)\prob(X_0=s)
  \end{array}\]
  By 2) (a), $u^*(s,a)\geq0\ \forall\ s\in S,a\in A(s)$, and $q(a|s)\prob(X_0=s)\geq0$ by the defintion of probability distributions.
  \par Thus all terms in the outer summation are non-negative. Further, as they sum to 0, all terms in the outer summation are 0.
  \par Since the question assumes that $\prob^\pi(X_0=s)>0\ \forall\ s\in S$, it must be that
  \[ 0=\sum_{a\in A(s)}u^*(s,a)q(a|s) \]
  \par This means that the decision probability distribution $q(a|s)$ of our policy $\pi$ does not satisfy 2) (b).
  \par Thus, under our policy there does not exist any $s\in S,a\in(A(s)\setminus D^*(s))$ where $q(a|s)>0$.
  \par Meaning, I can conclude that
  \[ \forall\ s\in S, a\in A(s)\setminus D^*(s),\ q(a|s)=0 \]
  \proved
\end{answer}

\end{document}
