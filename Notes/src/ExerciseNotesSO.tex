\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm}
\usepackage[section,nohyphen]{DomH}
\headertitle{Exercise Notes - Stochastic Optimisation}

\begin{document}

% \questionsfalse
% \answersfalse

\title{Exercise Notes - Stochastic Optimisation}
\author{Dom Hutchinson}
\date{\today}
\maketitle

\begin{question}{1) - Inventory Control}
  \textit{This is from \texttt{LectureSlides3cSO.pdf} and covers chapter 2.}.
  Consider the following \textit{Inventory Control Problem}. Let $n\in\nats$ be the maximum number of stored items allowed.
  \par The inventory is controlled over finte time-periods $\{0,\dots,N-1\}$ for $N\geq2$. At the beginning of each time-period a number of items are demanded by customers and delivered immediately. If more items are demanded than are currently in stock, then the orders are backlogged and satisfied when new items arrive. Backlogged items are satisfied before any new demands. Let $m$ be the maximum number of backlogged items allowed.
  \par Let $Z_t$ denote the number of items demanded by customers at time $t\in T$. We assume $Z_t$ are non-negative IID random variables with $p(k):=\prob(Z_t=k)$. Further, we assume $Z_t$ is independent from the number of items stored at the beginning of period $t=0$ and independent from the number of items currently backlogged.
  \par Let $c(k)$ denote the cost of ordering $k$ new items, $\alpha(k)$ be the cost of storing $k$ un-sold items, $\beta(k)$ be the penalty for having $k$ items in the backlog. If the demand for $k$ items is completely lost then $\gamma(k)$ is paid. We assume $c(k),\alpha(k),\beta(k),\gamma(k)$ are non-negative real numbers for $k\geq1$ and 0 when $k=0$.
  \par Our problem, is to determine, at the beginning of each time-period, how many new items to order whilst minimising total cost.
\end{question}

\begin{question}{1) (a)}
  Formulate the described inventory control problem as a finite-horizon \textit{Markov Decision Problem}
\end{question}

\begin{answer}{1) (a)}
  \begin{itemize}
    \item \textit{Stochsatic System} - Inventory and customers.
    \item \textit{Agent} - Inventory Manager
    \item \textit{Decision Epochs} - The start of each time period.
    \item \textit{Time Horizon} - $T=\{0,\dots,N-1\}$.

    \item \textit{System States}\footnote{System states are an encoding of available system information, which is relevant to the selection of $Y_t$.}
    \par Let $X_t$ denote the system state at epoch $t$, $X_t'$ be the number of items in the inventory at the beginning of period $t$ and $X_t''$ be the number of backlogged items a the beginning of period $t$. We have that if ${X_t''>0\implies 0}$ and ${X_t'>0\implies X_t''=0}$. We want to come up with a formulation for $X_t$, in terms of $X_t'$ and $X_t''$, which carries all sufficient information from $X_t',X_t''$ to make a prediction of $Y_t$ given $X_t$.
    \[ X_t=\begin{cases}
      X_t'&\text{if }X_t''=0\\
      -X_t''&\text{if }X_t''>0
    \end{cases} \]
    This means that
    \[\begin{array}{rcl}
      X_t\geq0&\implies&X_t'=X_t,X_t''=0\\
      X_t<0&\implies&X_t'=0,X_t''=-X_t
    \end{array}\]
    This shows that $X_t'$ and $X_t''$ can be uniquely retrieved from $X_t$.
    \par Since $n$ is the inventory capacity, $X_t'\leq n\implies X_t\leq n$. And, since $m$ is the backlog capacity, $X_t''\leq m\implies X_t\geq-X_t''\geq-m$. This means the state space $S=\{-m,\dots,0,\dots,n\}$.

    \item \textit{Agent Actions}.
    \par Let $Y-t$ denote the action the agent takes at epoch $t$. This is the number of new items ordered at the beginning of time-period $t$.
    \par If $X_t+Y_t>0$ then $X_t+Y_t$ is the number of items stored in the inventory after the arrival of newly ordered items. Since $n$ is the inventory capacity, $X_t+Y_t\leq n$. As $X_t\geq-m$ (See \textit{System States}) we have
    \[ Y_t\leq n-X_t\leq n+m \]
    This means the \textit{Action-Space} is $A=\{0,\dots,n+m\}$.
    \par If $X_t=s$  we have
    \[ Y_t\leq n-X_t=n-s \]
    This means the \textit{Admissible Actions} in state $s$ is $A(s)=\{0,\dots,n-s\}$.

    \item \textit{State Dynamics \& Immediate Costs}.
    The number of items in the invetory at the end of time-period $t$ is $X_t+Y_t-Z_t$.
    We have three cases
    \begin{itemize}
      \item $X_t+Y_t-Z_t\geq0$ - \textit{There is surplus in the inventory}.
      \par This means there are currently $X_{t+1}'=X_t+Y_t-Z_t$ items in storage. Further, there is no excess demand in period $t$, meaning the number of backlogged items at the start of the next period is $X_{t+1}''=0$. No demand was lost in time-period $t$.  We only pay the penalty $\alpha(X_t+Y_t-Z_t)$ for having items in storage. We also pay $c(Y_t)$ for ordering new items. The total cost incurred in time-period $t$ is
      \[ c(Y_t)+\alpha(X_t+Y_t-Z_t) \]
      \par Since $X_{t+1}''=0$, by our definition of $X_t$, we have
      \[ X_{t+1}=X_{t+1}'=X_t+Y_t-Z_t \]

      \item $X_t+Y_t-Z_t\in[-m,0)$ - \textit{There is a backlog but within capacity}.
      \par There are no items in the inventory at the end of period $t$, hence $X_{t+1}'=0'$. So no penalty is paid for storing stock.
      \par Since $X_t+Y_t-Z_t<0$ the excess demand in period $t$ is $-(X_t+Y-t-Z-t)\leq m$, this is within backlog capacity so no demand is lost. Hence the number of backlogged item at the end of time-period $t$ is $X_{t+1}''=-(X_t+Y_t-Z_t)$. We only pay a backlog penalty of $\beta(-(X_t+Y_t-Z_t))$ and a fee of $c(Y_t)$ for the new item. The total cost in period $t$ is
      \[ c(Y_t)+\beta(X_t+Y_t-Z_t) \]
      Since $X_{t+1}''=-(X_t+Y_t-Z_t)>0$, by our definition of $X_t$, we have
      \[ X_{t+1}=-X_{t+1}''=X_t+Y_t-Z_t \]

      \item $X_t+Y_t-Z_t<-m$ -\textit{ There is a backlog and some demand is lost}.
      \par There are no items in the inventory at the end of period $t$, hence $X_{t+1}'=0'$. So no penalty is paid for storing stock.
      \par Since $X_t+Y_t-Z_t<0$ the excess demand in period $t$ is $-(X_t+Y-t-Z-t)>m$, this is beyond backlog capacity so some demand is lost. Hence the number of backlogged item at the end of time-period $t$ is $X_{t+1}''=m$ and lost demand in period $t$ is $-(X_t+Y_t-Z-t)-m$.
      \par We pay a backlog penalty of $\beta(m)$, a lost demand penalty of $\gamma(-(X_t+Y_t-Z_t)-m)$ and pay $c(Y_t)$ to purchase new items. The total cost of time-period $t$ is
      \[ c(Y_t)+\beta(m)+\gamma(-(X_t+Y_t-Z_t)-m) \]
      As $X_{t+1}''=m>0$, by our definition of $X_t$, we have
      \[ X_{t+1}=-X_{t+1}''=-m \]
    \end{itemize}
    By combining these cases we have the state equations
    \[ X_{t+1}=\begin{cases}X_t+Y_t-Z_t&\text{if }X_t+Y_t-Z_t\geq-m\\-m&\text{if }X_t+Y_t-Z_t<-m\end{cases}=\max\{-m,\ X_t+Y_t-Z_t\} \]
    We have the cost incurrent in each period
    \[ G_t=g_t(X_t,Y_t,Z_t)=\begin{cases}
      c(Y_t)+\alpha(X_t+Y_t-Z_t)&\text{if }(X_t+Y_t-Z_t)\geq0\\
      c(Y_t)+\beta(Z_t-X_t-Y_t)&\text{if }(X_t+Y_t-Z_t)\in[-m,0)\\
      c(Y_t)+\beta(m)+\gamma(Z_t-X_t-Y_t-m)&\text{if }(X_t+Y_t-Z_t)<-m
    \end{cases} \]

    \item \textit{Transition Probabilities}.
    \par The definition of transition probabilities is
    \[ p_t(s'|s,a)=\prob^\pi(X_{t+1}=s'|X_t=s,Y_t=a) \]
    We can substitute in the state-equation
    \[\begin{array}{rcl}
      p_t(s'|s,a)&=&\prob^\pi(\max\{-m,\ X_t+Y_t-Z_t\}=s'|X_t=s,Y_t=a)\\
      &=&\prob^\pi(\max\{-m,\ s+a-Z_t\}=s'|X_t=s,Y_t=a)
    \end{array}\]
    $X_t,Y_t$ both represent values at the beginning of time-period $t$ and thus independent of $Z_t$, but dependent upon $Z_0,\dots,Z_{t-1}$. As $Z_t$ is independent of $Z_0,\dots,Z_{t-1}$ (from question) we conclude that $Z_t$ is independent of $X_t$ and $Y_t$. This means the following events are independent
    \[ \big\{\max\{s+a-Z_t,-m\}=s'\big\}\quad\big\{X_t=s,Y_t=a\big\} \]
    Therefore,
    \[ p_t(s'|s,a)=\prob^\pi(\max\{-m,\ s+a-Z_t\}=s') \]
    As demand $Z_t$ is independent of our policy $\pi$, we further have
    \[ p_t(s'|s,a)=\prob(\max\{-m,\ s+a-Z_t\}=s') \]
    We consider three cases
    \begin{itemize}
      \item $s'>s+a$.
      \par We need to compute $p_t(s'|s,a)=\prob(\max\{-m,\ s+a-Z_t\}=s')$.
      \par We want to determine the probability of $s'=\max\{-m,\ s+a-Z_t\}$. We have $s+a-Z_t\leq s+a$, as $Z_t\geq0$ by the question, consequently $\max\{s+a-Z_t,-m\}\leq\max\{s+a,-m\}$.
      \par We have $s+a\geq-m$, as $s\in\{-m,\dots,n\}$ and $a\in\{0,\dots,n-s\}$. Consequently,
      \[ \max\{s+a,-m\}=s+a \]
      Combining these we get
      \[\begin{array}{rcl}
        s+a&<&s'=\max\{s+a-Z_t,-m\}\\
        &\leq&\max\{s+a,-m\}\\
        &=&s+a
      \end{array}\]
      Hence, it is impossible for $s'=\max\{-m,\ s+a-Z_t\}$. Thus
      \[ p_t(s'|s,a)=\prob(\max\{-m,\ s+a-Z_t\}=s')=0 \]

      \item $s'\in(-m,s+a]$.
      \par We need to compute $p_t(s'|s,a)=\prob(\max\{-m,\ s+a-Z_t\}=s')$.
      \par Again, we want to determine the probability of $s'=\max\{-m,\ s+a-Z_t\}$. As $s'>-m$ we have
      \[ \max\{s+a-Z_t,-m\}=s+a-Z_t \implies s'=s+a-Z_t \]
      Hence, the result only holds if $Z_t=s+a-s'$. This gives transition probability
      \[\begin{array}{rcl}
        p_t(s'|s,a)&=&\prob(\max\{s+a-Z_t,-m\}=s')\\
        &=&\prob(Z_t=s+a-s')\\
        &=&p(s+a-s')
      \end{array}\]

      \item $s'=-m$.
      \par We need to compute $p_t(s'|s,a)=\prob(\max\{-m,\ s+a-Z_t\}=s')$.
      \par Again, we want to determine the probability of $s'=\max\{-m,\ s+a-Z_t\}$. As $s'=-m$ we have
      \[ s+a-Z_t\leq-m \]
      Hence, the result holds iff $Z_t\geq s+a+m$. This gives transition probability
      \[\begin{array}{rcl}
        p_t(s'|s,a)&=&\prob(\max(s+a-Z-t,-m)=s')\\
        &=&\prob(Z_t\geq s+a+m)\\
        &=&\sum_{k=s+a+m}^\infty\prob(Z_t=k)\\
        &=&\sum_{k=s+a+m}^\infty p(Z_t=k)
      \end{array}\]
    \end{itemize}

    \item \textit{Equivalent Rewards}.
    \par We want to select a policy $\pi\in HR(T)$ which minimises the expected total cost
    \[ \expect^\pi\left[\sum_{t=0}^{N-1}G_t\right]=\expect^\pi\left[\sum_{t=0}^{N-1}g_t(X_t,Y_t,Z_t)\right] \]
    This expectation depends on the demand $Z_0,\dots,Z_{N-1}$, thus minimising it does not fit the framework of a \textit{Markov Decision Problem}. Therefore we transform the expected total cost to an equivalent (but more convenient) form
    \[\begin{array}{rcl}
      \expect^\pi\left[\sum_{t=0}^{N-1}G_t\right]&=&\sum_{t=0}^{N-1}\expect^\pi[G_t]\\
      &=&\sum_{t=0}^{N-1}\expect^\pi\big[\expect^\pi[G_t|X_t,Y_t]\big]\text{ by Tower property}\\
      &=&\sum_{t=0}^{N-1}\expect^\pi\big[\expect^\pi[g_t(X_t,Y_t,Z_t)|X_t,Y_t]\big]
    \end{array}\]
    Define $r_t(s,a)$ and $r_N(s)$ to be
    \[\begin{array}{rcl}
      r_t(s,a)&=&-\expect^\pi[g_t(X_t,Y_t,Z_t)|X_t=s,Y_t=a]\\
      r_N(s)&=&0
    \end{array}\]
    Since $Z_t$ is independent of $(X_t,Y_t)$ we have
    \[ r_t(s,a)=-\expect^\pi[g_t(s,a,Z_t)|X_t=s,Y_t=a]=-\expect^\pi[g_t(s,a,Z_t)] \]
    Substituting these definitions into our formulation for total expected cost yields
    \[ \expect^\pi\left[\sum_{t=0}^{N-1}G_t\right]=-\expect^\pi\left[\sum_{t=0}^{N-1}r_t(X_t,Y_t)+r_N(X_N)\right] \]
    Minimising this expression is equivalent to maximising the following
    \[ \expect^\pi\left[\sum_{t=0}^{N-1}r_t(X_t,Y_t)+r_N(X_N)\right] \]
    This is congrent with the framework of a \textit{Markov Decision Problem}. $r_t(s,a)$ can be viewed as the equivalent reward at epoch $t$ and $r_N(s)$ as the equivalent terminal reward.
    Since demand $Z_t$ is not affected by our policy $\pi$ we have
    \[ r_t(s,a)=-\expect[g_t(s,a,Z_t)]=-\sum_{k=0}^\infty g_t(s,a,k)p(k) \]
    Further, by considering the full breakdown of the cost function we have
    \[\begin{array}{rcl}
      r_t(s,a)&=&-c(a)-\sum_{k=0}^{s+a}\alpha(s+a-k)p(k)-\sum_{k=s+a+1}^{s+a+m}\beta(k-s-a)p(k)\\
      &-&\sum_{k=s+a+m+1}^\infty\big(\beta(m)+\gamma(k-s-a-m)\big)p(k)
    \end{array}\]

    \item \textit{Equivalent Objective}.
    \par Find a policy $\pi\in HR(T)$ which maximises
    \[ \expect^\pi\left[\sum_{t=0}^{N-1}r_t(X_t,Y_t)+r_N(X_N)\right] \]
  \end{itemize}
\end{answer}

\begin{question}{1) (b)}
  By considering the markov decision problem formulated in \texttt{2) (a)}  and assume the following
  \begin{itemize}
    \item $N=2,n=1,m=1$.
    \item $p(0)=p(1)=p(2)=p(3)=.25$ and $p(k)=0$ if $k\geq0$.
    \item $c(k)=ck,\ \alpha(k)=\alpha k,\ \beta(k)=\beta k,\ \gamma(k)=\gamma k$ for $k\geq 0$.
    \item $c=1,\ \alpha=2,\ \beta=3,\ \gamma=4$.
  \end{itemize}
  Find an optimal policy $\pi^*$
\end{question}

\begin{answer}{1) (b)}
  From \texttt{3) (a)} we can quickly derive this formulation by substituting in the values specified.
  \begin{itemize}
    \item \textit{Number of Epochs} - $N=2$.
    \item \textit{Time-Horizon} - $T=\{0,1\}$.
    \item \textit{State-Space} - $S=\{-1,0,1\}$.
    \item \textit{Action-Space} - $A=\{0,1,2\}$.
    \item \textit{Admissible Action-Space}
    \[\begin{array}{rcl}
      A(-1)&=&\{0,1,2\}\\
      A(0)&=&\{0,1\}\\
      A(1)&=&\{0\}
    \end{array}\]
    \item \textit{Rewards}
    \begin{center}
      \begin{tabular}{rl}
        $r_t(s,a)=$&
        \begin{tabular}{c|ccc}
          $s$\textbackslash $a$&0&1&2\\
          -1&-9&-25/4&-5\\
          0&-21/4&-5&ND\\
          1&-3&ND&ND
        \end{tabular}
      \end{tabular}
    \end{center}
    ND denotes not-defined, since $r_t(s,a)$ is not defined if $a\not\in A(s)$.
    \item \textit{Terminal Reward} $r_2(s)=0$.
    \item \textit{Transition Probabilities}
    \begin{center}
      \begin{tabular}{rl}
        $p_t(s'|s,0)=$&
        \begin{tabular}{c|ccc}
          $s$\textbackslash $s'$&-1&0&1\\\hline
          -1&1&0&0\\
          0&3/4&1/4&0\\
          1&1/2&1/4&1/4
        \end{tabular}\\
        $p_t(s'|s,1)$&
        \begin{tabular}{c|ccc}
          $s$\textbackslash $s'$&-1&0&1\\\hline
          -1&3/4&1/4&0\\
          0&1/2&1/4&1/4\\
          1&ND&ND&ND
        \end{tabular}\\
        $p_t(s'|s,2)$&
        \begin{tabular}{c|ccc}
          $s$\textbackslash $s'$&-1&0&1\\\hline
          -1&1/2&1/4&1/4\\
          0&ND&ND&ND\\
          1&ND&ND&ND
        \end{tabular}
      \end{tabular}
    \end{center}
  \end{itemize}
  To find the optimal policy we use the \textit{Dynamic Programming Algorithm} which is defined as
  \[\begin{array}{rcl}
    u_t^*(s)&=&\max_{a\in A(s)}\left(r_t(s,a)+\sum_{s'\in S}u_{t+1}^*(s')p_t(s'|s,a)\right)\\
    d_t^*(s)&\in&\argmax_{a\in A(s)}\left(r_t(s,a)+\sum_{s'\in S}u_{t+1}^*(s')p_t(s'|s,a)\right)
  \end{array}\]
  where $t=N-1,\dots,0$ and $u_N^*(s)=r_N(s)$.
  For simplicity I will use the following to denote the expression we are maximising
  \[ w_t^*(s,a):=r_t(s,a)+\sum_{s'\in S}u_{t+1}^*(s')p_t(s'|s,a) \]
  \par For this specific scenario we have two epochs to consider
  \begin{itemize}
    \item Epoch $t=1$
    \par We need to compute $u_1^*(s),d_1^*(s)$. We have
    \[ w_1^*(s,a)=r_1(s,a)+r_2(-1)p_1(-1|s,a)+r_2(0)p_1(0|s,a)+r_2(1)p_1(1|s,a) \]
    This gives the following tables of results
    \begin{tabular}{lr}
      $w_1^*(s,a)=$&
      \begin{tabular}{c|ccc}
        $s$\textbackslash $a$&0&1&2\\\hline
        -1&-9&-25/4&-5\\
        0&-21/4&-4&ND\\
        1&-3&ND&ND
      \end{tabular}\\
      &
      \begin{tabular}{c|c|c}
        $s$&$u_1^*(s)$&$d_1^*(s)$\\
        -1&-5&2\\
        0&-4&1\\
        1&-3&0
      \end{tabular}
    \end{tabular}
    The optimal strategy depends on what state the system is in.

    \item Epoch $t=0$
    \par We need to compute $u_0^*(s),d_0^*(s)$. We have
    \[ w_0^*(s,a)=r_0(s,a)+u_1^*(-1)p_0(-1|s,a)+u_1^*(0)p_0(0|s,a)+u_1^*(1)p_0(1|s,a) \]
    This gives the following tables of results
    \begin{tabular}{lr}
      $w_0^*(s,a)=$&
      \begin{tabular}{c|ccc}
        $s$\textbackslash $a$&0&1&2\\\hline
        -1&-14&-11&-37/4\\
        0&-10&-33/4&ND\\
        1&-29/4&ND&ND
      \end{tabular}\\
      &
      \begin{tabular}{c|c|c}
        $s$&$u_0^*(s)$&$d_0^*(s)$\\
        -1&-37/4&2\\
        0&-33/4&1\\
        1&-29/4&0
      \end{tabular}
    \end{tabular}
    The optimal strategy depends on what state the system is in.
    \item \textit{Optimal Policy} - $\pi^*=(d_0^*(s),d_1^*(s))$ with $d_0^*(s),d_1^*(s)$ as defined above.
    \item \textit{Optimal Value Function} - $u_0^*(s)$ as defined above.
  \end{itemize}
\end{answer}

\newpage
\begin{question}{2) - Discount Reward Inventory Problem}
  \textit{This is from \texttt{LectureSlides4cSO.pdf} and covers chapter 3.}.
  \par This is an \textit{Inventory Control Problem} with discounted-cost (See \texttt{Question 1)} for the finite-time version).
  \par Our problem is to decide, at the beginning of each time-period, what number of items to order so the expected discounted cost is minimal.
\end{question}

\begin{question}{2) a)}
  Formulate the inventory control problem as a \textit{Discounted Reward Markov Decision Problem}.
\end{question}

\begin{answer}{2) a)}
  \begin{itemize}
    \item \textit{Stochastic System} - Inventory and customers.
    \item \textit{Agent} - Inventory Manager.
    \item \textit{Decision Epochs} - Beginning of the time-period.
    \item \textit{Epoch $t$} - The beginning of time-period $t$.
    \item \textit{Time Horizon} - $T=\{0,1,\dots\}$.
    \item \textit{System States}.
    \par Let $X_t$ be the system state at epoch $t$, $X_t'$ be the number of items in the inventory at the beginning of period $t$ and $X_t''$ be the number of backlogged items at the beginning of period $t$.
    \[ X_t':=\begin{cases}X_t'&\text{if }X_t''=0\\-X_t''\text{if }X_t''>0\end{cases} \]
    \item \textit{State-Space} - $T=\{-m,\dots,0,\dots,n\}$.
    \item \textit{Agent-Actions}.
    \par Let $Y_t$ be the agent action at epoch $t$. Define $Y_t$ as the number of items ordered at the beginning of time-period $t$.
    \item \textit{Action-Space} - $A=\{0,\dots,n+m\}$.
    \item \textit{Admissible Actions} - $A(s)=\{0,\dots,n-s\}$.
    \item \textit{Transition Probabilities}.
    \par Let $s\in S,\ a\in A(s)$. The corresponding transition probabilities are
    \[ p(s'|s,a)\begin{cases}
      0&\text{if }s'>s+a\\
      p(s+a-s')&\text{if }s'\in(-m,s+a]\\
      \sum_{k=s+a+m}^\infty p(k)&\text{if }s'=m
    \end{cases} \]
    \item \textit{Equivalent Rewards}.
    \par Let $s\in S,\ a\in A(s)$. The corresponding equivalent reward is
    \[\begin{array}{rcl}
      r(s,a)&=&=c(a)+\sum_{k=0}^{s+a}\alpha(s+a-k)p(k)-\sum_{k=s+a+1}^{s+a+m}\beta(k-s-a)p(k)\\
      &-&\sum_{k=s+a+m+1}^\infty\left[\beta(m)+\gamma(k-s-a-m)\right]p(k)
      \end{array}\]
    \item \textit{Equivalent Objective}.
    \par Find the policy, $\pi\in HR(T)$ which maximises the expected discount reward
    \[ \expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right]\quad\alpha\in(0,1) \]
  \end{itemize}
\end{answer}

\begin{question}{2) b)}
  Consider the MDP formulated in \texttt{4) a)}. Assume the following
  \begin{itemize}
    \item $n=1,m=1,\alpha=.5$.
    \item $p(0)=p(1)=p(2)=p(3)=\frac14$ and $p(k)=0\ \forall\ k\geq4$.
    \item $c(k)=ck,\alpha(k)=\alpha k,\beta(k)=\beta(k),\gamma(k)=\gamma k\ \forall\ k\geq 0$.
    \item $c=1,\alpha=2,\beta=3,\gamma=4$.
  \end{itemize}
  Using the policy iteration algorithm, find an algorithm policy.
\end{question}

\begin{answer}{2) b)}
  For these specific conditions we have the following
  \begin{itemize}
    \item \textit{Number of Epochs} - $N=\infty$.
    \item \textit{Time-Horizon} - $T=\{0,1,\dots\}$.
    \item \textit{State-Space} - $S=\{-1,0,1\}$.
    \item \textit{Action-Space} - $A=\{0,1,2\}$.
    \item \textit{Admissible Actions}
    \[\begin{array}{rcl}
      A(-1)&=&\{0,1,2\}\\
      A(0)&=&\{0,1\}\\
      A(1)&=&\{0\}
    \end{array}\]
    \item \textit{Rewards}
    \begin{center}$r(s,a)=$
      \begin{tabular}{c|ccc}
        $s$\textbackslash$a$&0&1&2\\\hline
        -1&-9&-25/4&-5\\
        0&-21/5&-4&ND\\
        1&-3&ND&ND
      \end{tabular}
    \end{center}
    \item \textit{Transition Probabilties}
    \begin{center}
      \begin{tabular}{rcl}
        $p(s'|s,0)$&=&
        \begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&1&0&0\\
          0&3/4&1/4&0\\
          1&1/2&1/4&1/4
        \end{tabular}\\
        $p(s'|s,1)$&=&
        \begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&3/4&1/4&0\\
          0&1/2&1/4&1/4\\
          1&ND&ND&ND
        \end{tabular}\\
        $p(s'|s,2)$&=&
        \begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&1/2&1/4&1/4\\
          0&ND&ND&ND\\
          1&ND&ND&ND
        \end{tabular}
      \end{tabular}
    \end{center}
    \item \textit{Policy Evaluation Step}
    \par In the $k^{th}$ iteration, we compute value function $v_k(s)$ where $v_k(s)$ is the solution to the following equations
    \[\begin{array}{rcl}
      v(s)&=&(T_dv)(s)\\
      &=&r(s,d_k(s))+\alpha\sum_{s'\in S}v(s')p(s'|s,d_k(s))
    \end{array}\]
    \item \textit{Policy Improvement Step}
    \par In the $k^{th}$ iteration, we compute the decision function $d_{k+1}(s)$
    \[ d_{k+1}(s)\in\argmax_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}v_k(s')p(s'|s,a)\right) \]
    \item \textit{Alternative Form of Policy Improvement Step}
    \[\begin{array}{rcl}
      w_{k+1}(s,a)&=&r(s,a)+\alpha\sum_{s'\in S}v_k(s')p(s'|s,a)\\
      d_{k+1}(s)&\in&\argmax_{a\in A(s)}w_{k+1}(s,a)
    \end{array}\]
    \item \textit{Initialisation}.
    \par We set
    \[\begin{array}{rcl}
      d_0(-1)&=&2\\
      d_0(0)&=&1\\
      d_0(1)&=&0
    \end{array}\]
    $d_0(s)$ can be any \textit{Markovian Decision Function} which satisfies $d_0(s)\in A(s)\ \forall\ s\in S$.
    \item \textit{Iteration} - $k=0$.
    \par \underline{Policy Evaluation}
    \par We compute solution $v_0(s)$ for the following system of equations
    \[\begin{array}{rcl}
      v(s)&=&(T_{d_0}v)(s)\\
      &=&r(s,d_0(s))+\alpha\sum_{s'=-1}^1v(s')p(s'|s,d_0(s))
    \end{array}\]
    where $v(s)$ is unknown and $s\in\{-1,0,1\}=S$. We can expand this equation as
    \[\begin{array}{rcl}
      v(-1)&=&r(-1,d_0(-1))+\alpha\sum_{s'=-1}^1v(s')p(s'|-1,d_0(-1))\\
      v(0)&=&r(0,d_0(0))+\alpha\sum_{s'=-1}^1v(s')p(s'|0,d_0(0))\\
      v(1)&=&r(1,d_0(1))+\alpha\sum_{s'=-1}^1v(s')p(s'|1,d_0(1))\\
    \end{array}\]
    This has solutions
    \[\begin{array}{rcl}
      v_0(-1)&=&-37/4\\
      v_0(0)&=&-33/4\\
      v_0(1)&=&-29/4
    \end{array}\]
    \underline{Policy Improvement}
    \par We compute $d_1(s)$ using the following system of equations
    \[\begin{array}{rcl}
      w_1(s,a)&=&r(s,a)+\alpha\sum_{s'=-1}^1v_0(s')p(s'|s,a)\\
      d_1(s)&\in&\argmax_{a\in A(s)}w_1(s,a)
    \end{array}\]
    The table below summarises the values for these equations
    \begin{center}
      \begin{tabular}{rcl}
        $w_1(s,a)$&=&\begin{tabular}{c|ccc}
          $s$\textbackslash$a$&0&1&2\\\hline
          -1&-109/8&-43/4&-37/4\\
          0&-39/4&-33/4&ND\\
          1&-29&ND&ND
        \end{tabular}\\
        $d_1(s)$&=&\begin{tabular}{c|c}
          $s$&$d_1(s)$\\\hline
          -1&2\\
          0&1\\
          1&0
        \end{tabular}
      \end{tabular}
    \end{center}
    \underline{Stopping Criterion} - As $d_1(s)=d_0(s)\ \forall\ s\in S$, then $d_1(s)$ is optimal.
    \item \textit{Optimal Solution}.
    \par The \textit{Optimal Markovian Decision Function} is $d_0(s)$ and the \textit{Optimal Value Function} is $v_0(s)$.
  \end{itemize}
\end{answer}

\begin{question}{2) c)}
  Formulate a linear problem equivalent to the MDP solved in \texttt{4) b)}.
\end{question}

\begin{answer}{2) c)}
  The \textit{Equivalent Linear Program} is to \underline{minimise}
  \[ \sum_{s'=-1}^s\gamma(s')v(s')\quad\text{wrt }v(-1),v(0),v(1) \]
  subject to the condition that
  \[ r(s,a)+\alpha\sum_{s'=-1}^1v(s')p(s'|s,a)\leq v(s) \]
  where $\gamma(-1),\gamma(0),\gamma(1)\in(0,\infty)$ and are constants.
  \par This condition can be expanded and restated as
  \[\begin{array}{rcl}
    r(-1,0)+\alpha\sum_{s'=1}^1v(s')p(s'|-1,0)\leq v(-1)\\
    r(-1,1)+\alpha\sum_{s'=1}^1v(s')p(s'|-1,1)\leq v(-1)\\
    r(-1,2)+\alpha\sum_{s'=1}^1v(s')p(s'|-1,2)\leq v(-1)\\
    r(0,0)+\alpha\sum_{s'=1}^1v(s')p(s'|0,0)\leq v(0)\\
    r(0,1)+\alpha\sum_{s'=1}^1v(s')p(s'|0,1)\leq v(0)\\
    r(1,0)+\alpha\sum_{s'=1}^1v(s')p(s'|1,0)\leq v(0)
  \end{array}\]
\end{answer}

\newpage
\begin{question}{3) - Average Reward Inventory Problem}
  \textit{This is from \texttt{LectureSlides5cSO.pdf} and covers chapter 4.}.
  \par This is an \textit{Inventory Control Problem} with discounted-cost (See \texttt{Question 1)} for the finite-time version).
  \par Our problem is to decide, at the beginning of each time-period, what number of items to order so the expected average cost is minimal.
\end{question}

\begin{question}{3) (a)}
  Formulate the considered inventory control problem as an \textit{Average Reward MDP}
\end{question}

\begin{answer}{3) (a)}
  \begin{itemize}
    \item \textit{Decision Epochs} - Beginning of each time-period.
    \item \textit{Time-Horizon} - $T=\{0,1,\dots\}$.
    \item \textit{System States}.
    \par Let $X_t$ be the system state in epoch $t$; $X_t'$ be the number of items kept in the inventory at the beginning of period $t$; $X_t''$ be the number of backlogged items at the beginning of period $t$.
    \[ X_t:=\begin{cases}X_t'&\text{if }X_t''=0\\-X_t''&\text{if }X_t''>0\end{cases} \]
    \item \textit{State-Space} - $S=\{-m,\dots,0,n\}$.
    \item \textit{Agent Actions}.
    \par Let $Y_t$ be the number of items ordered at the beginning of time-period $t$.
    \item \textit{Action-Space} - $A=\{0,\dots,n+m\}$.
    \item \textit{Admissible Actions} -  $A(s)=\{0,\dots,n-s\}$.
    \item \textit{Transition Probabilities}.
    \[ p(s'|s,a)=\begin{cases}0&\text{if }s'>s+a\\p(s+a-s')&\text{if }s'\in(-m,s+a]\\\sum_{k=s+a+m}^\infty p(k)&\text{if }s'=m\end{cases} \]
    \item \textit{Equivalent Rewards}.
    \[\begin{array}{rcl}
      r(s,a)&=&-c(a)-\sum_{k=0}^{s+a}\alpha(s+a-k)p(k)\\
      &-&\sum_{k=s+a+1}^{s+a+m}\beta(k-s-a)p(k)\\
      &-&\sum_{k=s+a+m+1}^\infty\big(\beta(m)+\gamma(k-s-a-m)\big)p(k)
    \end{array}\]
    \item \textit{Equivalent Objective}.
    \par Find $\pi\in HR(T)$ st
    \[ \pi=\argmax_\pi\lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right] \]
  \end{itemize}
\end{answer}

\begin{question}{3) (b)}
  Consider the \textit{MDP} formulated in \texttt{3) (a)}. Assume the following
  \begin{itemize}
    \item $n=1,\ m=1,\ \alpha=0.5$.
    \item $p(k)=\begin{cases}\frac14&\text{if }k\in[1,4]\\0&\text{otherwise}\end{cases}$.
    \item $c(k)=k,\ \alpha(k)=2k,\ \beta(k)=3k,\ \gamma(k)=4k$ for $k\geq0$.
  \end{itemize}
  Use the \textit{Policy Iteration Algorithm} to find an optimal policy
\end{question}

\begin{answer}{3) (b)}
  \begin{itemize}
    \item \textit{Time-Horizon} - $T=\{0,1,\dots\}$.
    \item \textit{State-Space} - $S=\{-1,0,1\}$.
    \item \textit{Action-Space} - $A=\{0,1,2\}$.
    \item \textit{Admissible Actions}.
    \[\begin{array}{rcl}
      A(-1)&=&\{0,1,2\}\\
      A(0)&=&\{0,1\}\\
      A(1)&=&\{0\}
    \end{array}\]
    \item \textit{Immediate Rewards}.
    \begin{center}
      $r(s,a)=$\begin{tabular}{c|ccc}
        $s$\textbackslash$a$&0&1&2\\\hline
        -1&-9&-25/4&-5\\
        0&-21/4&-4&ND\\
        1&-3&ND&ND
      \end{tabular}
    \end{center}
    \item \textit{Transition Probabilities}.
    \begin{center}
      \begin{tabular}{rcl}
        $p(s'|s,0)$&=&\begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&1&0&0\\
          0&3/4&1/4&0\\
          1&1/2&1/4&1/4
        \end{tabular}
        \\\\
        $p(s'|s,1)$&=&\begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&3/4&1/4&0\\
          0&1/2&1/4&1/4\\
          1&ND&ND&ND
        \end{tabular}
        \\\\
        $p(s'|s,2)$&=&\begin{tabular}{c|ccc}
          $s$\textbackslash$s'$&-1&0&1\\\hline
          -1&1/2&1/4&1/4\\
          0&ND&ND&ND\\
          1&ND&ND&ND
        \end{tabular}
      \end{tabular}
    \end{center}
  \end{itemize}
  Now I apply the \textit{Policy Iteration Algorithm}
  \begin{itemize}
    \item \textit{Initialisation} - Define the following Markovian Decision Function
    \[\begin{array}{rcl}
      d_0(-1)&=&2\\
      d_0(0)&=&1\\
      d_0(1)&=&0
    \end{array}\]
    \item \textit{Iteration} $k=0$.
    \item \textit{Policy Evaluation} - We compute a solution $\mu_0(s)$ to the following system of equations
    \[\begin{array}{rcl}
      \sum_{s'\in S}p_0(s|s')\mu(s')&=&\mu(s)\ \forall\ s\in S\\
      \sum_{s'\in S}\mu(s')&=&1
    \end{array}\]
    where $\mu(s)$ is unknown and $p_0(s'|s)=p(s'|s,d_0(s))$. We can derive the following three equations
    \[\begin{array}{rrcl}
      i)\quad&p(-1|-1)\mu(-1)+p(-1|0)\mu(0)+p(-1|1)\mu(1)&=&\mu(-1)\\
      ii)\quad&p(0|-1)\mu(-1)+p(0|0)\mu(0)+p(0|1)\mu(1)&=&\mu(0)\\
      iii)\quad&p(1|-1)\mu(-1)+p(1|0)\mu(0)+p(1|1)\mu(1)&=&\mu(1)\\\\
      \implies&\frac12\mu(-1)+\frac12\mu(0)+\frac12\mu(1)&=&\mu(-1)\text{ from }i)\\
      \implies&\mu(0)+\mu(1)&=&\mu(-1)\\
      \implies&\mu(-1)&=&\frac12\text{ as sum to 1}\\\\
      \implies&\frac14\mu(-1)+\frac14\mu(0)+\frac14\mu(1)&=&\mu(-1)\text{ from }ii)\\
      \implies&\mu(-1)+\mu(1)&=&3\mu(0)\\
      \implies&\frac12+\mu(0)+\left(3\mu(0)-\frac12\right)&=&1\text{ as sum to 1}\\
      \implies&\mu(0)&=&\frac14\\
      \implies&\mu(1)&=&\frac14\text{ as sum to 1}\\
    \end{array}\]
    We compute solutions $w_0(s)$ to the system of equations
    \[\begin{array}{rcl}
      w(s)-\sum_{s'\in S}p_0(s|s')w(s')&=&r_0(s)-\bar{r}(0)\ \forall\ s\in S\\
      \sum_{s'\in S}w(s')\mu_0(s')&=&0
    \end{array}\]
    where $w(s)$ is unknown and $r_0(s),\bar{r}_0$ are defined as
    \[\begin{array}{rcl}
      r_0(s)&=&r(s,d_0(s))
      =\begin{cases}
           -5&\text{if }s=-1\\
           -4&\text{if }s=0\\
           -3&\text{if }s=1
         \end{cases}\\
      \bar{r}_0&=&\sum_{s'\in S}r_0(s')\mu(s')\\
      &=&(-5)\cdot\frac12+(-4)\cdot\frac14+(-3)\cdot\frac14=-9/2
    \end{array}\]
    I can't be asked to do it, but the solution is
    \[\begin{array}{rcl}
      w_0(-1)&=&-3/4\\
      w_0(0)&=&-1/4\\
      w_0(1)&=&-5/4
    \end{array}\]
    \item \textit{Policy Improvement}.
    \par Compute $d_1(s)$ using the following equations
    \[\begin{array}{rcl}
      u_1(s,a)&=&r(s,a)+\sum_{s'\in S}w_0(s')p(s'|s,a)\\
      d_1(s)&\in&\argmax_{a\in A(s)}u_1(s,a)
    \end{array}\]
    This gives values
    \begin{center}
      \begin{tabular}{rcl}
        $u_1(s,a)$&=&\begin{tabular}{c|ccc}
            $s$\textbackslash$a$&0&1&2\\\hline
            -1&-39/4&-27/4&-5\\
            0&-23/4&-4&ND\\
            1&-3&ND&ND&ND
        \end{tabular}\\
        $u_1(s,a)$&=&\begin{tabular}{c|cc}
            $s$&$d_1(s)$\\\hline
            -1&2\\
            0&1\\
            1&0
        \end{tabular}
      \end{tabular}
    \end{center}
    \item As $d_1(s)=d_0(s)\ \forall\ s\in S$ the stopping condition is met.
  \end{itemize}
\end{answer}

\begin{question}{3) (c)}
  Formulate a linear program equivalent to the average reward problem solve in \texttt{3) (b)}.
\end{question}

\begin{answer}{3) (c)}
  Minimise $t$ wrt $r,w(-1),w(0),w(1)$\\
  Subject to
  \[ r(s,a)+\sum_{s'\in S}w(s')p(s'|s,a)\leq r+w(s)\quad s\in S,a\in A(s) \]
\end{answer}

\end{document}
