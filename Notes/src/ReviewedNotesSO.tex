\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm,graphicx,tikz}
\usetikzlibrary{automata,positioning}
\graphicspath{ {img/} }
\usepackage[section,nohyphen]{DomH}
\headertitle{Stochastic Optimisation - Reviewed Notes}

\begin{document}

\title{Stochastic Optimisation - Reviewed Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

\tableofcontents\newpage

\section{Probability}

\subsection{Probabilistic Inequalities}

  \begin{theorem}{Markov's Inequality}
    Let $X$ be a \underline{non-negative} random variable.\\
    \textit{Markov's Inequality} states
    \[ \forall\ c>0\quad\prob(X\geq c)\leq\frac{\expect(X)}c \]
  \end{theorem}

  \begin{proof}{Markov's Inequality}
    Let $X$ be a non-negative random variable and fix $c>0$.\\
    Consider partitioning the expectation of $X$ around the value $c$.
    \[ \expect(X)=\prob(X<c)\cdot\expect[X|X<c]+\prob(X\geq c)\cdot\expect[X|X\geq c] \]
    Note that $\expect[X|X<c]>0$ since $X$ is non-negative and $\expect[X|X\geq c]\geq c$ since it only considers the cases where $X\geq c$. Thus
    \[ \expect(X)\geq\prob(X<c)\cdot0+\prob(X\geq c)\cdot c \]
    Rearranging we get the result of the theorem.
    \[ \prob(X\geq c)\leq\frac{\expect(X)}c \]
    \proved
  \end{proof}

  \begin{theorem}{Chebyshev's Inequality}
    Let $X$ be a random-variable with \underline{finite} mean $\mu$ and variance $\sigma^2$.\\
    \textit{Chebyshev's Inequality} states
    \[ \forall\ c>0\quad\prob(|X-\mu|\geq c)\leq\frac{\sigma^2}{c^2}\]
    Further for $X_1,\dots,X_n$ IID RVs with \underline{finite} mean $\mu$ and variance $\sigma^2$. We have
    \[ \forall\ c>0\quad\prob\left(\left|\left(\sum_{i=1}^nX_i\right)-n\mu\right|\geq nc\right)\leq\frac{\sigma^2}{nc^2} \]
  \end{theorem}

  \begin{proof}{Chebyshev's Inequality - Single Random Variable}
    Let $X$ be a random-variable with \underline{finite} mean $\mu$ and variance $\sigma^2$, and fix $c>0$.\\
    Define random variable $Y:=(X-\mu)^2$, noting that $Y$ is non-negative and $\expect[Y]=\expect\left[(X-\mu)^2\right]=:\var(X)=\sigma^2$.\\
    By \textit{Markov's Inequality} we have that
    \[ \prob(Y\geq c^2)\leq\frac{\expect(Y)}{c^2}=\frac{\var(X)}{c^2} \]
    Note that the event $\{Y\geq c^2\}=\{(X-\mu)^2\geq c^2\}$ is equivalent to the event $\{|X-\mu|\geq c\}$ since $c>0$.\\
    Substituting this result into the above expression gives the result of the theorem.
    \[ \prob(|X-\mu|\geq c)\leq\frac{\expect(Y)}{c^2}=\frac{\var(X)}{c^2} \]
    \proved
  \end{proof}

  \begin{proof}{Chebyshev's Inequality - Sum of IID Random Variables}
    Let $X_1,\dots,X_n$ be IID RVs with \underline{finite} mean $\mu$ and variance $\sigma^2$.\\
    Define random variable $Y:=\sum_{i=1}^nX_i$. Note that
    \[\begin{array}{rclcrcll}
      \expect(Y)&=&\expect\left(\sum_{i=1}^nX_i\right)&\quad&\var(Y)&=&\var\left(\sum_{i=1}^nX_i\right)\\
      &=&\sum_{i=1}^n\expect(X_i)&&&=&\sum_{i=1}^n\var(X_i)&\text{ by independence}\\
      &=&n\mu&&&=&n\sigma^2&\text{ by identical distribution}
    \end{array}\]
    By applying \textit{Chebyshev's Inequality} to $Y$ bounded by $(nc)^2$, we get
    \everymath={\displaystyle}
    \[\begin{array}{rrcl}
    &\prob\left(\left|Y-\expect(Y)\right|\geq c\right)&\leq&\frac{\var(Y)}{(nc)^2}\\
    \implies&\prob\left(\left|\left(\sum_{i=1}^nX_i\right)-n\mu\right|\geq c\right)&\leq&\frac{n\sigma^2}{(nc)^2}=\frac{\sigma^2}{nc^2}
    \end{array}\]
    The result of the theorem for the sum of IID RVs.\proved
  \end{proof}

  \begin{theorem}{Chernoff Bounds}
    Let $X$ be a random variable whose moment-generating function $\expect[e^{\theta X}]$ is finite $\forall\ \theta$.\\
    \textit{Chernoff Bounds} state
    \[ \forall\ c\in\reals\quad\prob(X\geq c)\leq\inf_{\theta>0}e^{-\theta c}\expect[e^{\theta X}]\quad\text{and}\quad\prob(X\leq c)\leq\inf_{\theta<0}e^{-\theta c}\expect[e^{\theta X}) \]
    Further for $X_1,\dots,X_n$ IID RVs with finite moment-generating functions $\forall\ \theta$. We have
    \[ \forall\ c\in\reals\quad\prob\left(\sum_{i=1}^nX_i\geq nc\right)\leq\inf_{\theta>0}e^{-n\theta c}\expect[e^{\theta X}]^n\quad\text{and}\quad\prob\left(\sum_{i=1}^nX_i\leq c\right)\leq\inf_{\theta<0}e^{-n\theta c}\expect[e^{\theta X}]^n \]
  \end{theorem}

  \begin{proof}{Chernoff Bounds - Single Random Variable}
    Let $X$ be a random variable whose moment-generating function $\expect[e^{\theta X}]$ is finite $\forall\ \theta$.\\
    Note that $\forall\ \theta>0$ the events $\big\{X\geq c\big\}$ and $\big\{e^{\theta X}\geq e^{\theta c}\big\}$ are equivalent. Giving
    \[ \prob(X\geq c)=\prob(e^{\theta X}\geq e^{\theta c}) \]
    By \textit{Markov's Inequality} we have that
    \[ \prob(e^{\theta X}\geq e^{\theta c})\leq\frac{\expect[e^{\theta X}]}{e^{\theta c}}=e^{-\theta c}\expect[e^{\theta X}] \]
    As $\theta$ is any positive real and we want the tightest bound, we take the infinum of the bound wrt $\theta$. Giving
    \[\begin{array}{rrcl}
      &\prob(e^{\theta X}\geq e^{\theta c})&\leq&\inf_{\theta>0}e^{-\theta c}\expect[e^{\theta X}]\\
      \implies&\prob(X\geq c)&\leq&\inf_{\theta>0}e^{-\theta c}\expect[e^{\theta X}]
    \end{array}\]
    The result of the theorem.\proved\\
    \textit{An equivalent proof is used for the event $\{X\leq c\}$}.
  \end{proof}

  \begin{proof}{Chernoff Bounds - Sum of IID Random Variables}
    Let $X_1,\dots,X_n$ be IID RVs with finite moment-generating functions $\forall\ \theta$.\\
    Note that $\forall\ \theta>0$ the events $\left\{\sum_{i=1}^nX_i\geq nc\right\}$ and $\left\{e^{\theta\sum_{i=1}^nX_i}\geq e^{nc\theta}\right\}$ are equivalent. Giving
    \[ \prob\left(\sum_{i=1}^nX_i\geq nc\right)=\prob\left(e^{\theta\sum_{i=1}^nX_i}\geq e^{nc\theta}\right) \]
    By \textit{Markov's Inequality} we have that
    \[ \prob\left(e^{\theta\sum_{i=1}^nX_i}\geq e^{nc\theta}\right)\leq\frac{\expect\left[e^{\theta\sum_{i=1}^nX_i}\right]}{e^{nc\theta}}=e^{-nc\theta}\expect[e^{\theta X}]^n \]
    As $\theta$ is any positive real and we want the tightest bound, we take the infinum of the bound wrt $\theta$. Giving
    \[\begin{array}{rrcl}
      &\prob\left(e^{\theta\sum_{i=1}^nX_i}\geq e^{nc\theta}\right)&\leq&\inf_{\theta>0}\frac{\expect\left[e^{\theta\sum_{i=1}^nX_i}\right]}{e^{nc\theta}}=e^{-nc\theta}\expect[e^{\theta X}]^n\\
      \implies&\prob\big(\sum_{i=1}^nX\geq c\big)&\leq&\inf_{\theta>0}e^{-nc\theta}\expect[e^{\theta X}]^n
    \end{array}\]
    The result of the theorem.\proved\\
    \textit{An equivalent proof is used for the event $\big\{\sum_{i=1}^nX_i\leq c\big\}$}.
  \end{proof}

  \begin{theorem}{Jensen's Inequality}
    Let $f$ be a convex function and $X$ be a random variable.\\
    \textit{Jensen's Inequality} states that
    \[ \expect[f(X)]\geq f(E[X]) \]
  \end{theorem}

  \begin{theorem}{Hoeffding's Inequality}
    Let $X_1,\dots,X_n$ be IID random variables taking values in $[0,1]$ and finite mean $\mu$.\\
    \textit{Hoeffding's Inequality} states
    \everymath={\displaystyle}
    \[\begin{array}{rlrcl}
      &\forall\ c>0&\prob\left(\sum_{i=1}^n(X_i-\mu)>nc\right)&\leq& e^{-2nc^2}\\
      \Longleftrightarrow&\forall\ c>0&\prob\left(\hat\mu-\mu>c\right)&\leq&e^{-2nc^2}
    \end{array}\]
    The value of the bound is the same for inequalities in the other direction
    \[\begin{array}{rlrcl}
      &\forall\ c>0&\prob\left(\sum_{i=1}^n(X_i-\mu)<nc\right)&\leq& e^{-2nc^2}\\
      \Longleftrightarrow&\forall\ c>0&\prob\left(\hat\mu-\mu<c\right)&\leq&e^{-2nc^2}
    \end{array}\]
    the $n$ used in the expression involving sample mean is the size of the sample used to calculate the sample mean.
  \end{theorem}

  \begin{theorem}{Bound on Moment Generating Function}
    Let $X$ be a random variable taking values in $[0,1]$ with finite expected value $\mu$. Then we can bound the MGF of the centred random variable with
    \[ \forall\ \theta\in\reals\quad\expect\left[e^{\theta(X-\mu)}\right]\leq e^{\theta^2/8} \]
  \end{theorem}

  \begin{proof}{Hoeffding's Theorem}
    Let $X_1,\dots,X_n$ be IID random variables taking values in $[0,1]$ and finite mean $\mu$. Fix $c>0$.\\
    \textit{Chernoff Bounds} on $\sum_{i=1}^n(X_i-\mu)$ bounded below by $nc$ state
    \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nc\right)\leq e^{-\theta nc}\left(\expect[e^{\theta(X-\mu)}]\right)^n \]
    By \texttt{Theorem 1.6}
    \[ \forall\ \theta\in\reals\quad\expect[e^{\theta(X-\mu)}]^n\leq\left[e^\frac{\theta^2}8\right]^n=e^{n\frac{\theta^2}8} \]
    Incorporating this bound into the above expression we get
    \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{-\theta nt}\cdot e^{n\frac{\theta^2}8}=e^{n\left(-\theta t+\frac{\theta^2}8\right)} \]
    To get the tightest upper-bound we want to find the $\theta$ which minimises the expression on the RHS.
    This is equivalent to minimising the expression $-\theta t+\frac{\theta^2}8$ wrt $\theta$.
    \[\begin{array}{rrcl}
      &\frac{\partial }{\partial\theta}\left(-\theta t+\frac{\theta^2}8\right)&=&-t+\frac\theta4\\
      &\frac{\partial^2 }{\partial\theta^2}\left(-\theta t+\frac{\theta^2}8\right)&=&\frac14>0\\
      \text{Setting}&\frac{\partial }{\partial\theta}\left(-\theta t+\frac{\theta^2}8\right)&=&0\\
      \implies&-t+\frac\theta4&=&0\\
      \implies&\theta&=&4t
    \end{array}\]
    As the second derivative is strictly positive, the expression above is minimise for $\theta=4t$.\\
    By substituting this value of $\theta$ into the expression we get
    \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{n\left(-4t\cdot t+\frac{(4t)^2}8\right)}=e^{n\left(-4t^2+\frac{16t^2}8\right)}=e^{-2nt^2} \]
    The result of the theorem.\proved
  \end{proof}

  \begin{theorem}{Pinsker's Theorem}
    For any distributions $p,q\in[0,1]$
    \[ K(q;p)\geq2(p-q)^2 \]
  \end{theorem}

\subsubsection{Special Cases}

  \begin{theorem}{Chernoff Bound - Binomial Random Variable}
    Let $X\sim\text{Bin}(n,\alpha)$ with $n\in\nats,\ \alpha\in(0,1)$.
    \[\begin{array}{rl}
      \forall\ \beta>\alpha&\prob(X\geq\beta n)\leq e^{-nK(\beta;\alpha)}\\
      \forall\ \beta<\alpha&\prob(X\leq\beta n)\leq e^{-nK(\beta;\alpha)}
    \end{array}\]
    where
    \[ K(\beta;\alpha):=\begin{cases}\beta\ln\left(\frac\beta\alpha\right)+(1-\beta)\ln\left(\frac{1-\beta}{1-\alpha}\right)&\text{if }\beta\in[0,1]\\+\infty&\text{otherwise}\end{cases} \]
    with $x\ln(x):=0$ if $x=0$. Note that $K(\cdot;\cdot)$ is the \textit{Kullback-Leibler Divergence} for two \textit{Binomial Random Variables}.
  \end{theorem}

  \begin{theorem}{Heoffding's Inequality - Binomial Random Variables}
    Let $X\sim\text{Bin}(n,p)$ with $n\in\nats$ and $p\in[0,1]$
    \[\begin{array}{rl}
      \forall\ \varepsilon>0&\prob(X\leq(p-\varepsilon)n)\leq\exp\left(-2n\varepsilon^2\right)\\
      \forall\ \varepsilon>0&\prob(X\geq(p+\varepsilon)n)\leq\exp\left(-2n\varepsilon^2\right)
    \end{array}\]
  \end{theorem}

\subsection{Transformation of Random Variables}

  \begin{theorem}{Monotone Functions}
    Let $X$ be a random variable and $g$ be a differentiable and strictly \underline{monotone} function.\\
    Define $Y:=g(X)$. Then
    \[ f_Y(y)=f_X(g^{-1}(y))\frac{1}{|g'(g^{-1}(y))} \]
  \end{theorem}

  \begin{theorem}{Non-Monotone Functions}
    Let $X$ be a random variable and $g$ be a differentiable and \underline{non}-monotone function.\\
    Define $Y:=g(X)$.\\
    Since $g$ is not monotone, then for a fixed $y$ there are multiple $x$ which solve $y=g(x)$. (Think trig functions). In this case we have to sum the probability contribution from each of these $x$s
    \[ f_Y(y)=\sum_{x\in\{x:g(x)=y\}}f_X(x)\frac{1}{|g'(x)|} \]
  \end{theorem}

  \begin{theorem}{Joint Distributions}
    Let $\X:=\{X_1,\dots,X_n\}$ be random variables on the same sample space and $g:\reals^n\to\reals^n$ be a differentiable function.\\
    Define $\mathbf{Y}=(Y_1,\dots,Y_n):=g(X_1,\dots,X_n)$. Then
    \[ f_\mathbf{Y}(\mathbf{y})=\sum_{\x\in\{\x:g(\x)=\mathbf{y}\}}f_\X(\x)\frac1{|\text{det}(J_g(\x))} \]
    where $\text{det}(J_g(\x))$ denotes the determinant of the Jacobian of $g$ wrt $\x$ (See \texttt{Definition 0.1}).
  \end{theorem}

\section{The Multi-Armed Bandit Problem}

\subsection{The Problem}

  \subsubsection*{Definition}

  \begin{definition}{Multi-Armed Bandit Problem}
    In the \textit{Multi-Armed Bandit Problem} an agent is given the choice of $K$ actions, with each action giving a different reward modelled by an unknown random variable $X_i$. The agent is allowed to play a single at a time and the agent's aim is to maximise some measure of long-run reward (ie find the action with the greatest mean reward), typically whilst minimising loss during the learning stage.
  \end{definition}

  \begin{example}{Motivating Example for Multi-Armed Bandit Problem}
    Consider having a group of patients and several treatments they could be assigned to. How best do you go about determining which treatment is best?
    \par One approach is to assign a subset of the patients randomly to treatments, and then assign the rest to the best treatment. This leads to the questions around what is sufficient evidence for one treatment to be the best? And, how likely are you to choose a sub-optimal treatment?
  \end{example}

  \subsubsection*{Strategies}

  \begin{definition}{Strategy, $I(\cdot)$}
    The agent's \textit{Strategy} $I$ is a function which determines which action the agent shall make at each time step. The only information a \textit{Strategy} can utilise is which arms were played in the past and what reward was received each time.
    \par As it is assumed that this knowledge is utilised, we simplify the notation to only take time as a parameter.
    \[ I(t):=I\big(t,\underbrace{\{I(s)\}_{s\in[1,t)}}_\text{Prev. Actions},\underbrace{\{X_{I(s)}(s)\}_{s\in[1,t)}}_\text{Prev. Rewards}\big)\in[1,K] \]
  \end{definition}

  \begin{definition}{Policy}
    A \textit{Policy} $f(t)$ is a family of \textit{Strategies} and the \textit{Strategy} used at each time-step is chosen randomly from these \textit{Strategies}, typically uniformly at random.
    \[ I(t)=f_t\big(\underbrace{\{I(s)\}_{s\in[1,t)}}_\text{Prev. Actions},\underbrace{\{X_{I(s)}(s)\}_{s\in[1,t)}}_\text{Prev. Rewards},\underbrace{U(t)}_\text{Randomness}\big) \]
  \end{definition}

  \subsubsection*{Measures of Success}

  \begin{definition}{Long-Run Average Reward Criterion, $X_*$}
    The \textit{Long-Run Average Reward} $X_*$ is the average reward a chosen \textit{Strategy} $I(\cdot)$ produces. A \textit{Strategy} is said to be \textit{Optimal} if $X_*=\max_{k\in[1,K]}\expect[X_k]$
    \[ {\displaystyle X_*=\lim_{T\to\infty}\inf\frac1T\sum_{t=1}^T\expect(X_{I(t)})} \]
    The \textit{Infinum} is taken as there is no guarantee the limit exists.
  \end{definition}

  \begin{definition}{Regret $\R_n$}
    \textit{Regret} $R_n$ is the total reward lost during the first $n$ time-steps by using a strategy $I(\cdot)$, compared to if the optimal arm had been played every time.
    \[ \R_n:=n\mu^*-\sum_{i=1}^n\expect[X_{I(t)}(t)]\quad\text{where}\quad\mu^*:=\max_{k\in[1,K]}\expect[X_k] \]
  \end{definition}

  \begin{remark}{Learning Regret}
      \textit{Regret} only involves expectations and thus can be learnt from observations.
  \end{remark}

  \begin{definition}{Strongly Consistent}
    A strategy for the multi-armed bandit problem is said to be \textit{Strongly Consistent} if its regret satisfies $\R_T=o(T^\alpha)\ \forall\ \alpha>0$. (i.e. its regret grows slower than any fractional power of $T$).
  \end{definition}

  \begin{theorem}{Lai \& Robbins Theorem}
    Consider a $K$-armed bandit with Bernoulli arms.\\
    \textit{Lai \& Robbins Theorem} states that, for any \textit{Strongly Consistent} strategy, the number of times that a sub-optimal arm $i$ is played up to time $T$ ($N_i(T)$) satisfies
    \[ \underset{T\to\infty}{\lim\inf}\frac{\expect[N_i(T)]}{\ln(T)}\geq\frac1{K(\mu_i;\mu^*)}\quad\text{where }\mu*:=\max_{i=1}^K\mu_i \]
    where $K(q;p)$ is the \textit{KL-Divergence} of a $\text{Bern}(q)$ distribution wrt a $\text{Bern}(p)$ distribution (See \texttt{Theorem 1.7}).
  \end{theorem}

  \subsubsection*{Mathematical Setup}

  \begin{proposition}{Mathematical Setup for Multi-Armed Bandit Problem}
    Consider a \textit{Multi-Armed Bandit} with $K$ arms and let $X_i(t)$ model the reward obtained by playing arm $i$ at time set $t$, with $i\in[1,K]$ and $t\in\nats$. We make two assumptions about the reward distributions
    \begin{enumerate}
      \item The reward distributions $X_1(\cdot),\cdots,X_K(\cdots)$ are mutually independent.
      \item The reward of each distribution is independent of time. ie $X_i(t)$ and $X_i(t+m)$ are independent $\forall\ i,t,m$
    \end{enumerate}
    The agent is tasked with finding a \textit{Strategy} which minimises \textit{Regret} $R_n$ over a time horizon $T$.
    \[ \text{Find }I(\cdot)\text{ which minimises }\R_T:=T\mu^*-\sum_{t=1}^T\expect[X_{I(t)}(t)]\text{ where }\mu^*:=\max_{k\in[1,K]}\expect[X_k] \]
    \par There are strategies where \textit{Regret} over time $T$ grows sub-linearly (ie $\frac1TR_t\overset{T\to\infty}\longrightarrow0$).
  \end{proposition}

\subsection{Na\"ive Approaches} % Other Heuristics

  \begin{proposition}{Na\"ive Heuristic - Single Test, Bernoulli}
    Let $X_1,X_2$ be \textit{Bernoulli} reward distributions for a 2-armed bandit and defined $\mu_i:=\expect[X_i]$.\\
    Assume WLOG that $\mu_1>\mu_2$ consider the following heuristic
    \begin{center}
      \textit{Play each arm once. Whichever arms returns the greatest reward, play it for all reamining rounds.}
    \end{center}
    Since the reward distributions are \textit{Beroulii} random variables, this heuristic picks the sub-optimal arm with probability $\mu_2(1-\mu_1)$. If the sub-optimal arm is chosen, then it is played a total of $T-1$ times over time $T$. Giving the following lower-bound on the regret $\R_T$
    \[ \R_T\geq\underbrace{\mu_2(1-\mu_1)}_{\tiny\text{prob of wrong choice}}\cdot\underbrace{(\mu_1-\mu_2)}_{\tiny\text{Loss}}\cdot\underbrace{(T-1)}_{\tiny\text{\# steps}} \]
    This regret grows linearly in $T$.
  \end{proposition}

  \begin{proposition}{Better Heuristic - $N$ Tests, Bernoulli}
    Let $X_1,X_2$ be \textit{Bernoulli} reward distributions for a 2-armed bandit and defined $\mu_i:=\expect[X_i]$.\\
    Assume WLOG that $\mu_1>\mu_2$ consider the following heuristic
    \begin{center}
      \textit{Play each arm $N<\frac{T}2$. Pick the arm with the greatest sample mean reward (breaking ties arbitrarily) and playing that arm on all subsequent rounds.}
    \end{center}
    As $X_1,X_2$ are Bernoulli RVs, $S_i(n)\sim\text{Bin}(n,\mu_i)$ and $S_1,S_2$ are independent.\\
    For $\beta\in(\mu_2,\mu_1)$
    \[\begin{array}{rcl}
      \prob\big(S_1(N)<\beta N,\ S_2(N)>\beta N\big)&\leq&e^{-N(K(\beta;\mu_1)+K(\beta;\mu_2))}=e^{-NJ(\mu_1,\mu_2)}
    \end{array}\]
    by \texttt{Theorem 1.7} where
    \[ J(\mu_1,\mu_2):=\inf_{\beta\in[\mu_2,\mu_1]}\big(K(\beta;\mu_1)+K(\beta;\mu_2)\big) \]
    The values of $\beta$ which solve $J(\cdot;\cdot)$ describe the most likely ways for the event $(S_1(N)<S_2(N)$ to occur (ie the wrong decision is made).
  \end{proposition}

  \begin{proposition}{Optimal $N$ for \texttt{Proposition 2.3} }
    For the situation described in \texttt{Proposition 2.3} we want to find $N$ which minimises regret, given a total time horizon of $T$.
    \par With this heuristic it is guaranteed that $R_N=N(\mu_1-\mu_2)$ due to the learning phase. Regret only increases after the learning phase if the sub-optimal arm is chosen. This gives the following expression for regret over time horizon $T$.
    \par However, if the wrong decision is made in the end, regret is equal to $(T-N)\cdot(\mu_1-\mu_2)$.
    \par Thus, the overall regret up to time $T$ is
    \[\begin{array}{rcl}
      \R_T&=&\underbrace{(T-2N)(\mu_1-\mu_2)\prob\big(S_1(N)<S_2(N)\big)}_{\tiny\text{if wrong decision made}}+\underbrace{N(\mu_1-\mu_2)}_{\tiny\text{guaranteed regret}}\\
      &\leq&(T-2N)(\mu_1-\mu_2)\underbrace{e^{-NJ(\mu_1,\mu_2)}}_\texttt{Theorem 1.7}+N(\mu_1-\mu_2)\\
      &\simeq&(\mu_1-\mu_2)(N+Te^{-NJ(\mu_1,\mu_2)})\quad\text{ as } -2Ne^{-NJ(\mu_1,\mu_2)}\text{ is very small}
    \end{array}\]
    We want to minimise this expression wrt $N$
    \[\begin{array}{rrcl}
      &\frac{\partial }{\partial N}(\mu_1-\mu_2)(N+Te^{-NJ(\mu_1,\mu_2)})&=&-TJ(\mu_1,\mu_2)e^{-NJ(\mu_1,\mu_2)}\\
      &\frac{\partial^2 }{\partial N^2}(\mu_1-\mu_2)(N+Te^{-NJ(\mu_1,\mu_2)})&=&TJ(\mu_1,\mu_2)^2e^{-NJ(\mu_1,\mu_2)}>0\\
      \text{Setting}&-TJ(\mu_1,\mu_2)e^{-NJ(\mu_1,\mu_2)}&=&0\\
      \implies&TJ(\mu_1,\mu_2)e^{-NJ(\mu_1,\mu_2)}&=&0\\
      \implies&\ln[TJ(\mu_1,\mu_2)]-NJ(\mu_1,\mu_2)&=&0\\
      \implies&N&=&\frac{\ln[TJ(\mu_1,\mu_2)]}{J(\mu_1,\mu_2)}\\
      \implies&N&=&\frac{\ln[T]}{J(\mu_1,\mu_2)}+O(1)\text{ as }\ln[J(\mu_1,\mu_2)]\text{ is very small}
    \end{array}\]
    As the second derivative is strictly positive, $N:=\frac{\ln[T]}{J(\mu_1,\mu_2)}+O(1)$ is the optimal $N$ used during training and gives the following expression for regret
    \[ \R_T=\frac{\mu_1-\mu_2}{J(\mu_1,\mu_2)}\ln(T)+O(1) \]
    If $\mu_1\simeq\mu_2$ then $J(\mu_1,\mu_2)\simeq(\mu_1-\mu_2-2)^2$ and the above regret becomes $\R_T=\frac{\ln(T)}{\mu_1-\mu_2}+O(1)$.
  \end{proposition}

\subsection{UCB Algorithm}

  \begin{remark}{UCB Algorithm}
    The \textit{Upper Confidence Bound Algorithm} is a \textit{frequentist} algorithm for solving the \textit{Multi-Armed Bandit Problem} for a bandit with \textit{Bernoulli}.
    \par The premise of the algorithm is to play whichever arm has the greatest upper-bound on a confidence interval for the true value of the mean $\mu_i$/
  \end{remark}

  \begin{remark}{Motivation}
    The heuristics in \texttt{Proposition 2.2, 2.3}  treat the sample mean as if it is the true mean (\textit{Certainty Equivalence}), which it is not. The \textit{UCB Algorithm} considers a $1-\delta$ confidence interval for the value of $\mu_i$.
    \par Noting that \textit{Hoeffding's Inequality} states
    \[ \prob(\mu_i>\hat\mu_{i,n}+x)\leq e^{-2nx^2} \]
    We can use this to find an upper-bound of a $1-\delta$ confidence interval for the value of $\mu_i$. This can be done by setting $\delta=e^{-2nx^2}$, rearranging to get $x=\sqrt{\frac1{2n}\ln\left(\frac1\delta\right)}$, and substituting this value of $x$ into \textit{Hoeffding's Inequality} to get an upper bound on $\mu_i$
    \[ \prob\left(\mu_i>\hat\mu_{i,n}+\sqrt{\frac1{2n}\ln\left(\frac1\delta\right)}\right)\leq e^{-2nx^2} \]
    Here $\delta$ is a value we choose from $[0,1]$ depending upon the setting.
  \end{remark}

\subsubsection{Algorithm}

  \begin{definition}{UCB($\alpha$) Algorithm}
    Consider the set up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms and let $\alpha>0$.\\
    The \textit{UCB Algorithm} over time horizon $T$ is defined as
    \begin{enumerate}
      \item In rounds $t\in[1,K]$:
      \begin{enumerate}
        \item Play the $t^{th}$ arm.
      \end{enumerate}
      \item Calculate the $UCB(\alpha,i)$ value for each arm.
      \[ UCB(\alpha,i):=\hat\mu_{i,N_i(t)}+\sqrt{\frac1{2N_i(t)}\alpha\ln(t)} \]
      \item In rounds $t\in(K,T]$:
      \begin{enumerate}
        \item Play the arm $i$ which maximises $UCB(\alpha,i)$.
        \[ I(t)=\underset{i\in[1,K]}{\argmax}UCB(\alpha,i):=\underset{i\in[1,K]}{\argmax}\left\{\hat\mu_{i,N_i(t-1)}+\sqrt{\frac{\alpha\ln(t)}{2N_i(t-1)}}\right\} \]
        \item Update the $UCB(\alpha,i)$ value for the played arm.
      \end{enumerate}
    \end{enumerate}
  \end{definition}

\subsubsection{Analysis}

  \begin{remark}{UCB is Strongly Consistent}
    The \textit{UCB($\alpha$)} algorithm is strongly consistent for all $\alpha>1$ as its regret grows logarithmically with $T$.
  \end{remark}

  \begin{theorem}{Upper Bound on Regret}
    Consider the set up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms, let $\alpha>0$ and assume WLOG that arm 1 is the optimal arm (ie $\mu_1>\mu_i\ \forall\ i\in[2,K]$).
    \par If the \textit{UCB($\alpha$)} algorithm is used, with $\alpha>1$, then the regret in the first $T$ rounds is bounded above by
    \[ \R_T\leq\sum_{i=2}^K\left(\frac{\alpha+1}{\alpha-1}\Delta_i+\frac{2\alpha}{\Delta_i}\ln(T)\right) \]
    This bounds grows logarithmically in $T$, which is very good.
    \par \textit{This theorem is problem in \texttt{Proof 2.3}. }
  \end{theorem}

  \begin{remark}{Setting $\alpha$}
    The result in \texttt{Theorem 2.1} grows fast if $\alpha$ is taken to be large. However, if $\alpha$ is small then the constant term dominates for smaller values of $T$. Thus we typically choose $\alpha=2$.
  \end{remark}

  \begin{theorem}{When a sub-optimal arm is played}
    \everymath={\displaystyle}
    Consider the set up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms, let $\alpha>0$ and assume WLOG that arm 1 is the optimal arm (ie $\mu_1>\mu_i\ \forall\ i\in[2,K]$).
    \par Consider applying \textit{UCB($\alpha$)} to this bandit and under what circumstances a sub-optimal arm is played in steps $t\geq K$ (ie $I(t)=i\neq 1$ for some $t>K$). One of the following statements is true:
    \begin{enumerate}
      \item The sample mean reward from the optimal arm is much smaller than the true mean.
      \[ \hat\mu_{1,N_1(s)}\leq\mu_1-\sqrt{\frac{\alpha\ln(s)}{2N_1(s)}} \]
      \item The sample mean reward on arm $i$ is much larger than its true mean.
      \[ \hat\mu_{i,N_i(s)}\geq\mu_i+\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}} \]
      \item Arm $i$ has been played very few times meaning its the confidence interval on its true mean $\mu_i$ is wide.
      \[ N_i(s)<\frac{2\alpha\ln(s)}{\Delta_i^2} \]
    \end{enumerate}
  \end{theorem}

  \begin{proof}{Theorem 2.2}
    \everymath={\displaystyle}
    \textit{This is a proof by contradiction}.\\
    Consider the set up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms, let $\alpha>0$ and assume WLOG that arm 1 is the optimal arm (ie $\mu_1>\mu_i\ \forall\ i\in[2,K]$).
    \par Suppose $I(s+1)=i\neq1$ but that none of the three inequalities holds. Then
    \[\begin{array}{rcll}
      \underbrace{\hat\mu_{1,N_1(s)}+\displaystyle\sqrt{\frac{\alpha\ln(s)}{2N_1(s)}}}_{\tiny{UCB(\alpha,1)}}&>&\mu_1&\text{by not i)}\\
      &=&\mu_i+\Delta_j&\text{by def. of $\Delta_i$}\\
      &\geq&\mu_i+\displaystyle\sqrt{\frac{2\alpha\ln(s)}{N_i(s)}}&\text{by not iii)}\\
      &\geq&\hat\mu_{i,N_i(s)}-\displaystyle\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}}+\sqrt{\frac{2\alpha\ln(s)}{N_i(s)}}&\text{by not ii)}\\
      &\geq&\hat\mu_{i,N_i(s)}+\left(\sqrt2-\frac1{\sqrt2}\right)\sqrt{\dfrac{\alpha\ln(s)}{N_i(s)}}&\text{by collecting terms}\\
      &=&\underbrace{\hat\mu_{i,N_i(s)}+\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}}}_{\tiny{UCB(\alpha,i)}}
    \end{array}\]
    But, this implies that the $UCB(\alpha,1)>UCB(\alpha,i)$ at the end of round $s$. Hence arm $i$ would not be played in time slot $s+1$.\proved
  \end{proof}

  \begin{theorem}{Counting Lemma}
    Let $\{I(t)\}_{t\in\nats}$ be a $\{0,1\}$-valued sequence and $N_i(t):=\sum_{s=1}^t\indexed{I(s)=i}$. Then
    \[ \forall\ t,u\in\nats\quad N_i(t)\leq u+\sum_{s=u+1}^t\indexed\big\{(N(s-1)\geq u)\ \&\ (I(s)=i)\big\} \]
    with an empty sum defined to be zero.
    \par Note that $\big\{(N(s-1)\geq u)\ \&\ (I(s)=i)\big\}$ is the event where: arm $i$ has been played at least $u$ times so far \underline{and} is played this turn.
  \end{theorem}

  \begin{proof}{Theorem 2.3}
    Fix $t,u\in\nats$. There are two cases
    \par\textit{Case 1} $N_i(t)\leq u$. (ie Have not reached $u$ yet). The result holds trivially here.

    \par\textit{Case 2} $\exists\ s\in[1,t]$ st $N_i(s)>u$. (ie Already reached $u$).\\
    Let $s^*$ denote the smallest such $s$. Then it must be true that $N(s^*-1)=u$ and $s^*\geq u+1$. Hence
    \everymath={\displaystyle}
    \[\begin{array}{rcl}
      N_i(t)&=&\sum_{s=1}^{s^*-1}\indexed{I(s)=i}+\sum_{s=s^*}^t\indexed{I(s)=i}\\
      &=&\underbrace{N(s^*-1)}_{\tiny\text{by def.}}+\sum_{s=s^*}^t\indexed\{\underbrace{(N(s-1)\geq u)}_{\tiny\text{true for all in sum}}\ \&\ (I(s)=i)\}\\
      &=&u+\sum_{s=s^*}^t\indexed\{(N(s-1)\geq u)\ \&\ (I(s)=s)\}\\
      &\leq&\displaystyle u+\sum_{s=u+1}^t\indexed\{(N(s-1)\geq u)\ \&\ (I(s)=s)\}
    \end{array}\]
    The last step holds $u+1\leq s^*$ and thus the sum is done over more terms in the final expression than the one before.
    \proved
  \end{proof}

  \begin{proof}{Upper Bound on Regret}
    \everymath={\displaystyle}
    Consider the set up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms, let $\alpha>0$ and assume WLOG that arm 1 is the optimal arm (ie $\mu_1>\mu_i\ \forall\ i\in[2,K]$).
    \par Fix $t\in\nats$ and define $u_{t,i}:=\left\lceil\frac{2\alpha\ln(t)}{\Delta_i^2}\right\rceil$.
    By \texttt{Theorem 2.3} we have that
    \[ N_i(t)\leq u_{t,i}+\sum_{s=u+1}^t\indexed\{(N_i(s-1)\geq u_{t,i})\ \&\ (I(s)=i)\} \]
    Note that both sides involve random variables. By taking expectations of both sides we get
    \[ \expect[N_i(t)]\leq u_{t,i}+\sum_{s=u}^{t-1}\prob\big\{(N_i(s)\geq u_{t,i})\ \&\ (I(s+1)=i)\big\} \]
    By \texttt{Theorem 2.2} and the definition of $u_{t,i}$, \underline{if} $I(s+1)=i$\textit{ and }$N_j(s)\geq u$ (ie \texttt{Theorem 2.2 iii)} does not hold )\textit{ then}
    \[ \hat\mu_{1,N_1(s)}\leq u_1-\sqrt{\frac{\alpha\ln(s)}{2N_1(s)}}\quad\text{or}\quad\hat\mu_{i,N_i(s)}>\mu_i+\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}} \]
    Thus
    \[ \expect[N_i(t)]\leq u_{t,i}+\sum_{s=u_{t,i}}^{t-1}\left[\underbrace{\prob\left(\hat\mu_{1,N_1(s)}\leq\mu_1-\sqrt{\frac{\alpha\ln(s)}{2N_1(s)}}\right)}_{\tiny\text{$\hat\mu_1$ is unusually small}}+\underbrace{\prob\left(\hat\mu_{i,N_i(s)}>\mu_i+\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}}\right)}_{\tiny\text{$\hat\mu_i$ is unusually large}}\right] \]
    Consider trying to bound the two probabilities
    \[\begin{array}{rcl}
      \prob\left(\hat\mu_{i,N_i(s)}>\mu_i-\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}}\right)&=&\prob\left(\hat\mu_{i,N_i(s)}-\mu_i>\sqrt{\frac{\alpha\ln(s)}{2N_i(s)}}\right)\\
      &\leq&e^{-2N_i(s)\cdot\frac{\alpha\ln(s)}{2N_i(s)}}\quad\text{ by Hoeffding's Inequality}\\
      &=&e^{-\alpha\ln(s)}\\
      &=&s^{-\alpha}\\
    \end{array}\]
    The same bound can be applied to the other probability. Substituting these bounds into the previous expression gives
    \[\begin{array}{rcl}
    \expect[N_i(t)]&\leq& u_{t,i}+\sum_{s=u}^{t-1}2s^{-\alpha}\\
    &\leq&u_{t,i}+\int_{u-1}^\infty2s^{-\alpha}ds\quad\text{assumption $\alpha>1$ required here}\\
    &=&u_{t,i}+\frac{2(u-1)^{-(\alpha-1)}}{\alpha-1}\\
    &\leq& u_{t,i}+\frac{2}{\alpha-1}\quad\text{since }u\geq2\implies (u-1)^{-(\alpha-1)}\leq 1\\
    &=&\left\lceil\frac{2\alpha\ln(t)}{\Delta_i^2}\right\rceil+\frac2{\alpha-1}\quad\text{by def. of }u_{t,i}\\
    &\leq&\frac{2\alpha\ln(t)}{\Delta_i^2}+1+\frac2{\alpha-1}\quad\text{by def. of ceil}\\
    &=&\frac{2\alpha\ln(t)}{\Delta_i^2}+\frac{\alpha+1}{\alpha-1}
    \end{array}\]
    Due to the generality of $i$, this result holds $\forall\ i\in[2,K]$. Hence the total regret up to time $T$ is bounded by
    \[\begin{array}{rrl}
      \R_T&:=&\sum_{i=2}^K\Delta_i\expect[N_i(T)]\\
      &\leq&\sum_{i=2}^K\left(\frac{2\alpha\ln(T)}{\Delta_i}+\Delta_i\frac{\alpha+1}{\alpha-1}\right)
    \end{array}\]
    The result of the theorem.
    \proved
  \end{proof}

\subsubsection{Can we Improve?}

  \begin{remark}{The regret for UCB is almost optimal.}
    The regret of UCB grows logarithmically with $T$, no other algorithm can do better. Further, the constant factor of $\ln(T)$ used is almost optimal. This shall now be shown.
  \end{remark}

  \begin{proposition}{Lower Bound on Regret}
    \everymath={\displaystyle}
    To show the regret of $UCB(\alpha)$ is almost optimal, we derive a lower bound for the regret of any strongly consistent strategy for the multi-armed bandit problem
    \[\begin{array}{rcl}
      \underset{T\to\infty}{\lim\inf}\dfrac{\R_T}{\ln(T)}&=&\underset{T\to\infty}{\lim\inf}\frac1{\ln(T)}\sum_{i\in\{i:\mu_i<\mu^*\}}\Delta_i\expect[N_i(T)]\quad\text{ by def }\R_T\\
      &=&\sum_{i\in\{i:\mu_i<\mu^*\}}\Delta_i\left[\underset{T\to\infty}{\lim\inf}\frac{\expect[N_i(T)]}{\ln(T)}\right]\\
      &\geq&\sum_{i\in\{i:\mu_i<\mu^*\}}\frac{\Delta_i}{K(\mu_i;\mu^*)}\quad\text{by \textit{Lai \& Robbins Theorem}}
    \end{array}\]
  \end{proposition}

  \begin{proposition}{Upper Bound on Regret from UCB}
    To show the regret of $UCB(\alpha)$ is almost optimal, we derive an upper bound for the regret of any strongly consistent strategy for the multi-armed bandit problem
    \everymath={\displaystyle}
    \[\begin{array}{rcl}
      \underset{T\to\infty}{\lim\sup}\dfrac{\R_T}{\ln(T)}&\leq&\underset{T\to\infty}{\lim\sup}\frac1{\ln(T)}\sum_{i=2}^K\left(\frac{2\alpha\ln(T)}{\Delta_i}+\Delta_i\frac{\alpha+1}{\alpha-1}\right)\quad\text{by \texttt{Theorem 2.2}}\\
      &=&\underset{T\to\infty}{\lim\sup}\sum_{i=2}^K\left(\frac{2\alpha}{\Delta_i}+\frac{\Delta_i}{\ln(T)}\cdot\frac{\alpha+1}{(\alpha-1)}\right)\\
      &=&\underset{T\to\infty}{\lim\sup}\sum_{i=2}^K\frac{2\alpha}{\Delta_i}\\
      &\leq&\sum_{i=2}^K\frac2{\Delta_i} \text{ TODO check this}
    \end{array}\]
  \end{proposition}

  \begin{proposition}{Comparing UCB \& Minimum Lower Bound}
    Consider \texttt{Proposition 2.5}  and \textit{Pinsker's Inequality}, when equality is reached
    \[ \underset{T\to\infty}{\lim\inf}\dfrac{\R_T}{\ln(T)}\geq\sum_{i\in\{i:\mu_i<\mu^*\}}\frac{\Delta_i}{K(\mu_i;\mu^*)}\geq\frac1{2\Delta_i} \]
    Comparing this to the result in \texttt{Proposition 2.6}, we get that the regret $UCB(\alpha)$ is at most $\frac{\sum 2/\Delta_i}{\sum 1/2\Delta_i}=4$ times worse that the absolute best.
  \end{proposition}

\subsection{Thompson Sampling}

  \begin{remark}{Thompson Sampling}
    \textit{Thompson Sampling} is a \textit{Bayesian} algorithm for the multi-armed bandit problem. It was one of the first algorithms for solving the problem, but remains on of the best as it is asymptotically optimal.
  \end{remark}

\subsubsection{Algorithm}

  \begin{definition}{Thompson Sampling Algorithm - Bernoulli Arms}
    Consider the st up of a $K$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms.\\
    The \textit{Thompson Sampling Algorithm} over time horizon $T$ is defined as
    \begin{enumerate}
      \item Define a prior distribution $\text{Beta}(1,1)$ for the parameter of each arm.
      \item For $t\in[1,T]$:
      \begin{enumerate}
        \item For $i\in[1,K]$ sample $\hat\mu_i(t)$ from the priors of each arm, breaking ties arbitrarily.
        \item Play the arm with the greatest sample value.
        \[ I(t)=\text{argmax}_{i\in[1,K]}\hat\mu_i(t) \]
        \item Use the observed reward to calculate the posterior of the played arm:
        \begin{itemize}
          \item[-] Given the arm for this prior at time $t$ was $\text{Beta}(\alpha,\beta)$.
          \item[-] If (Reward Observed): Set posterior to $\text{Beta}(\alpha+1,\beta)$.
          \item[-] Else: Set posterior to $\text{Beta}(\alpha,\beta+1)$.
        \end{itemize}
        \item For all un-played arms, assign their prior as their posterior.
        \item For the next round, use the posteriors from this round as the priors.
      \end{enumerate}
    \end{enumerate}
  \end{definition}

  \begin{remark}{Choosing Priors for Thompson Sampling Algorithm}
    In the \textit{Thompson Sampling Algorithm} we choose priors which are \textit{conjugate} with the distribution of the arms of the bandit so the priors and posteriors are from the same family.
    \par For the \textit{Multi-Armed Bandit Problem} we are only interested in estimated the mean reward of a random variable. Here I list some sets of conjugate priors which can be used in \textit{Thompson Sampling} the means of specific distributions. See \texttt{Section 0.4} for a list of conjugate priors and their proofs.
  \end{remark}

\subsubsection{Genie Analysis}

  \begin{remark}{Genie}
    Analysing \textit{Thompson Sampling} is hard as it is difficult to account for the scenario where there is an initial run of bad luck on the optimal arm.
    \par In this section I analyse a simpler version of the Thompson Sampling algorithm for a 2-armed bandit. Consider the following scenario
    \begin{quote}
      The value of $\mu_1$ is known, but the value of $\mu_2$ is unknown. Further, it is unknown whether $\mu_1$ or $\mu_2$ is greater (ie it is not known which is the optimal arm). We only define a prior \& posterior for $\mu_2$ and we play arm 2 if the value $\theta_2(t)$ sampled from its prior is greater than the true value of $\mu_1$.
    \end{quote}
    \par It is likely that this scenario should be more successful (have lower regret)%TODO is this correct?
     than the standard scenario, thus we can only find an upper bound on the regret of the normal scenario.
  \end{remark}

  \begin{theorem}{Times Sub-Optimal arm is played}
    Suppose WLOG that arm two is the suboptimal arm (ie $\mu_1\geq\mu_2$) and consider a time horizon $T\in\nats$. Define $L:=\left\lceil\dfrac{2\ln(T)}{\Delta^2}\right\rceil\ \&\ \tau:=\inf\{t\in[1,T]:N_2(t)\geq L\}$ (The round in which arm 2 is played for the $L^{th}$ time). The probability arm two is played in any given round after round $\tau$ is bounded as
    \[ \forall\ t\geq\tau\quad\prob(\theta_2(t)\geq\mu_1)\leq\frac2T \]
    Futher, we can bound the expected number of times for arm two to be played after round $\tau$
    \[\begin{array}{rcl}
      \expect[\#\text{ plays of arm two after round }\tau]&=&\underbrace{(T-\tau)}_{\#\ Rounds}\cdot\prob(\theta_2(t)\geq\mu_1)\\
      &\leq&(T-\tau)\frac{2}T\\
      &\leq&2
    \end{array}\]
  \end{theorem}

  \begin{proof}{Theorem 2.5}
    \everymath={\displaystyle}
    Consider a time horizon $T\in\nats$ and define the quantities $L:=\left\lceil\dfrac{2\ln(T)}{\Delta^2}\right\rceil$ \&  ${\tau:=\inf\{t\in[1,T]:N_2(t)\geq L\}}$.\\
    Define the events
    \[ A_t:=\{\theta_2(t)\geq\mu_1\}\quad B_t:=\left\{\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta2\right\} \]
    $A_t$ is the event that the sample from the prior of $\mu_2$ in round $t$ is greater than $\mu_1$ (ie arm two is played in round $t$). $B_t$ is the event the average observed rewards from arm 2 up to round $t$ is closer to $\mu_2$ than $\mu_1$.
    We can bound $\prob(A_t)$ as follows
    \[\begin{array}{rcll}
      \prob(A_t)&=&\prob(A_t\cap B_t)+\prob(A_t\cap B_t^c)\\
      &=&\prob(A_t|B_t)\prob(B_t)+\prob(A_t|B_t^c)\prob(B_t^c)\\
      &\leq&\prob(A_t|B_t)+\prob(B_t^c)&(1)
    \end{array}\]
    The inequality occurs since $\prob(X)\geq\prob(X)\prob(Y)$ for all events $X,Y$.
    \par We shall derive bounds, which are independent of the which round it is, for the two RH terms in the final expression separately. First I bound $\prob(B_t^c)$.
    \par If $t\geq\tau$, then $N_2(t)\geq L$ and Hoeffding's inequality yields
    \[\begin{array}{rclll}
      \prob(B_t^c)&\equiv&\prob\left(\frac{S_2(t)}{N_2(t)}>\mu_2+\frac\Delta2\right)\\
      &\equiv&\prob\left(\hat\mu_2(t)>\mu_2+\frac\Delta2\right)\\
      &\leq&\exp\left(-2N_t\frac{\Delta^2}4\right)&\text{ by Hoeffding's Ineq.}\\
      &\leq&\exp\left(-L\frac{\Delta^2}2\right)&\text{ since }N_2(t)\geq L\\
      &\leq&\exp\left(-\frac{2\ln(T)}{\Delta^2}\cdot\frac{\Delta^2}2\right)=e^{-\ln(T)}&\text{ by def. }L\\
      &=&\frac1T&(2)
    \end{array}\]
    Now I bound $\prob(A_t|B_t)$. Let $\theta_2(t+1)$ is the value sampled from the posterior distribution of $\mu_2$ after $t$ rounds, thus, by \texttt{Proof 0.2}, it has the following distribution
    \[ \theta_2(t+1)\sim\text{Beta}\big(1+\underbrace{S_2(t)}_\text{\# successes},1+\underbrace{N_2(t)-S_2(t)}_\text{\# failures}\big) \]
    Hence, by \texttt{Theorem 0.3}, the following events are equivalent
    \[ \big\{A_{t+1}\big|S_2(t),N_2(t)\big\}:= \big\{\theta_2(t+1)\geq\mu_1\big|S_2(t),N_2(t)\big\}\equiv\big\{\text{Bin}(N_2(t)+1,\mu_1)\leq S_2(t)\big\} \]
    By applying the result in \texttt{Theorem 1.9}, for Hoeffding's Inequality on a binomial random variable, we can derive an explicit upper-bound on the probability of the RH event occurring.
    \[\begin{array}{rrcl}
      &\prob\left(\text{Bin}\left(N_2(t)+1,\mu_1\right)\leq S_2(t)\right)&\leq& \exp\left(-2(N_2(t)+1)\varepsilon^2\right)\text{ by \texttt{Theorem 1.9}}\\
      \text{where}&(N_2(t)+1)(\mu_1-\varepsilon)&=&S_2(t)\\
      \implies&\varepsilon&=&\mu_1-\frac{S_2(t)}{N_2(t)+1}\text{ since }N_2(t),S_2(t)\in\nats\\
      &&\geq&\mu_1-\frac{S_2(t)}{N_2(t)}\text{ noting }\mu_1<\frac{S_2(t)}{N_2(t)}\\
      \implies&\exp(-\varepsilon^2)&\leq&\exp\left(-\left(\mu_1-\frac{S_2(t)}{N_2(t)}\right)^2\right)
    \end{array}\]
    Note that $\textstyle\left(\mu_1-\frac{S_2(t)}{N_2(t)}\right)\in[0,1]$ by definition of the terms and ${\forall\ x\in[0,1],\ \left(e^{-x}\right)^n\geq\left(e^{-x}\right)^{n+1}}$. Using these results we derive an upper-bound on the binomial random variable and the equivalent event $A_{t+1}$.
    \[\begin{array}{rrcl}
      &\prob\big(\text{Bin}(N_2(t)+1,\mu_1)\leq S_2(t)\big)&\leq&\exp\left(-2N_2(t)\left(\mu_1-\frac{S_2(t)}{N_2(t)}\right)^2\right)\\
      \implies&\prob(A_{t+1}|S_2(t),N_2(t))&\leq&\exp\left(-2N_2(t)\left(\mu_1-\frac{S_2(t)}{N_2(t)}\right)^2\right)\\
    \end{array}\]
    Consider the following restatement of event $B_t$
    \[\begin{array}{rl}
      &\bigg\{\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta2\bigg\}\\
      \Longleftrightarrow&\bigg\{\frac{S_2(t)}{N_2(t)}\leq\mu_1-\frac\Delta2\bigg\}\text{ by def. }\Delta\\
      \Longleftrightarrow&\bigg\{\frac\Delta2\leq\mu_1-\frac{S_2(t)}{N_2(t)}\bigg\}
    \end{array}\]
    Hence, we can state a bound for $A_t$ given $B_t$ and $N_2(t)$
    \[ \prob\big(A_t|B_t,N_2(t)\big)\leq\exp\left(-2N_2(t)\left(\dfrac\Delta2\right)^2\right)=\exp\left(-2N_2(t)\dfrac{\Delta^2}4\right) \]
    By the definition of $\tau$, $\forall\ t\geq\tau,\ N_2(t)\geq L$. Hence we can derive a bound for $A_t$ given $B_t$ which is independent of $N_2(t)$
    \[\begin{array}{rrcll}
      \forall\ t\geq\tau&\prob(A_t|B_t)&\leq&\exp\left(-2N_2(t)\dfrac{\Delta^2}4\right)\\
      &&\leq&\exp\left(-L\frac{\Delta^2}2\right)\\
      &&=&\exp\left(-\frac{2\ln(T)}{\Delta^2}\cdot\frac{\Delta^2}2\right)\text{ by def. }L\\
      &&\leq&\exp(-\ln T)\\
      &&=&\frac1T&(3)
    \end{array}\]
    \par By substituting the bounds $(2)$ and $(3)$ into expression $(1)$ we get the following bound for event $A_t$
    \[ \forall\ t\geq\tau\quad\prob(A_t)\leq\prob(A_t|B_t)+\prob(B_t^c)\leq\frac1T+\frac1T=\frac2T \]
    This is the stated result of \texttt{Theorem 2.5} \proved
  \end{proof}

  \begin{proposition}{Bound of Regret}
    Using \texttt{Theorem 2.5} we can bound the regret of Genie-Thompson Sampling as
    \[ \mathcal{R}(T)\leq\Delta\cdot(L+2) \]
    where $L+2$ is the most time arm two is played in the first $T$ time steps.
  \end{proposition}

\subsubsection{Analysis}

  \begin{remark}{Analysis of Thompson Sampling is Hard}
    Analysing \textit{Thompson Sampling} is hard as it is difficult to deal with the situation where there is an initial run of bad luck on the optimal arm. This causes the posterior for the optimal arm to be biased towards small values. Hence, the optimal arm is not played very often meaning it takes a long time to recover from the initial bad luck.
    \par For contrast, we only worry about plays of the sub-optimal arm when they are played too often. However, in this scenario the posterior for the sub-optimal arm will be concentrated around the true parameter value and thus the samples arm truer representations.
  \end{remark}

  \begin{theorem}{Upper Bound on Regret}
    Consider a two-armed bandit with Bernoulli arms.\\
    The regret of \textit{Thompson Sampling} over time horizon $T$ is bounded as
    \[ \R_T\leq\frac{40\ln(T)}\Delta+c \]
    where $c$ is an arbitrary constant which is independent of $T$.
    \par\textit{The proof to this theorem is not given in full, but some useful lemmas are shown.}
  \end{theorem}

  \begin{theorem}{Number of times wrong arm is played}
    Consider the set up of a $2$-Armed bandit in \texttt{Proposition 2.1} with Bernoulli Arms and assume WLOG that arm 1 is the optimal arm (ie $\mu_1>\mu_2$).
    \par Consider using \textit{Thompson Sampling} over time horizon $T$. Define $L=\left\lceil\frac{24\ln(T)}{\Delta^2}\right\rceil$  and ${\tau=\inf\{t\in[0,T]:N_2(t)\geq L\}}$ (The time at which arm 2 is played for the $L^{th}$ time).\\ Then
    \[ \text{For }t\in[\tau,T]\quad\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\right)\leq\frac2{T^3} \]
    where $\theta_i(t)$ is the value sampled from the prior of $\mu_i$ at time $t$.
  \end{theorem}

  \begin{proof}{Theorem 2.7}
    Consider using \textit{Thompson Sampling} over time horizon $T$. Define $L=\left\lceil\frac{24\ln(T)}{\Delta^2}\right\rceil$  and ${\tau=\inf\{t\in[0,T]:N_2(t)\geq L\}}$ (The time at which arm 2 is played for the $L^{th}$ time).\\
    \everymath={\displaystyle}
    By the definition of $\tau$, if $t\geq\tau$ then $N_2(t)\geq L$. Thus
    \[\begin{array}{rlll}
    &\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\right)\\
    =&\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2,\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)+\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2,\frac{S_2(t)}{N_2(T)}>\mu_2+\frac\Delta4\right)\\
    \leq&\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\bigg|\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)+\prob\left(\frac{S_2(t)}{N_2(T)}>\mu_2+\frac\Delta4\right)&(1)
    \end{array}\]
    the last step occurs because \footnote{For all random variables $X,Y$ $\prob(X|Y)\geq\prob(X,Y)$ and $\prob(X)\geq\prob(X,Y)$}
    \[\begin{array}{rrcl}
      &\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2,\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)&\leq&\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\bigg|\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)\\
      \text{and }&\prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2,\frac{S_2(t)}{N_2(T)}>\mu_2+\frac\Delta4\right)&\leq&\prob\left(\frac{S_2(t)}{N_2(T)}>\mu_2+\frac\Delta4\right)
    \end{array}\]
    We now bound both terms in $(1)$ seperately.
    \par Firstly, conditional on the number of times the second arm is player $N_2(T)$, the total reward from these plays $S_2(t)$ is the sum of $N_2(t)$ independent $\text{Bern}(\mu_2)$ random variables. Hence, using \textit{Hoeffding's Inequality} and noting that $\textstyle\expect\left(\frac{S_2(t)}{N_2(t)}\right)=\mu_2$, we have
    \[ \prob\left(\frac{S_2(t)}{N_2(t)}>\mu_2+\frac\Delta4\bigg|N_2(t)\right)\leq\exp\left(-2N_2(t)\left(\frac\Delta4\right)^2\right)=\exp\left(-N_2(t)\frac{\Delta^2}8\right) \]
    As we have assumed that $N_2(t)\geq L\geq\frac1{\Delta^2}(24\ln(T))$ meaning $-N_2(t)\leq\frac1{\Delta^2}(24\ln(T))$. Thus
    \[\begin{array}{rrcll}
    &-N_2(t)\frac{\Delta^2}8&\geq&-\frac{24}{8}\ln(T)\\
    &&=&-3\ln(T)\\
    \implies&\prob\left(\frac{S_2(t)}{N_2(t)}>\mu_2+\frac\Delta4\right)&\leq&\exp\left(-3\ln(T)\right)\\
    &&=&\frac1{T^3}&(2)
    \end{array}\]
    Next, we note that conditional on $S_2(t)$ and $N_2(t)$, by \texttt{Proof 0.2}  the distribution of $\theta_2(t)$ is ${\text{Beta}\big(\underbrace{S_2(t)+1}_\alpha,\underbrace{N_2(t)-S_2(t)+1}_\beta\big)}$. Consequently, by \texttt{Proof 0.3}, we have that
    \[ \prob\bigg(\theta_2(t)\geq\underbrace{\mu_2+\frac\Delta2}_p\bigg)=\prob\bigg(\text{Bin}\bigg(\underbrace{N_2(t)+1}_{\alpha+\beta-1},\underbrace{\mu_2+\frac\Delta2}_p\bigg)\leq \underbrace{S_2(t)}_{\alpha-1}\bigg) \]
    By applying the result in \texttt{Theorem 1.9}, for Hoeffding's Inequality on a binomial random variable, we can derive an explicit upper-bound on the probability.
    \[\begin{array}{rrcl}
      &\prob\left(\text{Bin}\left(N_2(t)+1,\mu_2+\frac\Delta2\right)\leq S_2(t)\right)&\leq& \exp\left(-2(N_2(t)+1)\varepsilon^2\right)\text{ by \texttt{Theorem 1.9}}\\
      \text{where}&\left(N_2(t)+1\right)\left(\mu_2+\frac\Delta2-\varepsilon\right)&=&S_2(t)\\
      \implies&\mu_2+\frac\Delta2-\varepsilon&=&\frac{S_2(t)}{N_2(t)+1}\\
      \implies&\varepsilon&=&\mu_2+\frac\Delta2-\frac{S_2(t)}{N_2(t)+1}\\
      &&\leq&\mu_2+\frac\Delta2-\left(\mu_2+\frac\Delta4\right)\quad\text{assuming }\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\\
      &&=&\frac\Delta4\\
      \implies&\exp(-\varepsilon^2)&\leq&\exp\left(-\left(\frac\Delta4\right)^2\right)=\exp\left(-\frac{\Delta^2}{16}\right)
    \end{array}\]
    This gives us the following bound
    \[ \prob\left(\text{Bin}\left(N_2(t)+1,\mu_2+\frac\Delta2\right)\leq S_2(t)\bigg|\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)\leq\exp\left(-2(N_2(t)+1)\frac{\Delta^2}{16}\right) \]
    Substituting this result into the original expression involving the binomial we get
    \[\begin{array}{rcll}
      \prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\bigg|\frac{S_2(t)}{N_2(t)}\leq\mu_2+\frac\Delta4\right)&\leq&\exp\left(-2\big(N_2(t)+1\big)\frac{\Delta^2}{16}\right)\\
      &\leq&\exp\left(-\frac{L\Delta^2}8\right)\text{ since }N_2(t)\geq L\\
      &\leq&\exp\left(-\frac{24\ln(T)}{\Delta^2}\cdot\frac{\Delta^2}8\right)\text{ by def. of }L\\
      &=&\exp\left(-3\ln(T)\right)\\
      &=&\frac1{T^3}&(3)
    \end{array}\]
    Substituting $(2)$ and $(3)$ into $(1)$, we can conclude that if $t\geq\tau$ (ie $N_2(t)\geq L$) then
    \[ \prob\left(\theta_2(t)\geq\mu_2+\frac\Delta2\right)\leq\frac1{T^3}+\frac1{T^3}=\frac2{T^3} \]

    This is the stated result of \texttt{Theorem 2.7} \proved
  \end{proof}

\section{Stochastic Dynamic Optimisation Problems}

\subsection{General}

  \begin{definition}{Stochastic System}
    A \textit{Stochastic System} is a dynamic system where at least one part of the system relies on a random process, modelled by random variables.
  \end{definition}

  \begin{definition}{Stochastic Dynamic Optimisation}
    \textit{Stochastic Dynamic Optimisation} is the study of problems where an agent is tasked with making optimal or near-optimal decision in a \textit{Stochastic System}.
  \end{definition}

  \begin{definition}{Sequential Decision Process}
    In a \textit{Sequential Decision Process} an agent is tasked with choosing a sequence of actions such that a \textit{Stochastic System} performs optimally wrt some pre-specified \textit{Performance Criterion}. The agent is able to observe the current system-state before taking each action.
    \par A \textit{Sequential Decision Process} has the following components which need to be defined
    \begin{itemize}
      \item \textit{Time-Horizon}, $T$ - Time epochs in which actions are taken and their effect realised.
      \item \textit{State-Space}, $S$ - A mathematical encoding of available system information.
      \item \textit{Action-Space}, $A$ - Set of actions an agent is able to take, which affect the system. Available actions may depend on the current system state.
      \item \textit{Transition Probabilities}, $p_t(\cdot|\cdot,\cdot)$ - A mathematical description of the underlying stocahstic system, relating agent actions and system states.
      \item \textit{Immediate Rewards/Costs}, $r_t(\cdot,\cdot)$ - The reward/cost an agent recieves/incurs after taking an action.
    \end{itemize}
  \end{definition}

  \begin{definition}{Time-Horizon, $T$}
    The \textit{Time-Horizon} $T$ is the set of all \textit{Decision Epochs}. There are three types of \textit{Time-Horizon}
    \begin{enumerate}
      \item \textit{Continuous-Time} - $T=[t_0,t_1]$.\footnote{Continuous-time time-horizons are out of the scope of this module.}
      \item \textit{Finite Discrete-Time} - $T=\{t_0,\dots,t_N\}$.
      \item \textit{Infinite Discrete-Time} - $T=\{t_0,t_1,\dots\}$.
    \end{enumerate}
  \end{definition}

  \begin{definition}{State-Space, $S$}
    The \textit{State-Space} $S$ is the set of all states the \textit{Stochastic System} can take. There are three types of \textit{State-Space}\footnote{Only \textit{Finite Discrete-State-Spaces} are in scope of this module.}
    \begin{enumerate}
      \item \textit{Continuous-State} - \textit{State-Space} is uncountable.
      \item \textit{Finite Discrete-State} - \textit{State-Space} is countaly finite $S=\{s_1,\dots,s_n\}$.
      \item \textit{Infinite Discrete-State} - \textit{State-Space} is countably infinite.
    \end{enumerate}
  \end{definition}

  \begin{definition}{Action-Space, $A$}
    The \textit{Action-Space} $A$ is the set of actions the agent can take. There are three types of \textit{Action-Space}\footnote{Only \textit{Finite Discrete-Action-Spaces} are in scope of this module.}
    \begin{enumerate}
      \item \textit{Continuous-Action} - \textit{Action-Space} is uncountable.
      \item \textit{Finite Discrete-Action} - \textit{Action-Space} is countaly finite $A=\{s_1,\dots,s_n\}$.
      \item \textit{Infinite Discrete-Action} - \textit{Action-Space} is countably infinite.
    \end{enumerate}
    The \textit{Admissible Action-Space} $A(s)\subseteq A$ is the set of actions the agent can take if the system is in state $s$.
  \end{definition}

  \begin{definition}{Transition Probabilities, $p_t(\cdot|\cdot,\cdot)$}
    \textit{Transition Probabilities} $p_t(\cdot|\cdot,\cdot)$ are parametric-probability mass functions which define the probability of the
    \[ p_t(s'|s,a)=\prob(X_{t+1}=s'|X_t=s,Y_t=a) \]
  \end{definition}

  \begin{definition}{Decision Rules $q_t(\cdot)$,$d_t(\cdot)$}
    A \textit{Decision Rule} $q_t(\cdot)$ is a procedure the agent uses to decide what action to take, given available information (Current state $X_t$, previous states $X_{0:t-1}$, previous actions $Y_{0:t-1}$).
    \par There are four classes of \textit{Decision Rule}:
    \begin{enumerate}
      \item \textit{History Dependent Randomised}, HR - The \textit{Decision Rule} $q_t(\cdot)$ is a conditional probability mass function on the action-space $A$, using all available information.
      \[\begin{array}{rcl}
        \prob(Y_0=a_0|X_0=s_0)&=&q_0(a_0|s_0)\\
        (Y_0|X_0)&\sim&q_0(\cdot|X_0)\\
        \prob(Y_t=a_t|X_{0:t}=s_{0:t},Y_{0:t-1}=a_{0:t-1})&=&q_t(a_t|s_{0:t},a_{0:t-1})\\
        (Y_t|X_{0:t},Y_{0:t-1})&\sim&q_t(\cdot|X_{0:t},Y_{0:t-1})
      \end{array}\]
      \item \textit{History Dependent Deterministic}, HD - The \textit{Decision Rule} $d_t(\cdot)$ is a deterministic function of all currently available information
      \[ Y_t:=d_t(X_{0:t},Y_{0:t-1}) \]
      \item \textit{Markovian Randomised}, MR - The \textit{Decision Rule} $q_t(\cdot)$ is a conditional mass function on the action-space $A$, using \underline{only} the current system-state.
      \[\begin{array}{rcl}
        \prob(Y_t=a_t|X_{0:t}=s_{0:t},Y_{0:t-1}=a_{0:t-1})&=&\prob(Y_T=a_t|X_t=s_t)\\
        &=&q_t(a_t|s_t)\\
        (Y_t|X_{0:t},Y_{0:t-1})&\sim&q_t(\cdot|X_t)
      \end{array}\]
      \item \textit{Markovian Deterministic}, MD - The \textit{Decision Rule} $d_t(\cdot)$ is a deterministic function of the current system-state
      \[ Y_t:=d_t(X_t) \]
    \end{enumerate}
  \end{definition}

  \begin{remark}{Memoryless}
    \textit{Markovian Decision Rules} are memoryless.
  \end{remark}

  \begin{definition}{Decision Policy $\pi$}
    A \textit{Decision Policy} $\pi$ is a sequence of \textit{Decision Rules}, specifying which \textit{Decision Rule} $q_t(\cdot)$ to use in each epoch.
    \[ \pi:=\{q_t(\cdot)\}_{t\in T} \]
    There are two types of \textit{Decision Policy}
    \begin{enumerate}
      \item \textit{Stationary Decision-Policy} - The same decision rule is applied in each epoch.
      \[ \exists\ q(\cdot)\text{ st }q_t(\cdot)=q(\cdot)\ \forall\ t\in T \]
      \item \textit{Non-Stationary Decision-Policy} - A variety of \textit{Decision Rules} are used. Which specific one is used depends on the current epoch $t$.
    \end{enumerate}
  \end{definition}

  \begin{remark}{Static vs Dynamic Approach}
    There are two approaches to a \textit{Sequential Decision Process}.
    \begin{itemize}
      \item[\textit{Static}] The agent decides what actions they take before the first decision epoch. This means agent actions are independent of the system state.
      \item[\textit{Dynamic}] The agent decides their action each epoch, taking the current system state into account.
    \end{itemize}
    As there is no penalty for delaying choosing a move until the epoch in which you make it, there is little reason not to take the \textit{dynamic} approach.
  \end{remark}

  \begin{definition}{Induced Stochastic Process $\{(X_t,Y_t)\}_{t\geq0}$}
    The \textit{Induced Stochastic Process} $\{(X_t,Y_t)\}_{t\geq0}$ is the time-evolution of the agent actions $X_t$ and system states $Y_t$ in the stochastic system.
    \par The \textit{Induced Stochastic Process} can be fully defined by
    \begin{enumerate}
      \item The probability mass function of $X_0$.
      \item The system's transition probabilities $\{p_t(\cdot|\cdot,\cdot)\}_{t\in T}$.\footnote{Specifies the probability the system is in state $s'$, given the agent took action $a$ which the system was in state $s$.}
      \item The agent's decision policy $\pi:=\{q_t(\cdot|\cdot)\}_{t\in T}$ .\footnote{Specifies the probability of an agent taken a given action, given the current state of the system.}
    \end{enumerate}
  \end{definition}

  \begin{proposition}{Distributions of Induced Stochastic System}
    \everymath={\displaystyle}
    In an \textit{Induced Stochastic Process} the following distributions exist
    \[\begin{array}{rcl}
      \prob(X_{0:t}=s_{0:t},Y_{0:t-1}=a_{0:t-1})&=&\prob(X_0=s_0)\prod_{k=0}^{t-1}\underbrace{p_k(s_{k+1}|s_k,a_k)}_\text{Transition}\underbrace{q_k(a_k|s_{0:k},a_{0:k-1})}_\text{Decision}\\
      \prob(X_{t+1}=s'|X_t=s)&=&\sum_{a\in A(s)}p_t(s'|s,a)q_t(a,s)
    \end{array}\]
  \end{proposition}

  \begin{theorem}{Markov Chains in SDPs}
    When using a \textit{Markovian Decision Policy} in a \textit{Stochastic Dynamic Process}, the sequence of states $\{X_t\}_{t\in T}$ and the sequence of state-action pairs $\{(X_t,Y_t)\}_{t\in T}$ are \textit{Markov Chains}.
    \par Moreover, if the transition and decision probabilities are stationary (ie independent of $t$), then they are \textit{Homogeneous Markov Chains}.
  \end{theorem}

\subsection{Markov Decision Processes}

  \begin{definition}{Markov Decision Process, MDP}
    A \textit{Markov Decision Process}, MDP, is a \textit{Sequential Decision Problem} where the underlying \textit{Stochastic System} has the \textit{Markov Property}. This is realised by the state of the system in epoch $t+1$ only depending upon the system state and agent action in epoch $t$.
    \par In each decision epoch, \textit{Markov Decision Process} follows the following steps
    \begin{enumerate}
      \item The agent observes the system state $X_t$.
      \item Based on this observation, the agent chooses an action $Y_t$ to take.
      \item The agent recieves an immediate reward $r(X_t,Y_t)$ and the system evolves $X_{t+1}$.
    \end{enumerate}
  \end{definition}

  \begin{definition}{Markov Decision Problem}
    In a \textit{Markov Decision Problem}, the agent is tasked with finding a \textit{Decision Policy} $\pi$ which maximise the expected total reward received\footnote{As the reward received in each epoch $r_t(\cdot,\cdot)$ depends upon random quantities (namely system states), we cannot maximise total reward directly and instead maximise its expectation wrt the chosen policy.} in a given time-horizon $T$.
    \[ \max_\pi\expect^\pi\left[\sum_{t\in T}r(X_t,Y_t)\right] \]
    A \textit{Markov Decision Problem} is defined by the same components as a \textit{Sequential Decision Problem} (See \texttt{Definition 3.3}). The \textit{Transition Probabilities} $p_t(\cdot|\cdot,\cdot)$ are required to have the \textit{Markov Property}, meaning we have the following stochastic system
    \[\begin{array}{rcl}
      (X_{t+1}|X_{0:t},Y_{0:t})&\sim&(X_{t+1}|X_t,Y_t)\\
      &\sim&p_t(\cdot|X_t,Y_t)
    \end{array}\]
  \end{definition}

  \begin{remark}{Initial State $X_0$}
    The initital state $X_0$ of the system is independent of the agent's actions and thus the chosen policy $\pi$.
  \end{remark}

\subsection{General Finite-Horizon MDPs}

\subsubsection{Problem Formulation}

  \begin{definition}{General Finite-Horizon MDP}
    In a \textit{Finite-Horizon Markov Decision Problem} the agent has a finite-number of epochs in which to take actions in and seeks to maximise the total expected reward received.
    \par All \textit{Finite-Horizon MDPs} have the following features\footnote{The number of epochs $N$, state-space $S$, action-space $A$, transition probabilities $p_t(\cdot)$ and rewards $r_t(\cdot)$ are all specified on a problem-by-problem basis.}
    \begin{itemize}
      \item \textit{Number of Epochs} - $N\in\nats$.
      \item \textit{Time-Horizon} - $T=\{0,\dots,N-1\}$.
      \item \textit{Transition Probabilities} - $p_0(s'|s,a),\dots,p_{N-1}(s'|s,a)$.
      \item \textit{Immediate Rewards} - $r_0(s,a),\dots,r_{N-1}(s,a),r_N(s)$.\footnote{$r_N(s)$ is the \textit{Terminal Reward} and depends on the final state of the system.}
      \item \textit{Objective} - Given the transition probabilities $\{p_t(\cdot|\cdot,\cdot)\}_{t\in T}$, immediate rewards $\{r_t(\cdot,\cdot)\}_{t\in T}$ and terminal reward $r_N(\cdot)$, the agent is tasked to find a \textit{History Dependent Randomised} policy $\pi\in HR(T)$ over time-horizon $T$ which maximises the exepcted total reward
      \[ \argmax_{\pi\in HR(T)}\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right] \]
    \end{itemize}
  \end{definition}

  \begin{proposition}{Stochastic System of a Finite-Horizon MDP}
    In epoch $t$ \textit{Finite-Horizon MDPs} have the following \textit{Stochastic System}, given all available information
    \[\begin{array}{rcl}
      (X_{t+1}|X_{0:t},Y_{0:t})&\sim&(X_{t+1}|X_t,Y_t)\\
      &\sim&p_t(X_{t+1}|X_t,Y_t)
    \end{array}\]
  \end{proposition}

\subsubsection{Optimisation}

  \begin{remark}{Computational Cost of Optimisation}
    Calculating optimal strategies for \textit{Finite-Horizon MDPs} is computationally expensive, especially for large $N$. Hence approximating \textit{Finite-Horizon MDPs} are other problems is desirable. This is explored in \texttt{Section 3.4.2} and \texttt{Section 3.5.2}  /
  \end{remark}

  \begin{remark}{Dynamic Programming Algorithm}
    \everymath={\displaystyle}
    The \textit{Dynamic Programming Algorithm} is a system equation for determining the optimal \textit{Decision Policy} $\pi^*$ for a \textit{Finite-Horizon MDP}. These equations are defined as a \textit{Backwards Recursion}\footnote{Iterate from $N-1$ to $0$.}
    \[\begin{array}{rcl}
      u^*_N(s)&=&r_N(s)\\
      u_t^*(s)&=&\max_{a\in A(s)}\left\{r_t(s,a)+\sum_{s'\in S}u_{t+1}^*(s')p_t(s'|s,a)\right\}\quad t\in T=[0,N-1]\\
      d_t^*(s)&=&\argmax_{a\in A(s)}\left\{r_t(s,a)+\sum_{s'\in S}u_{t+1}^*(s')p_t(s'|s,a)\right\}\quad t\in t=[0,N-1]\\
    \end{array}\]
    $u_t^*(s)$ is the \textit{Optimality Equation} and takes the value of the maximum expected reward which can be earned in the last $N-t$ steps of the problem, due to its recursive definition.
    \par $d_t^*(s)$ is the \textit{Optimal Decision Rule} and takes the value of the action which produces the greatest expected reward from the last $N-t$ steps of the problem.
  \end{remark}

  \begin{definition}{Optimal Policy $\pi^*$}
    An \textit{Optimal Policy} $\pi^*$ is any policy which produces the maximum expected total reward when applied to the defined \textit{Finite-Horizon MDP}. In this case it is
    \[ \pi^*:=\{d_t^*(s)\}_{t\in T} \]
  \end{definition}

  \begin{definition}{Value Function $v^\pi(\cdot)$ and Optimal Value Function $v^*(\cdot)$}
    \everymath={\displaystyle}
    The \textit{Value Function} $v^pi(\cdot)$ is the expected total reward, given the initial state of the system $X_0=s$ and the policy $\pi\in HR(T)$ which is being used.
    \[ v^\pi(s):=\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\bigg|X_0=s\right] \]
    The \textit{Optimal Value Function} $v^*(\cdot)$ is the maximum expected total reward, given the initial state of the system $X_0=s$
    \[\begin{array}{rrl}
      v^*(s)&:=&\max_{\pi\in HR(T)}v^\pi(s)\\
      &=&\max_{\pi\in HR(T)}\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\bigg|X_0=s\right]
    \end{array}\]
  \end{definition}

  \begin{theorem}{Optimal Value Function and Dynamic Programming Algorithm}
    Here are two equivalent expressions of the \textit{Optimal Value Function} $v^*(\cdot)$
    \[\begin{array}{rclll}
      v^*(s)&=&u_0^*(s)&\quad&\forall\ s\in S\\
      v^*(s)&=&v^{\pi^*}(s)&\quad&\forall\ s\in S\\
    \end{array}\]
  \end{theorem}

\subsubsection{Optimality Principle}

  \begin{definition}{Tail Subproblem}
    Consider a \textit{Finite-Horizon MDP} over $N$ epochs.
    \par The \textit{Tail Subproblem of Length $L$}\footnote{$L\in[1,N]$} of this \textit{Finite-Horizon MDP} is a subproblem which is concerned with the last $L$ epochs of the full problem. It has the following features
    \begin{itemize}
      \item \textit{Number of Epochs} - $L\in[1,N]$.
      \item \textit{Time-Horizon} - $T_L=\{N-L,\dots,N-1\}$.
      \item \textit{Transition Probabilities} - $p_{N-L}(s'|s,a),\dots,p_{N-1}(s'|s,a)$.
      \item \textit{Immediate Rewards} - $r_{N-L}(s,a),\dots,r_{N-1}(s,a),r_N(s)$.
      \item \textit{Objective}\footnote{Same as the full problem, except over the reduced \textit{Time-Horizon}} - Given the transition probabilities $\{p_t(\cdot|\cdot,\cdot)\}_{t\in T}$, immediate rewards $\{r_t(\cdot,\cdot)\}_{t\in T}$ and terminal reward $r_N(\cdot)$, the agent is tasked to find a \textit{History Dependent Randomised} policy $\pi\in HR(T)$ over time-horizon $T$ which maximises the exepcted total reward
      \[ \argmax_{\pi\in HR(T_L)}\expect^\pi\left[\left(\sum_{t=N-L}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right] \]
    \end{itemize}
  \end{definition}

  \begin{remark}{Equivalence of MDP and Tail Subproblem}
    The \textit{Tail Subproblem of Length $L$} has \textit{Time-Horizon} $T=\{N-L,\dots,N-1\}$ and thus is equivalent to the full \textit{Finite-Horizon MDP} with time-horizon $T=\{0,\dots,L\}$\footnote{\textit{Finite-Horizon MDP} over $L$ epochs.}
    \par This means optimising the \textit{Tail Subproblem} only requires re-indexing the \textit{Optimality Equations} of the full problem.
  \end{remark}

  \begin{proposition}{Optimising Tail Subproblem}
    \everymath={\displaystyle}
    The \textit{Optimality Equations} for a \textit{Tail Subproblem of Length $L$} are defined sub-recursively as
    \[\begin{array}{rcl}
      u^*_{L,N}(s)&=&r_N(s)\\
      u_{L,t}^*(s)&=&\max_{a\in A(s)}\left\{r_t(s,a)+\sum_{s'\in S}u_{L,t+1}^*(s')p_t(s'|s,a)\right\}\quad t\in T_L=[N-L,N-1]\\
      d_{L,t}^*(s)&=&\argmax_{a\in A(s)}\left\{r_t(s,a)+\sum_{s'\in S}u_{L,t+1}^*(s')p_t(s'|s,a)\right\}\quad t\in t=[N-L,N-1]\\
    \end{array}\]
    and the \textit{Optimal Policy} $\pi_L^*$ is
    \[ \pi^*_L:=\{d_{L,t}^*(s)\}_{t\in T_L} \]
  \end{proposition}

  \begin{remark}{Optimising Tail Subproblem vs Optimising MDP}
    The initial condition of the \textit{Optimality Equations} for both the tail subproblem $u_{L,N}^*(\cdot)$ and the full problem $u_N^*(\cdot)$ have the same definition
    \[ u_{L,N}^*(s):=r_N(s)=:u_N^*(s) \]
    In the equivalent epoch $t$, the \textit{Optimality Equation} of the tail subproblem $u_{L,t}^*(\cdot)$ is a sub-recursion of the \textit{Optimality Equation} for the full problem $u_t^*(\cdot)$.
    \par Given these two properties, the \textit{Optimality Equations} for the subproblem are all identical to that of the full problem for the equivalent epoch.
    \[\begin{array}{rcl}
      u_{L,t}^*(s)&=&u_t^*(s)\\
      d_{L,t}^*(s)&=&d_t^*(s)
    \end{array}\]
  \end{remark}

  \begin{definition}{Optimal Value Function of Tail-Subproblem $v_L^*(\cdot)$}
    The \textit{Optimal value Function} for the \textit{Tail-Subproblem} of length $L$ is defined as
    \[ v_L^*(\cdot):=\max_{\pi\in HR(T_L)}\expect^\pi\left[\left(\sum_{t=N-L}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\bigg|X_{N-t}=s\right] \]
    This value can be interpreted as the maximum expected total reward received from the last $L$ epochs, given the system is in state $s$ at the start of epoch $t=N-L$.
    \[ v_L^*(s)=y_{L,N-L}^*(s) \]
  \end{definition}

  \begin{theorem}{Optimality Principle}
    Consider a \textit{Finite-Horizon MDP} over $N$ epochs and a \textit{Tail Subproblem of Length $L$}.
    \par The \textit{Optimality Principle} states
    \[\begin{array}{rcl}
      v_L^*(s)&=&u_{N-L}^*(s)\\
      \pi_L^*&=&\{d_t^*(s)\}_{t\in T_L}
    \end{array}\]
  \end{theorem}

  \begin{remark}{Optimality Principle}
    The \textit{Optimality Principle} shows that the \textit{Dynamic Programming Algorithm} can be solved by solving all the \textit{Tail Subproblems of Length $L$} for all $L\in[1,N]$.
    \par This can be used to restate the \textit{Dynamic Programming Algorithm} as a forwards-recursion
    \[\begin{array}{rcl}
      v_0^*(s)&:=&r_N(s)\\
      v_t^*(s)&:=&\max_{a\in A(s)}\left\{r_{N-k}(s,a)+\sum_{s'\in S}v_{k-1}^*(s')p_{N-k}(s'|s,a)\right\}\quad t\in[1,N]\\
      d_t^*(s)&:=&\argmax_{a\in A(s)}\left\{r_{N-k}(s,a)+\sum_{s'\in S}v_{k-1}^*(s')p_{N-k}(s'|s,a)\right\}\quad t\in[1,N]\\
    \end{array}\]
  \end{remark}

\subsection{Discounted Reward Infinite-Horizon MDPs}

\subsubsection{Problem Formulation}

  \begin{definition}{Discounted Reward Infinite-Horizon MDPs}
    In a \textit{Discounted Reward Infinite-Horizon MDP} the agent is tasked to find a policy which maximises the total expected discounted reward received.\footnote{The reward being ``discounted'' means that rewards received further into the further are weighted less. This is done by multiplying the expected reward in epoch $t\in T$ by $\alpha^t$ where $\alpha\in(0,1)$.}
    All \textit{Discounted Reward MDPs} have the following features
    \begin{itemize}
      \item \textit{Number of Epochs} - $N=\infty$.
      \item \textit{Time-Horizon} - $T_L=\{0,1,\dots\}$.
      \item \textit{Transition Probabilities} - $p_t(s'|s,a)=p(s'|s,a)\ \forall\ t\in T$.\footnote{These are \textit{Stationary Transition Probabilities}.}
      \item \textit{Immediate Rewards} - $r_t(s,a)=\alpha^tr(s,a)\ \forall\ t\in T$.\footnote{These are \textit{Stationary Rewards}.}
      \item \textit{Objective} - Given the transition probabilities $p(s'|s,a)$, immediate rewards $r(s,a)$ and discounting factor $\alpha$, the agent is tasked to find a \textit{History Dependent Randomised Policy} $\pi\in HR(T)$ over time-horizon $T$ which maximises the expected total reward
      \[ \argmax_{\pi\in HR(T)}\expect^\pi\left[\sum_{t=0}^\infty r_t(X_t,Y_t)\right]=\argmax_{\pi\in HR(T)}\expect^\pi\left[\sum_{t=0}^\infty \alpha^tr(X_t,Y_t)\right] \]
    \end{itemize}
  \end{definition}

  \begin{proposition}{Stochastic System of a Discounted Reward MDP}
    In epoch $t$, \textit{Discounted Reward MDPs} have the following \textit{Stochastic System}, given all available information
    \[\begin{array}{rcl}
      (X_{t+1}|X_{0:t},Y_{0:t})&\sim&(X_{t+1}|X_t,Y_t)\\
      &\sim&p_t(\cdot|X_t,Y_t)\\
      &=&p(\cdot|X_t,Y_t)
    \end{array}\]
  \end{proposition}

  \begin{remark}{Time-Importance of Rewards}
    The value of $\alpha^t$ characterises the importance of the reward received in epoch $t$. The closer the value of $\alpha$ is to 0, the quicker the importance of rewards diminishes.
  \end{remark}

  \begin{theorem}{Discounted Reward Converges}
    \textit{Discounted Reward} converges, thus it is reasonable to expect the \textit{Value Function} to converge.
    \[ \sum_{t=0}^\infty\alpha^t|r(X_t,Y_t)|<\infty\quad\text{where }\alpha\in(0,1) \]
  \end{theorem}

\subsubsection{Using for Approximation}

  \begin{proposition}{Approximating Finite-Horizon MDPs as Discounted Reward MDPs}
    \everymath={\displaystyle}
    Consider a \textit{Finite-Horizon MDP} as defined in \texttt{Definition 3.13}  and assume the following
    \begin{enumerate}
      \item The parameters of the \textit{Stochastic System} and the parameters of the \textit{Immediate Rewards} change slowly wrt time.
      \item $N\gg1$.
    \end{enumerate}
    Using these assumptions we can derive the following approximations of the \textit{Transition probabilities} $p_t(s'|s,a)$, \textit{Immediate Rewards} $r_t(s,a)$ and \textit{Objective Function} of this \textit{Finite-Horizon MDP} as
    \[\begin{array}{rcll}
      p_t(s'|s,a)&\approx&p(s'|s,a)&\text{By i)}\\\\
      r_t(s,a)&\approx&r(s,a)&\text{By i)}\\\\
      \left|\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right|&\gg&|r_N(X_N)|&\text{By ii)}\\
      \implies\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right]&\approx&\expect^\pi\left[\sum_{t=0}^{N_1}r_t(X_t,Y_t)\right]\\
      &\approx&\expect^\pi\left[\sum_{t=0}^{N_1}r(X_t,Y_t)\right]\\
      &\approx&\lim_{N\to\infty}\expect^\pi\left[\sum_{t=0}^{N_1}r(X_t,Y_t)\right]\\
      &=&\expect^\pi\left[\sum_{t=0}^\infty r(X_t,Y_t)\right]\\
      &\approx&\expect^\pi\left[\sum_{t=0}^\infty \alpha^tr(X_t,Y_t)\right]&\text{for }\alpha\approx1\footnotemark\\
    \end{array}\]
    \footnotetext{See \texttt{Remark 3.9}}
    These approximations can form the definition of a \textit{Discounted Reward Infinite-Horizon MDP}.
  \end{proposition}

  \begin{remark}{The approximation is well-defined}
    \everymath={\displaystyle}
    It is possible that the penultimate expression in \texttt{Proposition 3.5} is \underline{not} well-defined\footnote{i.e. It may not have a finite value.}. To overcome this, we multiple the reward by $\alpha^t$ for $\alpha\in(0,1)$.
    \[ \expect^\pi\left[\sum_{t=0}^\infty r(X_t,Y_t)\right]\approx\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right]\text{ for }\alpha\approx1 \]
    \par Since the \textit{State-Space} $S$ and the \textit{Action-Space} $A$ are both finite-sets, there is a finite upper-bound to the reward recieved
    \[\begin{array}{rcl}
      \max_{s\in S,a\in A(s)}|r(s,a)|&=&c<\infty\\
      \implies\sum_{t=0}^\infty\alpha^t|r(X_t,Y_t)|&\leq&\sum_{t=0}^\infty\alpha^tc\\
      &=&\frac{c}{1-\alpha}<\infty
    \end{array}\]
    Hence, the expected discounted reward is well-defined and finite.
  \end{remark}

  \begin{remark}{Quality of Approximation}
    \everymath={\displaystyle}
    We have that
    \[\begin{array}{rcl}
      \expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right]&\approx&\expect^\pi\left[\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\text{ since }N\gg1\\
      &\approx&\expect^\pi\left[\sum_{t=0}^{N-1}\alpha^tr(X_t,Y_t)\right]\text{ since }\alpha\approx1\\
      &\approx&\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\right]\text{ since }N\gg1
    \end{array}\]
    Given this, we can conclude that \textit{Discounted Reward MDPs} accurately approximate \textit{Finite-Horizon MDPs} under the following assumptions
    \begin{enumerate}
      \item $\alpha\in(0,1)$ and $\alpha\approx1$.
      \item The parameters of the stochastic system and the immediate rewards change slowly in time.
      \item $N\gg1$.
    \end{enumerate}
  \end{remark}

\subsubsection{Optimisation}

  \begin{definition}{Value Function $v^\pi(\cdot)$ and Optimal Value Function $v^*(\cdot)$}
    \everymath={\displaystyle}
    The \textit{Value Function} $v^\pi(\cdot)$ of a policy $\pi\in HR(T)$ is the expected total discounted reward when using that policy, given the initial state of system $X_0=s$.
    \[ v^\pi(s):=\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\bigg|X_0=s\right] \]
    The \textit{Optimal Value Function} $v^*(\cdot)$ is the maximum expected total reward, given the initial state of the system is $X_0=s$.
    \[\begin{array}{rcl}
      v^*(s)&:=&\max_{\pi\in HR(T)}v^\pi(s)\\
      &:=&\max_{\pi\in HR(T)}\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\bigg|X_0=s\right]
    \end{array}\]
  \end{definition}

  \begin{theorem}{Optimality Principle for $FHMDP(T_{N+1})$}
    \everymath={\displaystyle}
    Let $u_N^*(s)$ be the maximum expected discounted reward for an \textit{Approximated Finite-Horizon MDP} over $N$ epochs, given the system starts in state $X_0=s$.
    \[ u_N^*(s):=\max_{\pi\in HR(T_N)}\expect^\pi\left[\sum_{t=0}^N\alpha^tr(X_t,Y_t)\bigg|X_0=s\right]\quad\text{where }T_n:=\{0,\dots,N-1\} \]
    The \textit{Optimality Principle} for an \textit{Approximated Finite-Horizon MDP} over $N+1$ epochs gives a recursive definition for $u_{N+1}^*(s)$
    \[ u_{N+1}^*(s)=\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}u_N^*p(s'|s,a)\right) \]
  \end{theorem}

  \begin{definition}{Bellman Equation}
    The \textit{Bellman Equation} is the \textit{Optimality Equation} for a \textit{Discounted Reward MDP}, stated as
    \[ v^*(s)=\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right) \]
    where $v^*(\cdot)$ is unknown\footnote{And thus the function we wish to find.}
  \end{definition}

  \begin{proposition}{Derivation of Bellman Equation}
    \everymath={\displaystyle}
    We want to establish the relationship between $u_N^*(\cdot)$ and $v^*(\cdot)$.\footnote{i.e. relate the optimality equation for a \textit{Finite-Horizon MDP} $u_N^*(\cdot)$ to the optimality equation for a \textit{Discounted Reward MDP} $v^*(\cdot)$}
    \par Since \textit{Discounted Reward} converges\footnote{See \texttt{Theorem 3.4}.}
    \[\begin{array}{rrcll}
      &\lim_{N\to\infty}u_N^*(s)&=&\lim_{N\to\infty}\left(\max_{\pi\in HR(T)}\expect^\pi\left[\sum_{t=0}^N\alpha^tr(X_t,Y_t)\bigg|X_0=s\right]\right)\\
      &&=&\max_{\pi\in HR(T)}\expect^\pi\left[\lim_{N\to\infty}\left(\sum_{t=0}^N\alpha^tr(X_t,Y_t)\right)\bigg|X_0=s\right]\\
      &&=&\max_{\pi\in HR(T)}\expect^\pi\left[\sum_{t=0}^\infty\alpha^tr(X_t,Y_t)\bigg|X_0=s\right]\\
      &&=&v^*(s)\text{ by def.}\\
      \implies&v^*(s)&=&\lim_{N\to\infty}u_{N+1}^*(s)&\text{ by above}\\
      &&=&\lim_{N\to\infty}\left(\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}u_N^*(s')p(s'|s,a)\right)\right)&\text{ by Optimality Principle}\\
      &&=&\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}\left(\lim_{N\to\infty}u_N^*(s')\right)p(s'|s,a)\right)\\
      &&=&\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right)&\text{ by above}\\
      \implies&v^*(s)&=&\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right)\footnotemark
    \end{array}\]
    \footnotetext{This is the \textit{Bellman Equation}.}
  \end{proposition}

  \begin{remark}{Compact Bellman Equation}
    The \textit{Bellman Equation} can be written as\footnote{This equation is known as the \textit{Fixed-Point Equation} and is used in the \textit{Banach Fixed Point Theorems} (\texttt{Theorem 3.8,3.9} )}
    \[ (Tv)(s)=v(s) \]
    where $v(\dot)$ is unknown and $T:(S\to\reals)\to(S\to\reals)$ is the following transform
    \[ T(v(s)):=\max_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s\in S'}v(s')p(s'|s,a)\right) \]
  \end{remark}

  \begin{theorem}{Transform $T$ is a Contractive Mapping}
    Let $v',v'':S\to\reals$ be value functions\footnote{ie Value functions for different policies} and $\alpha\in(0,1)$ be the \textit{Discount Factor}
    \par The transform $T$, defined in \texttt{Remark 3.12}, is a \textit{Contractive Mapping} since $\alpha\in(0,1)$
    \[ \|(Tv')(s)-(Tv'')(s)\|\leq\alpha\|v'(s)-v''(s)\|\ \forall\ s\in S \]
  \end{theorem}

  \begin{definition}{Transform $T_d$}
    Let $d:S\to A$ be a \textit{Markovian Decision Function}.
    \par We define the transform $T_d:(S\to\reals)\to(S\to\reals)$ as
    \[ (T_dv)(s):=r(s,d(s))+\alpha\sum_{s'\in S}v(s')p(s'|s,d(s))\text{ where }v\in V,s\in S \]
    This is the expected reward from the single epoch when the system in state $s$ and decision function $d$ is used.
  \end{definition}

  \begin{theorem}{Bounding Distance between Transformations}
    Let $v',v'':S\to\reals$ be value functions and $\alpha\in(0,1)$ be the \textit{Discount Factor}
    \par The transform $T_d$, defined in \texttt{Definition 3.21}, is a \textit{Contractive Mapping} since $\alpha\in(0,1)$
    \[ \|(T_dv')(s)-(T_dv'')(s)\|\leq\alpha\|v'(s)-v''(s)\|\ \forall\ s\in S \]
  \end{theorem}

  \begin{theorem}{Banach Fixed-Point Theorem Applied to Transform $T$}
    \everymath={\displaystyle}
    The following is the \textit{Banach Fixed-Point Theorem} applied to transform $T$.
    \begin{enumerate}
      \item Let $v_0:S\to\reals$ be an arbitrary value function and $\{v_k\}_{k\geq0}$ be recusively defined as
      \[ v_{k+1}(s):=(Tv_k)(s)\footnotemark\implies v_{k+1}(s)=(T^{k+1}v_0)(s) \]
      \footnotetext{This is known as the \textit{Fixed-Point Recursion}}
      \par Then, after applying transform $T$ sufficiently many times to $v_0$ we get a solution to the \textit{Compact Bellman Equation}
      \[\begin{array}{rrcll}
        &\lim_{k\to\infty}v_k(s)=\lim_{k\to\infty}(T^kv)(s)&=&v(s)&\forall\ s\in S\\
        \text{and }&(Tv)(s)&=&v(s)&\forall\ s\in S\\
      \end{array}\]
      Moreover, we can bound how close we are to a solution
      \[\begin{array}{rrcll}
        &\|v_k(s)-v(s)\|&\leq&\frac{\alpha^k\|v_1(s)-v_0(s)\|}{1-\alpha}&\forall\ s\in S,k\geq1\\
        \Longleftrightarrow&\|(T^kv_0)(s)-v(s)\|&\leq&\frac{\alpha^k\|(Tv_0)(s)-v_0(s)\|}{1-\alpha}&\forall\ s\in S,k\geq1
      \end{array}\]
      \item
        \[\text{If }\exists\ v':S\to\reals\text{ st }\forall\ s\in S,\ (Tv')(s)=v'(s)\implies v'(s)=v(s)\ \forall\ s\in S  \]
    \end{enumerate}
  \end{theorem}

  \begin{theorem}{Banach Fixed-Point Theorem Applied to Transform $T_d$}
    The following is the \textit{Banach Fixed-Point Theorem} applied to transform $T$.
    \par Let $d:S\to A$ be a \textit{Markovian Decision Function}. Then there is a unique solution $v_d:S\to\reals$ to the \textit{Copact Bellman Equation} for transform $T_d$
    \[ \exists!\ v_d:S\to\reals\text{ st }\forall\ s\in S,\ (T_dv_d)(s)=v_d(s) \]
    Moreover, this solution $v_d(\cdot)$ is equivalent to the value function $v^{\pi_d}(\cdot)$ for $\pi_d$\footnote{$\pi_d$ is the stationary policy based on decision policy $d$.}
    \[ v_d(s)=v^{\pi_d}(s)\quad\forall\ s\in S \]
  \end{theorem}

  \begin{theorem}{Bounding Distance between Optimal Value Function over Infinite $v^*$ and Finite $v_N^*$ Horizons}
    Let $v^*$ be the \textit{Optimal Value Function} over an infinite time-horizon and $v^*_N$ be the \textit{Optimal Value Function} over a finite time-horizon with $N$ epochs.
    \par Then
    \[ \forall\ N\geq1,\ \|v_N^*-v^*\|\leq \frac{\alpha^Nc}{1-\alpha}\text{ where }c:=\max_{s\in S,a\in A(s)}|r(s,a)| \]
  \end{theorem}

  \begin{theorem}{Solution to Bellman Equation}
    The \textit{Optimal Value Function} $v^*$ is the unique solution to the \textit{Bellman Equation}
    \[ (Tv^*)(s)=v^*(s)\ \forall\ s\in S \]
    Since $v^*(s)=v^{\pi^*}(s)\ \forall\ s\in S$ then we can deduce the \textit{Optimal Decision Rule} is
    \[ d^*(s)\in\argmax_{a\in A(s)}\left(r(s,a)+\alpha\sum_{s'\in S}v^*(s')p(s'|s,a)\right) \]
    The \textit{Optimal Policy} is $\pi^*=\pi_{d^*}$.
  \end{theorem}

  \begin{remark}{$\pi_{d^*}$ is an Optimal Policy}

  \end{remark}

\subsubsection{Policy Iteration Algorithm}

  \begin{definition}{Policy Iteration Algorithm}
    The \textit{Policy Iteration Algorithm} is an algorithm for finding an optimal decision policy $\pi^*$ for an \textit{Discounted Reward MDP}. Here are the stages of the \textit{Policy Iteration Algorithm}
    \begin{itemize}
      \item \textit{Initialisation} - Arbitrarily choose a \textit{Markovian Decision Function} $d_0(s)$ and set $k=0$.
      \item \textit{Body} - For $k\geq0$ perform the following
      \begin{enumerate}
        \item \textit{Policy Evaluation} - Compute a solution $v_k(\cdot)$ to the \textit{Compact Bellman Equation}
        \[ (T_{d_k}v)=v \]
        where $v$ is unknown and $T_{d_k}$ is the \textit{Transform} defined in \texttt{Definition 3.21}.
        \item \textit{Policy Improvement} - Use the function $v_k$ which has just been computed, to select a \textit{Markovian Decision Function} $d_{k+1}(s)$ which, in each states $s\in S$, maximises the \textit{Bellman Equation}.
        \[ \forall\ s\in S\ d_{k+1}(s)\in\argmax_{a\in A(s)}\left(r(s,a)+\sum_{s'\in S}v_k(s')p(s'|s,a)\right)\ \forall\ s\in S \]
        \item \textit{Terminatiom?} -
        \begin{itemize}
          \item[If] $\forall\ s\in S\ d_k(s)=d_{k+1}(s)$:\footnote{If the decision function is unchanged.} Stop the algorithm and return the last calculated \textit{Markovian Decision Function} $d_{k+1}(\cdot)$.
          \item[Else]: Increment $k$ and repeat i)-iii).
        \end{itemize}
      \end{enumerate}
    \end{itemize}
  \end{definition}

  \begin{remark}{Policy Iteration Algorithm}
    Here are some properties of the \textit{Policy Iteration Algorithm}
    \begin{enumerate}
      \item The algorithm terminates after a finite number of iterations.
      \item The returned decision function is optimal $\forall\ s\in S,\ d_k(s)=d^*(s)$.
    \end{enumerate}
  \end{remark}

\subsubsection{Equivalent Linear Program}

  \begin{remark}{Linear Programming}
    \textit{Linear Programming} methods can be used to solve \textit{Discounted Reward MDPs}.
  \end{remark}

  \begin{proposition}{Equivalent Linear Programming Problem for Discounted Reward MDP}
    \everymath={\displaystyle}
    The following \textit{Linear Program} is equivalent to a \textit{Discounted Reward MDP}
    \begin{quote}
      Find $v:S\to\reals$ which minimises $\sum_{s\in S}\gamma(s)v(s)$ under the restrictions that
      \begin{enumerate}
        \item $r(s,a)+\sum_{s'\in S}\alpha p(s'|s,a)v(s')\leq v(s)\quad\forall\ s\in S,a\in A(s)$.
        \item $\gamma(s)>0\ \forall\ s\in S$
      \end{enumerate}
    \end{quote}
  \end{proposition}

\subsection{Average Reward Infinite-Horizon MDPs}

\subsubsection{Problem Formulation}

  \begin{definition}{Average Reward Infinite-Horizon MDPs}
    \everymath={\displaystyle}
    In an \textit{Average Reward Infinite-Horizon MDP} the agent is tasked to find a policy which maximises the average reward in the long-run. All \textit{Average Reward MDPs} have the following features
    \begin{itemize}
      \item \textit{Number of Epochs} - $N=\infty$.
      \item \textit{Time-Horizon} - $T=\{0,1,\dots\}$.
      \item \textit{Transition Probabilities} - $p_t(s'|s,a)=p(s'|s,a)\ \forall\ t\in T$.\footnote{The \textit{Transitions Probabilities} are stationary.}
      \item \textit{Immediate Rewards} - $r_t(s,a)=r(s,a)\ \forall\ t\in T$.\footnote{The \textit{Rewards} are stationary.}
      \item \textit{Objective} - Given the transition probabilities $p(s'|s,a)$ and immediate rewards $r(s,a)$, the agent is tasked to find a \textit{History Dependent Randomised Policy} $\pi\in HR(T)$ which maximises the expected average reward-per-epoch over the infinite time-horizon
      \[\begin{array}{rl}
        &\argmax_{\pi\in HR(T)}\left\{\lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right]\right\}\\
        =&\argmax_{\pi\in HR(T)}\left\{\lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\right\}
      \end{array}\]
    \end{itemize}
  \end{definition}

  \begin{proposition}{Stochastic System of an Average Reward MDP}
    In epoch $t$, \textit{Average Reward MDPs} have the following \textit{Stochastic System}, given all available information
    \[\begin{array}{rcl}
      (X_{t+1}|X_{0:t},Y_{0:t})&\sim&(X_{t+1}|X_t,Y_t)\\
      &\sim&p_t(\cdot|X_t,Y_t)\\
      &=&p(\cdot|X_t,Y_t)\\
    \end{array}\]
  \end{proposition}

  \begin{remark}{The problem is well-defined}
    \everymath={\displaystyle}
    Since the \textit{State-Space} $S$ and \textit{Action-Space} $A$ are finite-sets, the maximum reward is finite and thus the average reward over finite-time is finite.
    \[\begin{array}{rrcl}
      &c:=\max_{s\in S,a\in A(s)}|r(s,a)|&<&\infty\\
      \implies&\left|\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right|&\leq&\frac1N\sum_{t=0}^{N-1}|r(X_t,Y_t)|\leq c
    \end{array}\]
    Therefore the limit of the infimum and supremum exist and are finite
    \[\begin{array}{rcl}
      \lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\quad\text{and}\quad\lim_{N\to\infty}\sup\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]
    \end{array}\]
  \end{remark}

  \begin{remark}{Average Reward MDP vs Discounted Reward MDP}
    An \textit{Average Reward MDP} places the same emphasis on all received rewards. A \textit{Discounted Reward MDP} only does this if the \textit{Discount Factor} $\alpha$ is close to 1.
    \par If $\alpha\approx0$ then a \textit{Discounted Reward MDP} places significantly more emphasis on near-future rewards than far-future.
  \end{remark}

  \begin{remark}{Average Reward MDPs are similar to Irreducible Markov Chains}
    See \texttt{Subsection 0.5}  for details on \textit{Irreducible Morkov Chains}.
  \end{remark}

\subsubsection{Using for Approximation}

  \begin{proposition}{Approximating Finite-Horizon MDP as Average Reward MDP}
    \everymath={\displaystyle}
    Consider a \textit{Finite-Horizon MDP} as defined in \texttt{Definition 3.13} and assume the following
    \begin{enumerate}
      \item The parameters of the \textit{Stochastic System} and the parameters of the \textit{Immediate Rewards} change slowlt wrt time.
      \item $N\gg1$.
    \end{enumerate}
    Using thses assumptions we can derive the following approximation of the \textit{Transition Probabilities} $p_t(s'|s,a)$, \textit{Immediate Rewards} $r_t(s,a)$ of this \textit{Finite-Horizon MDP}
    \[\begin{array}{rcll}
      p_t(s'|s,a)&\approx&p(s'|s,a)&\text{By i)}\\
      \\
      r_t(s,a)&\approx&r(s,a)&\text{By i)}\\
      \\
      \left|\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right|&\gg&|r_N(X_N)|&\text{By ii)}\\
      \implies\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right]&\approx&\expect^\pi\left[\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right]\\
      &\approx&\expect^\pi\left[\sum_{t=0}^{N-1}r(X_t,Y_t)\right]&\text{By i)}
    \end{array}\]
    Since introducing a multiplicative constant does affect the $\argmax$ of an expression, we have that the following expressions are all equivalent. Thus optimising the total reward and average reward are equivalent objectives.
    \[\begin{array}{rcl}
    \argmax_\pi\expect^\pi\left[\sum_{t=0}^{N-1}r(X_t,Y_t)\right]&=&\argmax_\pi\frac1N\expect^\pi\left[\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\\
    &=&\argmax_\pi\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\\
    \end{array}\]
    We can approximate this \textit{Objective Function} using a limit
    \[\begin{array}{rcll}
      \implies\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right]&\approx&\lim_{N\to\infty}\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]&\text{By ii)}\\
      &=&\lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]&\footnotemark
    \end{array}\]
    \footnotetext{We take the infimum to ensure the limit exists.}
    These approximations can form the definition of an \textit{Average Reward Infnite-Horizon MDP}.
  \end{proposition}

  \begin{proposition}{Quality of Approximation}
    \everymath={\displaystyle}
    We have that
    \[\begin{array}{rl}
      &\argmax_\pi\expect^\pi\left[\left(\sum_{t=0}^{N-1}r_t(X_t,Y_t)\right)+r_N(X_N)\right]\\
      \approx&\argmax_\pi\expect^\pi\left[\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\text{ since }N\gg1\\
      =&\argmax_\pi\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\\
      =&\argmax_\pi\left(\lim_{N\to\infty}\inf\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\right)\text{ since }N\gg1
    \end{array}\]
  \end{proposition}

\subsubsection{Optimisation}

  \begin{definition}{Bellman Equation}
    The \textit{Bellman Optimality Equation} for an \textit{Average Reward MDP}, stated as
    \[ w^*(s)+r^*=\max_{a\in A(s)}\left(r(s,a)+\sum_{s'\in S}w^*(s')p(s'|s,a)\right) \]
    where $w^*:S\to\reals,\ r^*\in\reals$ are unkowns to be found.\footnote{$w^*$ represents an invariant distribution in the stochastic system. $r^*$ represents the reward value.}
  \end{definition}

  \begin{definition}{Optimal Markovian Decision Function $d^*(\cdot)$}
    The \textit{Optimal Markovian Decision Function} $d^*:S\to A$ for an \textit{Average Reward MDP} is on which chooses the action $a\in A(s)$ which maximises the RHS of the \textit{Bellman Equation}
    \[ d^*(s)\in\argmax_{a\in A(s)}\left(r(s,a)+\sum_{s'\in S}w^*(s')p(s'|s,a)\right) \]
  \end{definition}

  \begin{theorem}{Solutions to the Bellman Equation Exist}
    There exist $w^*:S\to\reals,\ r^*\in\reals$ which staisfy the \textit{Bellman Equation} for an \textit{Average Reward MDP} (\texttt{Definition 3.25}).
  \end{theorem}

  \begin{theorem}{$r^*$ is the Maximum Asymptotic Expected Average Reward}
    Let $(w^*(s),r^*)$ be a solution-pair for the \textit{Bellman Equation} for an \textit{Average Reward MDP}.
    \par Then
    \[ \forall\ \pi\in HR(T),\ \lim_{N\to\infty}\sup\expect^\pi\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]\leq r^* \]
    Further, let $d^*:S\to A$ be the \textit{Optimal Markovian Decision Function} and $\pi^*$ be the \textit{Decision Policy} based on $d^*(s)$. Then
    \[ \lim_{N\to\infty}\sup\expect^{\pi^*}\left[\frac1N\sum_{t=0}^{N-1}r(X_t,Y_t)\right]=r^* \]
  \end{theorem}

  \begin{theorem}{Uniqueness of Solutions to the Bellman Equation}
    Consider any two solutions-pairs $(w^*(s),r^*)$ and $(\tilde{w}^*(s),\tilde{r}^*(s))$ for the \textit{Bellman Equation} for an \textit{Average Reward MDP}. Then
    \begin{enumerate}
      \item The $r$-part will be the same in both solutions.
      \[ r^*=\tilde{r}^* \]
      \item The $w$-part will only differ by an additive constant
      \[ \exists\ c\in\reals\text{ st }\forall\ s\in S,\ w^*(s)=\tilde{w}^*(s)+c \]
    \end{enumerate}
  \end{theorem}

\subsubsection{Policy Iteration Algorithm}

  \begin{definition}{Policy Iteration Algorithm}
    \everymath={\displaystyle}
    The \textit{Policy Iteration Algorithm} is an algorithm for finding an optimal decision policy $\pi^*$ for an \textit{Average Reward MDP}. Here are the stages of the \textit{Policy Iteration Algorithm}.
    \begin{itemize}
      \item \textit{Initialisation} - Arbitratily choose a \textit{Markovian Decision Function} $d_0(s)$ and set $k=0.$
      \item \textit{Body} - For $k\geq0$ perform the following:
      \begin{enumerate}
        \item \textit{Policy Evaluation}
        \begin{itemize}
          \item Compute a solution $\mu_k(\cdot)$ to the following equations\footnote{This is the \textit{Invariant Mass Function} for the current decision function $d_k$.}
          \[\begin{array}{rcl}
            \sum_{s'\in S}\mu(s')&=&1\\
            \mu(s)&=&\sum_{s'\in S}\mu(s')p(s|s',d(s'))
          \end{array}\]
          where $\mu(\cdot)$ is the unknown to be found.
          \item Using this $\mu_k(\cdot)$, compute a solution $w_k(\cdot)$ to the following set of equations\footnote{This is the \textit{Poisson Equation} associated with function $r(s,d_k(s))$ and the transition kernel $p(\cdot|s,d_k(s))$.}
          \[\begin{array}{rll}
            w(s)-\sum_{s'\in S}w(s')p(s'|s,d_k(s))&=&r(s,d_k(s))-r_k\\
            \text{where }r_k&:=&\sum_{s\in S}r(s,d_k(s))\mu_k(s)
          \end{array}\]
          where $w(\cdot)$ is the unkown to be found. Note that $r_k$ is defined explicitly given we known $\mu_k(\cdot)$.
        \end{itemize}
        \item \textit{Policy Improvement} - Select a \textit{Markovian Decision Policy} $d_{k+1}(\cdot)$ which, in each state $s\in S$, chooses an action which maximises the \textit{Bellman Equation}.
        \[ \forall\ s\in S\ d_{k+1}(s)\in\argmax_{a\in A(s)}\left\{r(s,a)+\sum_{s'\in S}w_k(s')p(s'|s,a)\right\} \]
        \item \textit{Termination?}
        \begin{itemize}
          \item[If] $\forall\ s\in S\ d_k(s)=d_{k+1}(s)$:\footnote{If the decision function is unchanged.} Stop the algorithm and return $d_{k+1}(\cdot)$.
          \item[Else]: Increment $k$ and repeat i)-iii).
        \end{itemize}
      \end{enumerate}
    \end{itemize}
  \end{definition}

  \begin{theorem}{Optimality of the Policy Iteration Algorithm}
    The following are properties of the the \textit{Policy Iteration Algorithm} for \textit{Average Reward MDPs}
    \begin{enumerate}
      \item The algorithm terminates after a finite number of iterations.
      \item The returned decision function is optimal $\forall\ s\in S,\ d_k(s)=d^*(s)$.
    \end{enumerate}
  \end{theorem}

\subsubsection{Equivalent Linear Program}

  \begin{remark}{Linear Programming}
    \textit{Linear Programming} methods can be used to solve \textit{Discounted Reward MDPs}.
  \end{remark}

  \begin{proposition}{Equivalent Linear Programming Problem for Average Reward MDP}
    \everymath={\displaystyle}
    The following \textit{Linear Program} is equivalent to an \textit{Average Reward MDP}.
    \begin{quote}
      Minimise $r\in\reals$ under the restrictions that
      \begin{itemize}
        \item $r(s,a)+\sum_{s'\in S}p(s'|s,a)w(s')\leq r+w(s)\quad\forall\ s\in S,\forall\ a\in A(s)$.\footnote{Both $r\in\reals$ and $w:S\to\reals$ are unknown. IDK what to do about $w$.}
      \end{itemize}
    \end{quote}
  \end{proposition}

  \begin{theorem}{Optimality of Equivalent Linear Programming Problem}
    Let $(\hat{r},\hat{w}(s))$ be an optimal solution to the \textit{Linear Program} defined in \texttt{Proposition 3.11} and $\hat{d}(\cdot)$ be a \textit{Markovian Decision Function} which chooses actions which maximise the RHS of the \textit{Bellman Equation} using the invariant distribution $\hat{w}$
    \[ \forall\ s\in S,\ \hat{d}(s)\in\argmax_{a\in A(s)}\left(r(s,a)+\sum_{s'\in S}\hat{w}(s')p(s'|s,a)\right) \]
    Then the decision function $\hat{d}(\cdot)$ will be optimal
    \[ \forall\ s\in S,\ \hat{d}(s)=d^*(s) \]
  \end{theorem}

\newpage
\setcounter{section}{-1}
\section{Reference}

\subsection{Notation}

\subsubsection{Problem Specific}

  \begin{proposition}{Notation for Multi-Armed Bandit Problem}
    The following notation is used to simplifier analysis of the \textit{Multi-Armed Bandit Problem}
    \begin{tabular}{|rcl|l|}
      \hline
      $I(t)$&$\in$&$[1,K]$&The arm out strategy $I$ plays at time $t$.\\
      $N_j(t)$&$:=$&$\displaystyle\sum_{s=1}^t\indexed(I(s)=j)$&The number of times arm $j$ has been played in the first $t$ rounds.\\
      $S_j(t)$&$:=$&$\displaystyle\sum_{s=1}^tX_j(s)\indexed(I(s)=j)$&The total reward from arm $j$ in the first $t$ rounds.\\
      $\hat\mu_{j,n}$&$:=$&$\displaystyle\frac{S_j(t)}{N_j(t)}$&The sample mean reward from arm $j$ in the first $n$ plays of arm $j$.\\
      $\Delta_i$&$:=$&$(\mu^*-\mu_i)$ & The reward lost from playing arm $i$ rather than the optimal arm.\\
      \hline
    \end{tabular}
  \end{proposition}

  \begin{proposition}{Notation for Stochastic Optimisation Processes}
    The following notation is used to simplifier analysis of the \textit{Stochastic Optimisation Processes}
    \begin{center}
      \begin{tabular}{|r|l|}
        \hline
        $X_t$&System state at the start of epoch $t$.\\
        $Y_t$&Agent action in epoch $t$.\\
        $T$&The time horizon.\\
        $A$&Action-space.\\
        $A(s)$&Admissible action-space.\\
        $S$&State-space.\\
        $p_t(s'|s,a)$&Transition probabilities.\\
        $q_t(a|s)$&Policy decision probabilities.\\
        \hline
      \end{tabular}
    \end{center}
  \end{proposition}

\subsection{Definitions}

  \begin{definition}{Jacobian $J(\cdot)$}
    Let $f:\reals^n\to\reals^m$ and $\x\in\reals^n$.
    \[ J_f(\x)=\begin{pmatrix}
      \frac{\partial f_1}{\partial x_1}(\x)&\dots&\frac{\partial f_m}{\partial x_1}(\x)\\
      \vdots&\ddots&\vdots\\
      \frac{\partial f_1}{\partial x_n}(\x)&\dots&\frac{\partial f_m}{\partial x_n}(\x)
    \end{pmatrix} \]
  \end{definition}

\subsection{Theorems}

  \begin{theorem}{Relationship between Beta \& Gamma Distribution}
    Let $X\sim\text{Gamma}(\alpha,\lambda)$ and $Y\sim\text{Gamma}(\beta,\lambda)$ (ie shared scale parameter but different shape parameters). Then
    \[ V:=\frac{X}{X+Y}\sim\text{Beta}(\alpha,\beta) \]
    \textit{A proof for this is given in the full notes.}
  \end{theorem}

  \begin{theorem}{Result for Poisson Processes}
    Let $\{N_s\}_{s\in\nats}$ be a poisson process with intensity $\lambda>0$ and fix $n,t\in\nats$. Then, given that $N_t=n$, the random times at which the process sees an increment in time $[0,t]$ are mutually independent and uniformly distributed on $[0,t]$
  \end{theorem}

  \begin{theorem}{Relationship between Beta and Bernoulli Random Variables}
    Let $X\sim\text{Beta}(\alpha,\beta)$ for $\alpha,\beta\in\nats$ and $Y\sim\text{Bin}(\alpha+\beta-1,p)$ for $p\in(0,1)$. Then
    \[ \prob(X>p)=\prob(Y\leq\alpha-1) \]
  \end{theorem}

  \begin{proof}{Theorem 0.3}
    Let $\{N_t\}_{t\in\nats}$ be a poisson process with unit-intensity and $T_n$ be the time of the $n^{th}$ increment.
    \par Let $X\sim\text{Beta}(\alpha,\beta)$ then by \texttt{Theorem 0.1} we can write $X=\frac{V}{V+W}$ where $V,W$ are independent with distributions $V\sim\text{Gamma}(\alpha,1),\ W\sim\text{Gamma}(\beta,1)$. If $\alpha,\beta$ are integers then we can interpret $T_\alpha\sim V$ and $(T_{\alpha+\beta}-T_\alpha)\sim W$.
    \par Hence, the following events are equivalent
    \[ \{X>p\}\Longleftrightarrow\left\{\frac{T_\alpha}{T_{\alpha+\beta}}>p\right\}\Longleftrightarrow\{T_\alpha>pT_{\alpha+\beta}\} \]
    $N_t$ increments $\alpha+\beta-1$ times in $(0,T_{\alpha+\beta})$. By \texttt{Theorem 0.2}, these increments are uniformly and independently distributed in $[0,T_{\alpha+\beta}]$.
    \par Hence the number of increments in time $[0,pT_{\alpha+\beta}]$ has a $\text{Bin}(\alpha+\beta-1,p)$ distribution. This the same distribution as $Y$ from the stated theorem.
    \par The event $\{T_\alpha>pT_{\alpha+\beta}\}$ is the event that the number of increments of $N_t$ in $[0,T_{\alpha+\beta}]$ is at most $\alpha-1$. Meaning the following events are equivalent
    \[ \{T_\alpha>pT_{\alpha+\beta}\}\Longleftrightarrow\{Y\leq \alpha-1\} \].
    Thus, we have a full chain of equivalent events
    \[\begin{array}{rl}
    &\{X>p\}\Longleftrightarrow\left\{\frac{T_\alpha}{T_{\alpha+\beta}}>p\right\}\Longleftrightarrow\{T_\alpha>pT_{\alpha+\beta}\}\Longleftrightarrow\{Y\leq \alpha-1\}\\
    \implies&\{X>p\}\Longleftrightarrow\{Y\leq \alpha-1\}\\
    \implies&\prob(X>p)\Longleftrightarrow\prob(Y\leq\alpha-1)
    \end{array}\]
    The result of the theorem.
    \proved
  \end{proof}

\subsection{Conjugate Priors}

  {
  \hspace*{-1cm}
    \begin{tabular}{|l|l|l|l|}
      \hline
      \textbf{Reward Distribution} $X$&\textbf{Prior} $\pi_0$&\textbf{Posterior} $\pi_1(\cdot|x)$&\textbf{Proof}\\
      \hline
      $\text{Bernoulli}(p)$ with $p$ unknown&$\pi_0(p)\sim\text{Beta}(\alpha,\beta)$&$\pi_1(p|x)\sim\begin{cases}\text{Beta}(\alpha+1,\beta)&\text{if }x=1\\\text{Beta}(\alpha,\beta+1)&\text{if }x=0\end{cases}$&\texttt{Proof 0.2}\\
      \hline
      $\text{Poisson}(\lambda)$ with $\lambda$ unknown&$\pi_0(\lambda)\sim\text{Gamma}(\alpha,\beta)$&$\pi_1(\lambda|n)\sim\text{Gamma}(\alpha+n,\beta+1)$&\texttt{Proof 0.3}\\
      \hline
      $\text{Normal}(\mu,1)$ with $\mu$&$\pi_0\sim\text{Normal}(\mu_0,\sigma_0^2)$&$\pi_1(\mu|x)\sim\text{Normal}\left(\frac{\mu_0+x\sigma_0^2}{1+\sigma_0^2},\frac{\sigma_0^2}{1+\sigma_0^2}\right)$&\texttt{Proof 0.4}\\
      \hline
    \end{tabular}
  }

  \begin{proof}{Beta Distributions are Conjugate Priors for Bernoulli Observations}
    Let $X\sim\text{Bern}(\mu)$, let $\pi_0\sim\text{Beta}(\alpha,\beta)$ be the prior for $\mu$ and $\pi_1(\cdot|X)$ be the posterior distribution for $\mu$ given $X$ was observed. This means
    \[ \pi_1(\mu|x)\propto\pi_0(\mu)p_X(x) \]
    Note that
    \[ \pi_0(\mu)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\mu^{\alpha-1}(1-\mu)^{\beta-1}\quad\text{and}\quad p_X(x)=\begin{cases}\mu&\text{x=1}\\1-\mu&\text{x=0}\end{cases} \]
    First, consider the case when $X=1$
    \[\begin{array}{rcl}
      \pi_1(\mu|X=1)&\propto&\pi_0(\mu)p_X(1)\\
      &\propto&[\mu^{\alpha-1}(1-\mu)^{\beta-1}]\cdot\mu\text{ (only terms involving $\mu$)}\\
      &=&\mu^\alpha(1-\mu)^{\beta-1}\\
      &\sim&\text{Beta}(\alpha+1,\beta)
    \end{array}\]
    Now, consider the case when $X=0$
    \[\begin{array}{rcl}
      \pi_1(\mu|X=0)&\propto&\pi_0(\mu)p_X(0)\\
      &\propto&[\mu^{\alpha-1}(1-\mu)^{\beta-1}]\cdot(1-\mu)\text{ (only terms involving $\mu$)}\\
      &=&\mu^{\alpha-1}(1-\mu)^{\beta}\\
      &\sim&\text{Beta}(\alpha,\beta+1)
    \end{array}\]
    Combining these two cases we get the result of the theorem
    \[ \pi_1(\mu|x)\sim\begin{cases}\text{Beta}(\alpha+1,\beta)&\text{if }x=1\\\text{Beta}(\alpha,\beta+1)&\text{if }x=0\end{cases} \]
    \proved
  \end{proof}

  \begin{proof}{Gamma Distributions are Conjugate Priors for Poisson Observations}
    Let $X\sim\text{Poisson}(\lambda)$ where $\lambda$ is unknown, $\pi_0$ be the prior distribution for $\lambda$ and $\pi_1(\cdot|n)$ be the posterior distribution for $\lambda$, given the value $n$ was sampled from $X$. This means
    \[ \pi_1(\lambda|n)\propto\pi_0(\lambda)p_\lambda(n) \]
    where $p_\lambda(n):=\prob(X=n)$ given $X\sim\text{Poisson}(\lambda)$.
    \par Suppose $\pi_0\sim\text{Gamma}(\alpha,\beta)$ and note that
    \[ p_\lambda(n)=\frac{\lambda^ne^{-\lambda}}{n!}\quad\text{and}\quad \pi_0(\lambda)=\frac{\lambda^{\alpha-1}e^{-\lambda\beta}}{\Gamma(\alpha)\beta^\alpha} \]
    As we are considering proportionality wrt $\lambda$, we can ignore terms which do not involve $\lambda$. Giving
    \[ p_\lambda(n)\propto\lambda^ne^{-\lambda}\quad\text{and}\quad \pi_0(\lambda)\propto\lambda^{\alpha-1}e^{-\lambda\beta} \]
    Using these results we can build an expression for the posterior $\pi_1(\cdot|n)$
    \[\begin{array}{rcl}
        \pi_1(\lambda|n)&\propto&\pi_0(\lambda)p_\lambda(n)\\
        &\propto&\left(\lambda^{\alpha-1}e^{-\lambda\beta}\right)\cdot\left(\lambda^ne^{-\lambda}\right)\\
        &=&\lambda^{n+\alpha-1}e^{-\lambda(\beta+1)}
    \end{array}\]
    By comparing this expression to that of a Gamma distribution we have that
    \[ \pi_1(\lambda|n)\sim\text{Gamma}(\alpha+n,\beta+1) \]
    \proved
  \end{proof}

  \begin{proof}{Normal Distributions are Conjugate Priors for Normal Distributions with Unit Variance}
    Let $X\sim\text{Normal}(\theta,1)$ with $\theta$ unknown and fix $\mu_0\in\reals,\sigma^2_0>0$.\\
    Let $\pi_0\sim\text{Normal}(\mu_0,\sigma^2_0)$ be the prior for $\theta$ and $\pi_1(\cdot|x)$ be the posterior for $\theta$ given $x$ is observed from $X$. This means
    \[ \pi_1(\theta|x)\propto\pi_0(\theta)f_\theta(x)\quad\text{where }f_\theta(x):=\prob(X=x|\theta) \]
    Note that
    \[ f_\theta(x)=\frac1{\sqrt{2\pi}}e^{-\frac12(x-\theta)^2}\quad\text{and}\quad\pi_0(\theta)=\frac1{\sqrt{2\pi\sigma^2_0}}e^{-\frac12\frac{(\theta-\mu_0)^2}{\sigma_0^2}} \]
    By considering only the terms involving $\theta$ we have
    \[ f_\theta(x)\propto e^{-\frac12(x-\theta)^2}\quad\text{and}\quad\pi_0(\theta)\propto e^{-\frac12\frac{(\theta-\mu_0)^2}{\sigma_0^2}} \]
    This means
    \[ \pi_1(\theta|x)\propto e^{-\frac12(x-\theta)^2}\cdot e^{-\frac12\frac{(\theta-\mu_0)^2}{\sigma_0^2}}=\exp\left(-\frac12\left((x-\theta)^2+\frac{(\mu_0-\theta)^2}{\sigma_0^2}\right)\right)\]
    Consider just the term of the exponent involving $\theta$
    \[\begin{array}{rl}
      &(x-\theta)^2+\frac{(\mu_0-\theta)^2}{\sigma_0^2}\\
      =&x^2-2x\theta+\theta^2+\frac1{\sigma_0^2}\left(\mu_0^2-2\mu_0\theta+\theta^2\right)\\
      \propto&-2x\theta+\theta^2+\frac1{\sigma_0^2}\left(-2\mu_0\theta+\theta^2\right)\\
      =&\frac1{\sigma_0^2}\left[-2\sigma_0^2x\theta+\sigma_0^2\theta^2-2\mu_0\theta+\theta^2\right]\\
      =&\frac1{\sigma_0^2}\left[\theta^2(1+\sigma^2_0)-2\theta(\mu_0+x\sigma^2_0)\right]\\
      =&\frac{1+\sigma^2}{\sigma_0^2}\left[\theta^2-2\theta\left(\frac{\mu_0+x\sigma^2_0}{1+\sigma^2_0}\right)\right]\\
      \propto&\frac{1+\sigma^2}{\sigma_0^2}\left(\theta-\left(\frac{\mu_0+x\sigma^2_0}{1+\sigma^2_0}\right)\right)^2\text{ by completing the square}
    \end{array}\]
    Substituting this result back into the expression for the posterior gives
    \[\begin{array}{rcl}
    \pi_1(\theta|x)&\propto&\exp\left(-\frac12\cdot\frac{\left(\theta-\left(\frac{\mu_0+x\sigma^2_0}{1+\sigma_0^2}\right)\right)^2}{\sigma_0^2/(1+\sigma_0^2)}\right)\\
    &\sim&\text{Normal}\left(\frac{\mu_0+x\sigma^2_0}{1+\sigma^2_0},\frac{\sigma^2_0}{1+\sigma^2_0}\right)
    \end{array}\]
    Thus $\pi_1\sim\text{Normal}(\mu_1,\sigma_1^2)$ where
    \[ \mu_1:=\frac{\mu_0+x\sigma^2_0}{1+\sigma^2_0}\quad\text{and}\quad\sigma^2_1:=\frac{\sigma^2_0}{1+\sigma^2_0} \]
    \proved
  \end{proof}

\subsection{Irreducible Markov Chains} %TODO

  \begin{definition}{Markov Chain}
    A \textit{Stochastic Process} $\{X_t\}_{t\geq}$ taking values in $S$ is a \textit{Markov Chain} if it has the \textit{Markov Property}
    \[ \prob(X_{t+1}=s_{t+1}|X_t=s_t,\dots,X_0=s_0)=\prob(X_{t+1}=s_{t=1}|X_t=s_t)\ \forall\ t\in T \]
    A \textit{Markov Chain} $\{X_t\}_{t\geq0}$ is \textit{Homogeneous} if the transitions probabilities are the same in all time-periods
    \[ \prob(X_{t+1}=s'|X_t=s)=\prob(X_1=s'|X_0=s)\ \forall\ t\in T \]
    The \textit{Transition Kernel} of a \textit{Homogeneous Markov Chain} $\{X_t\}_{t\geq0}$ is the transition probabilities
    \[ p(s'|s):=\prob(X_1=s'|X_0=s) \]
    A \textit{Homogeneous Markov Chain} $\{X_t\}_{t\geq0}$ is \textit{Irreducible} if $\forall\ s,s'\in S$ there exists $t\geq 1$ st
    \[ p^t(s'|s)>0 \]
  \end{definition}

  \begin{definition}{Invariant Probability Mass Function}
    A function $\mu(s)$ is an \textit{Invariant Probability Mass Function} of a \textit{Homogeneous Markov Chain} $\{X_t\}_{t\geq0}$ if
    \[ \mu(s)=\sum_{s'\in S}p(s|s')\mu(s') \]
  \end{definition}

  \begin{theorem}{Invariant PMF exists for all Irreducible Markov Chain}
    Let $\{X_t\}_{t\geq0}$ be an \textit{Irreducible Markov Chain}. Then the follow hold
    \begin{enumerate}
      \item $\{X_t\}_{t\geq0}$ has a unique invariant probability mass function $\mu(s)$.
      \item $\mu(s)>0\ \forall\ s\in S$.
    \end{enumerate}
  \end{theorem}

  \begin{theorem}{Weak Law of Large Numbers}
    Let $\{X_t\}_{t\geq0}$ be an \textit{Irreducible Markov Chain} with invariant pmf $\mu(\cdot)$ and let $f:S\to\reals$ by any function. The \textit{Weak Law of Large Numbers} state
    \[ \lim_{N\to\infty}\expect\left[\frac1N\sum_{t=0}^{N-1}f(X_t)\right]=\sum_{s\in S}f(s)\mu(s) \]
    Note that the RHS is the expected value of $f(s)$ wrt $\mu(s)$.
  \end{theorem}

  \begin{theorem}{Poisson Equation}
    Let $\{X_t\}_{t\geq0}$ be an \textit{Irreducible Markov Chain} with \textit{transition kernel} $p(s'|s)$ and \textit{Invariant PMF} $\mu(s)$. Let $f:S\to\reals$ be any function and $\bar{f}$ the expected value of $f$ wrt $\mu$
    \[ \bar{f}:=\sum_{s\in S}f(s)\mu(s) \]
    Then the following hold
    \begin{enumerate}
      \item There exists a function $\check{f}:S\to\reals$ st
      \[ f(s)-\bar{f}=\check{f}(s)-\sum_{s'\in S}\check{f}(s')p(s'|s)\quad\forall\ s\in S \]
      \item Further, if there exists another function $\check{f}':S\to\reals$ st
      \[ f(s)-\bar{f}=\check{f}(s)-\sum_{s'\in S}\check{f}'(s')p(s'|s)\quad\forall\ s\in S \]
      then $\exists\ c\in\reals$ st
      \[ \check{f}'(s)=\check{f}(s)+c\quad\forall\ s\in S \]
    \end{enumerate}
    This $\check{f}$ is known as the \textit{Poisson Equation} for $\{X_t\}_{t\geq0}$ and $f(s)$.
  \end{theorem}

  \begin{theorem}{Laurent Expansion of Resolvent}
    \everymath={\displaystyle}
    Let $\{X_t\}_{t\geq0}$ be an \textit{Irreducible Markov Chain} with \textit{transition kernel} $p(s'|s)$ and \textit{Invariant PMF} $\mu(s)$. Let $f:S\to\reals$ be any function, $\bar{f}$ the expected value of $f$ wrt $\mu$ and $\check{f}(s)$ be a solution to the \textit{Poisson Equation}
    \[\begin{array}{rrl}
      \bar{f}&:=&\sum_{s\in S}f(s)\mu(s)\\
      f(s)-\bar{f}&=&\check{f}-\sum_{s'\in S}\check{f}(s')p(s'|s)
    \end{array}\]
    \[ \bar{f}:=\sum_{s\in S}f(s)\mu(s) \]
    Consider the following function
    \[\begin{array}{rcl}
      \check{f}'(s)&:=&\check{f}(s)-\sum_{s'\in S}\check{f}(s')\\
      f_\alpha(s)&:=&\expect^\pi\left[\sum_{t=0}^\infty\alpha^tf(X_t)\big|X_0=s\right]\quad\alpha\in(0,1)\\
      \tilde{f}_\alpha&:=&f_\alpha(s)-\left(\frac{\bar{f}}{1-\alpha}+\check{f}'(s)\right)
    \end{array}\]
    $\tilde{f}_\alpha(s)$ is known as the \textit{Residual} in the \textit{Laurent Expansion of $f_\alpha(s)$}
    Then
    \[ \lim_{\alpha\to1}\tilde{f}_\alpha(s)=0\quad\forall\ s\in S \]
  \end{theorem}

  \begin{remark}{Poisson Equation}
    The function $\check{f}'(s)'$ is a solution to the \textit{Poisson Equation} associated with \textit{Markov Chain} $\{X_t\}_{t\geq0}$ and function $f(s)$
    \[ f(s)-\bar{f}=\check{f}'(s)-\sum_{s'\in S}\check{f}'(s')p(s'|s) \]
  \end{remark}

  \begin{remark}{Expectation of $\check{f}'(s)$ wrt $\mu(s)$}
    The expectation of function $\check{f}(s)$ wrt $\mu(s)$ is zero
    \[ \sum_{s\in S}\check{f}'(s)\mu(s)=0 \]
  \end{remark}

  \begin{remark}{$f_\alpha(s)$ and $f\tilde{f}_\alpha(s)$}
    $f_\alpha(s)$ is known as the \textit{$alpha$-resolvent} associated with \textit{Markov Chain} $\{X_t\}_{t\geq0}$ and function $f(s)$.
    \par The defining equation for $\tilde{f}_\alpha$ can be rewritten as
    \[ f_\alpha(s)=\frac{\bar{f}}{(1-\alpha)}+\check{f}(s)+\tilde{f}_\alpha(s) \]
    This equation is known as the \textit{Laurent Expansion} of $f_\alpha(s)$ at $\alpha=1$.
    \par By the \textit{Laurent Expansion of Resolvent},
    \[ f_\alpha(s)\approx\frac{\bar{f}}{1-\alpha}+\check{f}(s)\quad\text{when }\alpha\approx1 \]
  \end{remark}

\end{document}
