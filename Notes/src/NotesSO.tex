\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm}
\usepackage[section]{DomH}
\headertitle{Stochastic Optimisation - Notes}

\begin{document}

\title{Stochastic Optimisation - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

\section{Multi-Armed Bandit}

\subsection{The Problem}

\begin{example}{Motivating Example}
  Consider having a group of patients and several treatments they could be assigned to. How best do you go about determining which treatment is best? The obvious approach is to assign some of the patients randomly and then assign the rest to the best treatment, but how much evidence is sufficient? And how likely are you to choose a sub-optimal treatment?
\end{example}

\begin{definition}{Multi-Armed Bandit Problem}
   An agent is faced with a choice of $K$ actions. Each time the agent plays action $i$ they receive a reward from the random real-valued distribution $\nu_i$. Each reward is independent of the past. The distributions $\nu_1,\dots,\nu_K$ are unknown to the agent.\\
   In the \textit{Multi-Armed Bandit Problem} the agent seeks to maximise a measure of long-run reward.
\end{definition}

\begin{remark}{Informal Definition of Multi-Armed Bandit Problem}
  Given a finite set of actions and a random reward for each action, how best do we learn the reward distribution and maximise reward in the long-run.
\end{remark}

\begin{definition}{Formal Definition of Multi-Armed Bandit Problem}
  Consider a sequence of mutually independent random variables $\{X_i(t)\}_{i\in[1,K]}$, with $t\in\nats$. Consider $X_i(t)$ to be the distribution of rewards an agent would receive if they performed action $i$ at time $t$. Since the rewards are independent of the past $X_i(t),X_i(t+1),\dots$ are IID random variables. The \textit{Multi-Armed Bandit Problem} tasks us to find the greatest expected reward from all the actions.
  \[ \mu^*:=\max_{i=1}^K\mu_i\quad\text{where }\mu_i=\expect(X_i(t)) \]
\end{definition}

\begin{remark}{Assumptions}
  For the \textit{Multi-Armed Bandit Problem} we make the following assumptions about the set up
  \begin{itemize}
    \item When action $i$ is played only the realisation of $X_i(t)$ is observed and none of $X_j(t),\ j\neq i$, are observed. Thus when the agent's $t^{th}$ action is played only the rewards of actions $\{1,\dots,t-1\}$ are known to the agent.
    \item The agent has access to an external source of randomness which is used to choose it's next action.
  \end{itemize}
\end{remark}

\begin{definition}{Strategy, $I(\cdot)$}
  Our agent's strategy $I:\nats\to[1,K]$ is a function which determines which action the agent shall make at a given point in time. The strategy can use the knowledge gained from previous actions \& their rewards only.
  \[ I(t)=I\big(t,\underbrace{\{I(s)\}_{\in[1,t)}}_\text{Prev. Actions},\underbrace{\{X_{I(s)}(s)\}_{\in[1,t)}}_\text{Prev. Rewards}\big)\in[1,K] \]
\end{definition}

\begin{definition}{Long-Run Average Reward Criterion, $X_*$}
  For a strategy $I(\cdot)$ we define the following measure for \textit{Long-Run Average Reward}
  \[ {\displaystyle X_*=\lim_{T\to\infty}\inf\frac1T\sum_{t=1}^T\expect(X_{I(t)})} \]
  Most strategies as based only on realisations of $\{X_i(s)\}_{s\in[1,t)}$, thus $\expect(X_{I(t)})\leq\mu^*$ and thus $X_*\leq\mu^*$. A strategy $I(\cdot)$ is \textit{Optimal} if $X_*=\mu^*$.
\end{definition}

\begin{remark}{It is not hard to find an \textit{Optimal Strategy} in the (very) long run.}

\end{remark}

\begin{definition}{Regret, $R_n$}
  \textit{Regret} is a measure of how much reward was lost during the first $n$ time steps. The \textit{Regret} $R_n$ of a strategy $\{I(t)\}_{t\in\nats}$ in the first $n$ time steps is given by
  \[\begin{array}{rcl}
    R_n&=&{\displaystyle \max\limits_{k=1}^K\sum_{t=1}^n \expect[X_k(t)-X_{I(t)}(t)]}\\
    &=&n\mu^*-{\displaystyle\sum_{t=1}^n\expect\big[X_{I(t)}(t)\big]}
  \end{array}\]
  \textit{Regret} only involves expectation and thus can be learnt from observations
\end{definition}

\begin{definition}{Pseudo-Regret, $\tilde{R}_n$}
  \textit{Pseudo-Regret} $\tilde{R}_n$ is a less popular alternative to \textit{Regret} $R_n$.
  The \textit{Pseudo-Regret} $R_n$ of a strategy $\{I(t)\}_{t\in\nats}$ in the first $n$ time steps is given by
  \[ \tilde{R}_n=\max\limits_{k=1}^K\sum_{t=1}^n\big(X_k(t)-X_{I(t)}(t)\big) \]
  \textit{Pseudo-Regret} includes intrinsic randomness (which is independent of the past) and thus cannot be learnt from observations.
\end{definition}

\section{Probability Inequalities}

\begin{remark}{We can use the moments of a random variable to determine bounds on the probability of it taking values in a certain set.}
\end{remark}

\begin{theorem}{Markov's Inequality}
  Let $X$ be a non-negative random variable. Then
  \[ \forall\ c>0\quad\prob(X\geq c)\leq\frac{\expect(X)}c \]
  \textit{Proof}\\
  Consider an event $A$ and define its indicator $\mathbbm{1}(A)(\omega):=\begin{cases}1&\omega\in A\\0&\omega\not\in A\end{cases}$. Fix $c>0$, then
  \[\begin{array}{rrcl}
    &\expect(X)&\geq&\expect[X\mathbbm{1}(X\geq c)]\\
    &&\geq&\expect[c\mathbbm1(X\geq c)]\\
    &&=&c\prob(X\geq c)\\
    \implies&\prob(X\geq c)&\leq&\frac1c\expect(X)
  \end{array}\]
\end{theorem}

\begin{theorem}{Chebyshev's Inequality}
  Let $X$ be a random-variable with finite mean and variance. Then
  \[ \forall\ c>0\quad\prob(|X-\expect(X)|\geq c)\leq\frac{\var(X)}{c^2}\]
  \textit{Proof}\\
  Note that the events $|X-\expect(X)|\geq c$ and $(X-\expect(X))^2\ge c^2)$ are equivalent. Note that $\var([X-\expect(X)]^2)=\var(X)$. Then the result follows by \textit{Markov's Inequality}.
\end{theorem}

\begin{theorem}{Chebyshev's Inequality for Sum of IIDs}
  Let $X_1,\dots,X_n$ be IID random variables with finite mean $\mu$ and finite variance $\sigma^2$.
  \[ \forall\ c>0\quad\prob\left(\left|\left(\sum_{i=1}^nX_i\right)-n\mu\right|\geq nc\right)\leq\frac{\sigma^2}{nc^2} \]
  \textit{Proof}\\
  This is proved by extending the proof of \texttt{Theorem 2.2} and noting that the variance of a sum of IIDs is the sum of the individual variances.
\end{theorem}

\begin{theorem}{Chernoff Bounds}
  Let $X$ be a random variable whose moment-generating function $\expect[e^{\theta X}]$ is finite $\forall\theta$. Then
  \[ \forall\ c\in\reals\quad\prob(X\geq c)\leq\inf_{\theta>0}e^{-\theta c}\expect(e^{\theta X})\quad\text{and}\quad\prob(X\leq c)\leq\inf_{\theta<0}e^{-\theta c}\expect(e^{\theta X}) \]
  \textit{Proof}\\
  Note that the events $X\geq c$ and $e^{\theta X}\geq e^{\theta c}$ are equivalent for all $\theta>0$. The result follows by applying \textit{Markov's Inequality} to $r^{\theta X}$ and taking the best bound over all possible $\theta$.
  \[\begin{array}{rcl}
    \prob(X\geq c)&=&\prob(e^{\theta X}\geq e^{\theta c})\\
    &\leq&e^{-\theta c}\expect(e^{\theta X})\\
    &\leq&\inf_{\theta<0}e^{-\theta c}\expect(e^{\theta X}) %TODO check this step
  \end{array}\]
\end{theorem}

\begin{theorem}{Chernoff Bounds for Sum of IIDs}
  Let $X_1,\dots,X_n$ be IID random variables. Then $\forall\ c\in\reals$
  \[\begin{array}{rcl}
    {\displaystyle\prob\left(\sum_{i=1}^nX_i\geq nc\right)}&\leq\inf\limits_{\theta>0}e^{-n\theta c}\left(\expect\left[e^{\theta X}\right]\right)^n\\
    {\displaystyle\prob\left(\sum_{i=1}^nX_i\leq nc\right)}&\leq\inf\limits_{\theta<0}e^{-n\theta c}\left(\expect\left[e^{\theta X}\right]\right)^n
  \end{array}\]
\end{theorem}

\begin{theorem}{Bound on Moment Generating Function}
  Let $X$ be a random variable taking values in $[0,1]$ with finite expected value $\mu$. Then
  \[ \forall\ \theta\in\reals\quad\expect\left[e^{\theta(X-\mu)}\right]\leq e^{\theta^2/8} \]
\end{theorem}

\begin{theorem}{Hoeffding's Theorem}
  Let $X_1,\dots,X_n$ be IID random variables taking values in $[0,1]$ and with finite expected value $\mu$. Then
  \[ \forall\ t>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{-2nt^2} \]
  \textit{Proof}\\
  From \textit{Chernoff's Bound} we have that
  \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{-\theta nt}\left(\expect[e^{\theta(X-\mu)}]\right)^n \]
  Using \texttt{Theorem 2.6} to bound the moment generating function, we get
  \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{-\theta nt}\cdot e^{n\frac{\theta^2}8}=e^{n\left(-\theta t+\frac{1}8\theta^2\right)} \]
  Thus, by taking $\log$s and rearranging, we get
  \[ \forall\ \theta>0\quad\frac1n\log\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq-\theta t+\frac{\theta^2}8 \]
  We have that $-\theta t+\frac{\theta^2}8$ is minimised at $\theta=4t$ which is positive if $t$ is positive. Thus, by applying this bound and substituting $\theta=4t$ we get
  \[ \forall\ \theta>0\quad\prob\left(\sum_{i=1}^n(X_i-\mu)>nt\right)\leq e^{n\left(-4t^2+\frac{1}8(16t^2)\right)}=e^{n(-4t^2+2t^2)}=e^{-2nt^2} \]
  $\hfill\square$
\end{theorem}

\begin{theorem}{Jensen's Inequality}
  Let $f$ be a \textit{Convex Function} and $X$ be a random variable. Then
  \[ \expect[f(X)]\geq f(\expect[X]) \]
\end{theorem}

\setcounter{section}{-1}
\section{Reference}

\begin{definition}{Convex Function}
  A function $f:\reals\to(\reals\cup\{+\infty\})$ is \textit{Convex} if, $\forall\ x,y\in\reals,\ \alpha\in[0,1]$, we have
  \[ f(\alpha x+(1-\alpha)y)\leq\alpha f(x)+(1-\alpha)f(y) \]
  A smooth function $f$ is convex iff $f$ is twice differentiable and $f''(x)\geq0\ \forall\ x\in\reals$.
\end{definition}

\end{document}
